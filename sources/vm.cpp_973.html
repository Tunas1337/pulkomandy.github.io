
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>vm.cpp</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">/*</a>
<a name="ln2"> * Copyright 2009-2011, Ingo Weinhold, ingo_weinhold@gmx.de.</a>
<a name="ln3"> * Copyright 2002-2010, Axel DÃ¶rfler, axeld@pinc-software.de.</a>
<a name="ln4"> * Distributed under the terms of the MIT License.</a>
<a name="ln5"> *</a>
<a name="ln6"> * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.</a>
<a name="ln7"> * Distributed under the terms of the NewOS License.</a>
<a name="ln8"> */</a>
<a name="ln9"> </a>
<a name="ln10"> </a>
<a name="ln11">#include &lt;vm/vm.h&gt;</a>
<a name="ln12"> </a>
<a name="ln13">#include &lt;ctype.h&gt;</a>
<a name="ln14">#include &lt;stdlib.h&gt;</a>
<a name="ln15">#include &lt;stdio.h&gt;</a>
<a name="ln16">#include &lt;string.h&gt;</a>
<a name="ln17">#include &lt;sys/mman.h&gt;</a>
<a name="ln18"> </a>
<a name="ln19">#include &lt;algorithm&gt;</a>
<a name="ln20"> </a>
<a name="ln21">#include &lt;OS.h&gt;</a>
<a name="ln22">#include &lt;KernelExport.h&gt;</a>
<a name="ln23"> </a>
<a name="ln24">#include &lt;AutoDeleter.h&gt;</a>
<a name="ln25"> </a>
<a name="ln26">#include &lt;symbol_versioning.h&gt;</a>
<a name="ln27"> </a>
<a name="ln28">#include &lt;arch/cpu.h&gt;</a>
<a name="ln29">#include &lt;arch/vm.h&gt;</a>
<a name="ln30">#include &lt;arch/user_memory.h&gt;</a>
<a name="ln31">#include &lt;boot/elf.h&gt;</a>
<a name="ln32">#include &lt;boot/stage2.h&gt;</a>
<a name="ln33">#include &lt;condition_variable.h&gt;</a>
<a name="ln34">#include &lt;console.h&gt;</a>
<a name="ln35">#include &lt;debug.h&gt;</a>
<a name="ln36">#include &lt;file_cache.h&gt;</a>
<a name="ln37">#include &lt;fs/fd.h&gt;</a>
<a name="ln38">#include &lt;heap.h&gt;</a>
<a name="ln39">#include &lt;kernel.h&gt;</a>
<a name="ln40">#include &lt;int.h&gt;</a>
<a name="ln41">#include &lt;lock.h&gt;</a>
<a name="ln42">#include &lt;low_resource_manager.h&gt;</a>
<a name="ln43">#include &lt;slab/Slab.h&gt;</a>
<a name="ln44">#include &lt;smp.h&gt;</a>
<a name="ln45">#include &lt;system_info.h&gt;</a>
<a name="ln46">#include &lt;thread.h&gt;</a>
<a name="ln47">#include &lt;team.h&gt;</a>
<a name="ln48">#include &lt;tracing.h&gt;</a>
<a name="ln49">#include &lt;util/AutoLock.h&gt;</a>
<a name="ln50">#include &lt;vm/vm_page.h&gt;</a>
<a name="ln51">#include &lt;vm/vm_priv.h&gt;</a>
<a name="ln52">#include &lt;vm/VMAddressSpace.h&gt;</a>
<a name="ln53">#include &lt;vm/VMArea.h&gt;</a>
<a name="ln54">#include &lt;vm/VMCache.h&gt;</a>
<a name="ln55"> </a>
<a name="ln56">#include &quot;VMAddressSpaceLocking.h&quot;</a>
<a name="ln57">#include &quot;VMAnonymousCache.h&quot;</a>
<a name="ln58">#include &quot;VMAnonymousNoSwapCache.h&quot;</a>
<a name="ln59">#include &quot;IORequest.h&quot;</a>
<a name="ln60"> </a>
<a name="ln61"> </a>
<a name="ln62">//#define TRACE_VM</a>
<a name="ln63">//#define TRACE_FAULTS</a>
<a name="ln64">#ifdef TRACE_VM</a>
<a name="ln65">#	define TRACE(x) dprintf x</a>
<a name="ln66">#else</a>
<a name="ln67">#	define TRACE(x) ;</a>
<a name="ln68">#endif</a>
<a name="ln69">#ifdef TRACE_FAULTS</a>
<a name="ln70">#	define FTRACE(x) dprintf x</a>
<a name="ln71">#else</a>
<a name="ln72">#	define FTRACE(x) ;</a>
<a name="ln73">#endif</a>
<a name="ln74"> </a>
<a name="ln75"> </a>
<a name="ln76">namespace {</a>
<a name="ln77"> </a>
<a name="ln78">class AreaCacheLocking {</a>
<a name="ln79">public:</a>
<a name="ln80">	inline bool Lock(VMCache* lockable)</a>
<a name="ln81">	{</a>
<a name="ln82">		return false;</a>
<a name="ln83">	}</a>
<a name="ln84"> </a>
<a name="ln85">	inline void Unlock(VMCache* lockable)</a>
<a name="ln86">	{</a>
<a name="ln87">		vm_area_put_locked_cache(lockable);</a>
<a name="ln88">	}</a>
<a name="ln89">};</a>
<a name="ln90"> </a>
<a name="ln91">class AreaCacheLocker : public AutoLocker&lt;VMCache, AreaCacheLocking&gt; {</a>
<a name="ln92">public:</a>
<a name="ln93">	inline AreaCacheLocker(VMCache* cache = NULL)</a>
<a name="ln94">		: AutoLocker&lt;VMCache, AreaCacheLocking&gt;(cache, true)</a>
<a name="ln95">	{</a>
<a name="ln96">	}</a>
<a name="ln97"> </a>
<a name="ln98">	inline AreaCacheLocker(VMArea* area)</a>
<a name="ln99">		: AutoLocker&lt;VMCache, AreaCacheLocking&gt;()</a>
<a name="ln100">	{</a>
<a name="ln101">		SetTo(area);</a>
<a name="ln102">	}</a>
<a name="ln103"> </a>
<a name="ln104">	inline void SetTo(VMCache* cache, bool alreadyLocked)</a>
<a name="ln105">	{</a>
<a name="ln106">		AutoLocker&lt;VMCache, AreaCacheLocking&gt;::SetTo(cache, alreadyLocked);</a>
<a name="ln107">	}</a>
<a name="ln108"> </a>
<a name="ln109">	inline void SetTo(VMArea* area)</a>
<a name="ln110">	{</a>
<a name="ln111">		return AutoLocker&lt;VMCache, AreaCacheLocking&gt;::SetTo(</a>
<a name="ln112">			area != NULL ? vm_area_get_locked_cache(area) : NULL, true, true);</a>
<a name="ln113">	}</a>
<a name="ln114">};</a>
<a name="ln115"> </a>
<a name="ln116"> </a>
<a name="ln117">class VMCacheChainLocker {</a>
<a name="ln118">public:</a>
<a name="ln119">	VMCacheChainLocker()</a>
<a name="ln120">		:</a>
<a name="ln121">		fTopCache(NULL),</a>
<a name="ln122">		fBottomCache(NULL)</a>
<a name="ln123">	{</a>
<a name="ln124">	}</a>
<a name="ln125"> </a>
<a name="ln126">	VMCacheChainLocker(VMCache* topCache)</a>
<a name="ln127">		:</a>
<a name="ln128">		fTopCache(topCache),</a>
<a name="ln129">		fBottomCache(topCache)</a>
<a name="ln130">	{</a>
<a name="ln131">	}</a>
<a name="ln132"> </a>
<a name="ln133">	~VMCacheChainLocker()</a>
<a name="ln134">	{</a>
<a name="ln135">		Unlock();</a>
<a name="ln136">	}</a>
<a name="ln137"> </a>
<a name="ln138">	void SetTo(VMCache* topCache)</a>
<a name="ln139">	{</a>
<a name="ln140">		fTopCache = topCache;</a>
<a name="ln141">		fBottomCache = topCache;</a>
<a name="ln142"> </a>
<a name="ln143">		if (topCache != NULL)</a>
<a name="ln144">			topCache-&gt;SetUserData(NULL);</a>
<a name="ln145">	}</a>
<a name="ln146"> </a>
<a name="ln147">	VMCache* LockSourceCache()</a>
<a name="ln148">	{</a>
<a name="ln149">		if (fBottomCache == NULL || fBottomCache-&gt;source == NULL)</a>
<a name="ln150">			return NULL;</a>
<a name="ln151"> </a>
<a name="ln152">		VMCache* previousCache = fBottomCache;</a>
<a name="ln153"> </a>
<a name="ln154">		fBottomCache = fBottomCache-&gt;source;</a>
<a name="ln155">		fBottomCache-&gt;Lock();</a>
<a name="ln156">		fBottomCache-&gt;AcquireRefLocked();</a>
<a name="ln157">		fBottomCache-&gt;SetUserData(previousCache);</a>
<a name="ln158"> </a>
<a name="ln159">		return fBottomCache;</a>
<a name="ln160">	}</a>
<a name="ln161"> </a>
<a name="ln162">	void LockAllSourceCaches()</a>
<a name="ln163">	{</a>
<a name="ln164">		while (LockSourceCache() != NULL) {</a>
<a name="ln165">		}</a>
<a name="ln166">	}</a>
<a name="ln167"> </a>
<a name="ln168">	void Unlock(VMCache* exceptCache = NULL)</a>
<a name="ln169">	{</a>
<a name="ln170">		if (fTopCache == NULL)</a>
<a name="ln171">			return;</a>
<a name="ln172"> </a>
<a name="ln173">		// Unlock caches in source -&gt; consumer direction. This is important to</a>
<a name="ln174">		// avoid double-locking and a reversal of locking order in case a cache</a>
<a name="ln175">		// is eligable for merging.</a>
<a name="ln176">		VMCache* cache = fBottomCache;</a>
<a name="ln177">		while (cache != NULL) {</a>
<a name="ln178">			VMCache* nextCache = (VMCache*)cache-&gt;UserData();</a>
<a name="ln179">			if (cache != exceptCache)</a>
<a name="ln180">				cache-&gt;ReleaseRefAndUnlock(cache != fTopCache);</a>
<a name="ln181"> </a>
<a name="ln182">			if (cache == fTopCache)</a>
<a name="ln183">				break;</a>
<a name="ln184"> </a>
<a name="ln185">			cache = nextCache;</a>
<a name="ln186">		}</a>
<a name="ln187"> </a>
<a name="ln188">		fTopCache = NULL;</a>
<a name="ln189">		fBottomCache = NULL;</a>
<a name="ln190">	}</a>
<a name="ln191"> </a>
<a name="ln192">	void UnlockKeepRefs(bool keepTopCacheLocked)</a>
<a name="ln193">	{</a>
<a name="ln194">		if (fTopCache == NULL)</a>
<a name="ln195">			return;</a>
<a name="ln196"> </a>
<a name="ln197">		VMCache* nextCache = fBottomCache;</a>
<a name="ln198">		VMCache* cache = NULL;</a>
<a name="ln199"> </a>
<a name="ln200">		while (keepTopCacheLocked</a>
<a name="ln201">				? nextCache != fTopCache : cache != fTopCache) {</a>
<a name="ln202">			cache = nextCache;</a>
<a name="ln203">			nextCache = (VMCache*)cache-&gt;UserData();</a>
<a name="ln204">			cache-&gt;Unlock(cache != fTopCache);</a>
<a name="ln205">		}</a>
<a name="ln206">	}</a>
<a name="ln207"> </a>
<a name="ln208">	void RelockCaches(bool topCacheLocked)</a>
<a name="ln209">	{</a>
<a name="ln210">		if (fTopCache == NULL)</a>
<a name="ln211">			return;</a>
<a name="ln212"> </a>
<a name="ln213">		VMCache* nextCache = fTopCache;</a>
<a name="ln214">		VMCache* cache = NULL;</a>
<a name="ln215">		if (topCacheLocked) {</a>
<a name="ln216">			cache = nextCache;</a>
<a name="ln217">			nextCache = cache-&gt;source;</a>
<a name="ln218">		}</a>
<a name="ln219"> </a>
<a name="ln220">		while (cache != fBottomCache &amp;&amp; nextCache != NULL) {</a>
<a name="ln221">			VMCache* consumer = cache;</a>
<a name="ln222">			cache = nextCache;</a>
<a name="ln223">			nextCache = cache-&gt;source;</a>
<a name="ln224">			cache-&gt;Lock();</a>
<a name="ln225">			cache-&gt;SetUserData(consumer);</a>
<a name="ln226">		}</a>
<a name="ln227">	}</a>
<a name="ln228"> </a>
<a name="ln229">private:</a>
<a name="ln230">	VMCache*	fTopCache;</a>
<a name="ln231">	VMCache*	fBottomCache;</a>
<a name="ln232">};</a>
<a name="ln233"> </a>
<a name="ln234">} // namespace</a>
<a name="ln235"> </a>
<a name="ln236"> </a>
<a name="ln237">// The memory reserve an allocation of the certain priority must not touch.</a>
<a name="ln238">static const size_t kMemoryReserveForPriority[] = {</a>
<a name="ln239">	VM_MEMORY_RESERVE_USER,		// user</a>
<a name="ln240">	VM_MEMORY_RESERVE_SYSTEM,	// system</a>
<a name="ln241">	0							// VIP</a>
<a name="ln242">};</a>
<a name="ln243"> </a>
<a name="ln244"> </a>
<a name="ln245">ObjectCache* gPageMappingsObjectCache;</a>
<a name="ln246"> </a>
<a name="ln247">static rw_lock sAreaCacheLock = RW_LOCK_INITIALIZER(&quot;area-&gt;cache&quot;);</a>
<a name="ln248"> </a>
<a name="ln249">static off_t sAvailableMemory;</a>
<a name="ln250">static off_t sNeededMemory;</a>
<a name="ln251">static mutex sAvailableMemoryLock = MUTEX_INITIALIZER(&quot;available memory lock&quot;);</a>
<a name="ln252">static uint32 sPageFaults;</a>
<a name="ln253"> </a>
<a name="ln254">static VMPhysicalPageMapper* sPhysicalPageMapper;</a>
<a name="ln255"> </a>
<a name="ln256">#if DEBUG_CACHE_LIST</a>
<a name="ln257"> </a>
<a name="ln258">struct cache_info {</a>
<a name="ln259">	VMCache*	cache;</a>
<a name="ln260">	addr_t		page_count;</a>
<a name="ln261">	addr_t		committed;</a>
<a name="ln262">};</a>
<a name="ln263"> </a>
<a name="ln264">static const int kCacheInfoTableCount = 100 * 1024;</a>
<a name="ln265">static cache_info* sCacheInfoTable;</a>
<a name="ln266"> </a>
<a name="ln267">#endif	// DEBUG_CACHE_LIST</a>
<a name="ln268"> </a>
<a name="ln269"> </a>
<a name="ln270">// function declarations</a>
<a name="ln271">static void delete_area(VMAddressSpace* addressSpace, VMArea* area,</a>
<a name="ln272">	bool addressSpaceCleanup);</a>
<a name="ln273">static status_t vm_soft_fault(VMAddressSpace* addressSpace, addr_t address,</a>
<a name="ln274">	bool isWrite, bool isExecute, bool isUser, vm_page** wirePage);</a>
<a name="ln275">static status_t map_backing_store(VMAddressSpace* addressSpace,</a>
<a name="ln276">	VMCache* cache, off_t offset, const char* areaName, addr_t size, int wiring,</a>
<a name="ln277">	int protection, int mapping, uint32 flags,</a>
<a name="ln278">	const virtual_address_restrictions* addressRestrictions, bool kernel,</a>
<a name="ln279">	VMArea** _area, void** _virtualAddress);</a>
<a name="ln280">static void fix_protection(uint32* protection);</a>
<a name="ln281"> </a>
<a name="ln282"> </a>
<a name="ln283">//	#pragma mark -</a>
<a name="ln284"> </a>
<a name="ln285"> </a>
<a name="ln286">#if VM_PAGE_FAULT_TRACING</a>
<a name="ln287"> </a>
<a name="ln288">namespace VMPageFaultTracing {</a>
<a name="ln289"> </a>
<a name="ln290">class PageFaultStart : public AbstractTraceEntry {</a>
<a name="ln291">public:</a>
<a name="ln292">	PageFaultStart(addr_t address, bool write, bool user, addr_t pc)</a>
<a name="ln293">		:</a>
<a name="ln294">		fAddress(address),</a>
<a name="ln295">		fPC(pc),</a>
<a name="ln296">		fWrite(write),</a>
<a name="ln297">		fUser(user)</a>
<a name="ln298">	{</a>
<a name="ln299">		Initialized();</a>
<a name="ln300">	}</a>
<a name="ln301"> </a>
<a name="ln302">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln303">	{</a>
<a name="ln304">		out.Print(&quot;page fault %#lx %s %s, pc: %#lx&quot;, fAddress,</a>
<a name="ln305">			fWrite ? &quot;write&quot; : &quot;read&quot;, fUser ? &quot;user&quot; : &quot;kernel&quot;, fPC);</a>
<a name="ln306">	}</a>
<a name="ln307"> </a>
<a name="ln308">private:</a>
<a name="ln309">	addr_t	fAddress;</a>
<a name="ln310">	addr_t	fPC;</a>
<a name="ln311">	bool	fWrite;</a>
<a name="ln312">	bool	fUser;</a>
<a name="ln313">};</a>
<a name="ln314"> </a>
<a name="ln315"> </a>
<a name="ln316">// page fault errors</a>
<a name="ln317">enum {</a>
<a name="ln318">	PAGE_FAULT_ERROR_NO_AREA		= 0,</a>
<a name="ln319">	PAGE_FAULT_ERROR_KERNEL_ONLY,</a>
<a name="ln320">	PAGE_FAULT_ERROR_WRITE_PROTECTED,</a>
<a name="ln321">	PAGE_FAULT_ERROR_READ_PROTECTED,</a>
<a name="ln322">	PAGE_FAULT_ERROR_EXECUTE_PROTECTED,</a>
<a name="ln323">	PAGE_FAULT_ERROR_KERNEL_BAD_USER_MEMORY,</a>
<a name="ln324">	PAGE_FAULT_ERROR_NO_ADDRESS_SPACE</a>
<a name="ln325">};</a>
<a name="ln326"> </a>
<a name="ln327"> </a>
<a name="ln328">class PageFaultError : public AbstractTraceEntry {</a>
<a name="ln329">public:</a>
<a name="ln330">	PageFaultError(area_id area, status_t error)</a>
<a name="ln331">		:</a>
<a name="ln332">		fArea(area),</a>
<a name="ln333">		fError(error)</a>
<a name="ln334">	{</a>
<a name="ln335">		Initialized();</a>
<a name="ln336">	}</a>
<a name="ln337"> </a>
<a name="ln338">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln339">	{</a>
<a name="ln340">		switch (fError) {</a>
<a name="ln341">			case PAGE_FAULT_ERROR_NO_AREA:</a>
<a name="ln342">				out.Print(&quot;page fault error: no area&quot;);</a>
<a name="ln343">				break;</a>
<a name="ln344">			case PAGE_FAULT_ERROR_KERNEL_ONLY:</a>
<a name="ln345">				out.Print(&quot;page fault error: area: %ld, kernel only&quot;, fArea);</a>
<a name="ln346">				break;</a>
<a name="ln347">			case PAGE_FAULT_ERROR_WRITE_PROTECTED:</a>
<a name="ln348">				out.Print(&quot;page fault error: area: %ld, write protected&quot;,</a>
<a name="ln349">					fArea);</a>
<a name="ln350">				break;</a>
<a name="ln351">			case PAGE_FAULT_ERROR_READ_PROTECTED:</a>
<a name="ln352">				out.Print(&quot;page fault error: area: %ld, read protected&quot;, fArea);</a>
<a name="ln353">				break;</a>
<a name="ln354">			case PAGE_FAULT_ERROR_EXECUTE_PROTECTED:</a>
<a name="ln355">				out.Print(&quot;page fault error: area: %ld, execute protected&quot;,</a>
<a name="ln356">					fArea);</a>
<a name="ln357">				break;</a>
<a name="ln358">			case PAGE_FAULT_ERROR_KERNEL_BAD_USER_MEMORY:</a>
<a name="ln359">				out.Print(&quot;page fault error: kernel touching bad user memory&quot;);</a>
<a name="ln360">				break;</a>
<a name="ln361">			case PAGE_FAULT_ERROR_NO_ADDRESS_SPACE:</a>
<a name="ln362">				out.Print(&quot;page fault error: no address space&quot;);</a>
<a name="ln363">				break;</a>
<a name="ln364">			default:</a>
<a name="ln365">				out.Print(&quot;page fault error: area: %ld, error: %s&quot;, fArea,</a>
<a name="ln366">					strerror(fError));</a>
<a name="ln367">				break;</a>
<a name="ln368">		}</a>
<a name="ln369">	}</a>
<a name="ln370"> </a>
<a name="ln371">private:</a>
<a name="ln372">	area_id		fArea;</a>
<a name="ln373">	status_t	fError;</a>
<a name="ln374">};</a>
<a name="ln375"> </a>
<a name="ln376"> </a>
<a name="ln377">class PageFaultDone : public AbstractTraceEntry {</a>
<a name="ln378">public:</a>
<a name="ln379">	PageFaultDone(area_id area, VMCache* topCache, VMCache* cache,</a>
<a name="ln380">			vm_page* page)</a>
<a name="ln381">		:</a>
<a name="ln382">		fArea(area),</a>
<a name="ln383">		fTopCache(topCache),</a>
<a name="ln384">		fCache(cache),</a>
<a name="ln385">		fPage(page)</a>
<a name="ln386">	{</a>
<a name="ln387">		Initialized();</a>
<a name="ln388">	}</a>
<a name="ln389"> </a>
<a name="ln390">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln391">	{</a>
<a name="ln392">		out.Print(&quot;page fault done: area: %ld, top cache: %p, cache: %p, &quot;</a>
<a name="ln393">			&quot;page: %p&quot;, fArea, fTopCache, fCache, fPage);</a>
<a name="ln394">	}</a>
<a name="ln395"> </a>
<a name="ln396">private:</a>
<a name="ln397">	area_id		fArea;</a>
<a name="ln398">	VMCache*	fTopCache;</a>
<a name="ln399">	VMCache*	fCache;</a>
<a name="ln400">	vm_page*	fPage;</a>
<a name="ln401">};</a>
<a name="ln402"> </a>
<a name="ln403">}	// namespace VMPageFaultTracing</a>
<a name="ln404"> </a>
<a name="ln405">#	define TPF(x) new(std::nothrow) VMPageFaultTracing::x;</a>
<a name="ln406">#else</a>
<a name="ln407">#	define TPF(x) ;</a>
<a name="ln408">#endif	// VM_PAGE_FAULT_TRACING</a>
<a name="ln409"> </a>
<a name="ln410"> </a>
<a name="ln411">//	#pragma mark -</a>
<a name="ln412"> </a>
<a name="ln413"> </a>
<a name="ln414">/*!	The page's cache must be locked.</a>
<a name="ln415">*/</a>
<a name="ln416">static inline void</a>
<a name="ln417">increment_page_wired_count(vm_page* page)</a>
<a name="ln418">{</a>
<a name="ln419">	if (!page-&gt;IsMapped())</a>
<a name="ln420">		atomic_add(&amp;gMappedPagesCount, 1);</a>
<a name="ln421">	page-&gt;IncrementWiredCount();</a>
<a name="ln422">}</a>
<a name="ln423"> </a>
<a name="ln424"> </a>
<a name="ln425">/*!	The page's cache must be locked.</a>
<a name="ln426">*/</a>
<a name="ln427">static inline void</a>
<a name="ln428">decrement_page_wired_count(vm_page* page)</a>
<a name="ln429">{</a>
<a name="ln430">	page-&gt;DecrementWiredCount();</a>
<a name="ln431">	if (!page-&gt;IsMapped())</a>
<a name="ln432">		atomic_add(&amp;gMappedPagesCount, -1);</a>
<a name="ln433">}</a>
<a name="ln434"> </a>
<a name="ln435"> </a>
<a name="ln436">static inline addr_t</a>
<a name="ln437">virtual_page_address(VMArea* area, vm_page* page)</a>
<a name="ln438">{</a>
<a name="ln439">	return area-&gt;Base()</a>
<a name="ln440">		+ ((page-&gt;cache_offset &lt;&lt; PAGE_SHIFT) - area-&gt;cache_offset);</a>
<a name="ln441">}</a>
<a name="ln442"> </a>
<a name="ln443"> </a>
<a name="ln444">//! You need to have the address space locked when calling this function</a>
<a name="ln445">static VMArea*</a>
<a name="ln446">lookup_area(VMAddressSpace* addressSpace, area_id id)</a>
<a name="ln447">{</a>
<a name="ln448">	VMAreaHash::ReadLock();</a>
<a name="ln449"> </a>
<a name="ln450">	VMArea* area = VMAreaHash::LookupLocked(id);</a>
<a name="ln451">	if (area != NULL &amp;&amp; area-&gt;address_space != addressSpace)</a>
<a name="ln452">		area = NULL;</a>
<a name="ln453"> </a>
<a name="ln454">	VMAreaHash::ReadUnlock();</a>
<a name="ln455"> </a>
<a name="ln456">	return area;</a>
<a name="ln457">}</a>
<a name="ln458"> </a>
<a name="ln459"> </a>
<a name="ln460">static status_t</a>
<a name="ln461">allocate_area_page_protections(VMArea* area)</a>
<a name="ln462">{</a>
<a name="ln463">	// In the page protections we store only the three user protections,</a>
<a name="ln464">	// so we use 4 bits per page.</a>
<a name="ln465">	uint32 bytes = (area-&gt;Size() / B_PAGE_SIZE + 1) / 2;</a>
<a name="ln466">	area-&gt;page_protections = (uint8*)malloc_etc(bytes,</a>
<a name="ln467">		HEAP_DONT_LOCK_KERNEL_SPACE);</a>
<a name="ln468">	if (area-&gt;page_protections == NULL)</a>
<a name="ln469">		return B_NO_MEMORY;</a>
<a name="ln470"> </a>
<a name="ln471">	// init the page protections for all pages to that of the area</a>
<a name="ln472">	uint32 areaProtection = area-&gt;protection</a>
<a name="ln473">		&amp; (B_READ_AREA | B_WRITE_AREA | B_EXECUTE_AREA);</a>
<a name="ln474">	memset(area-&gt;page_protections, areaProtection | (areaProtection &lt;&lt; 4),</a>
<a name="ln475">		bytes);</a>
<a name="ln476">	return B_OK;</a>
<a name="ln477">}</a>
<a name="ln478"> </a>
<a name="ln479"> </a>
<a name="ln480">static inline void</a>
<a name="ln481">set_area_page_protection(VMArea* area, addr_t pageAddress, uint32 protection)</a>
<a name="ln482">{</a>
<a name="ln483">	protection &amp;= B_READ_AREA | B_WRITE_AREA | B_EXECUTE_AREA;</a>
<a name="ln484">	uint32 pageIndex = (pageAddress - area-&gt;Base()) / B_PAGE_SIZE;</a>
<a name="ln485">	uint8&amp; entry = area-&gt;page_protections[pageIndex / 2];</a>
<a name="ln486">	if (pageIndex % 2 == 0)</a>
<a name="ln487">		entry = (entry &amp; 0xf0) | protection;</a>
<a name="ln488">	else</a>
<a name="ln489">		entry = (entry &amp; 0x0f) | (protection &lt;&lt; 4);</a>
<a name="ln490">}</a>
<a name="ln491"> </a>
<a name="ln492"> </a>
<a name="ln493">static inline uint32</a>
<a name="ln494">get_area_page_protection(VMArea* area, addr_t pageAddress)</a>
<a name="ln495">{</a>
<a name="ln496">	if (area-&gt;page_protections == NULL)</a>
<a name="ln497">		return area-&gt;protection;</a>
<a name="ln498"> </a>
<a name="ln499">	uint32 pageIndex = (pageAddress - area-&gt;Base()) / B_PAGE_SIZE;</a>
<a name="ln500">	uint32 protection = area-&gt;page_protections[pageIndex / 2];</a>
<a name="ln501">	if (pageIndex % 2 == 0)</a>
<a name="ln502">		protection &amp;= 0x0f;</a>
<a name="ln503">	else</a>
<a name="ln504">		protection &gt;&gt;= 4;</a>
<a name="ln505"> </a>
<a name="ln506">	// If this is a kernel area we translate the user flags to kernel flags.</a>
<a name="ln507">	if (area-&gt;address_space == VMAddressSpace::Kernel()) {</a>
<a name="ln508">		uint32 kernelProtection = 0;</a>
<a name="ln509">		if ((protection &amp; B_READ_AREA) != 0)</a>
<a name="ln510">			kernelProtection |= B_KERNEL_READ_AREA;</a>
<a name="ln511">		if ((protection &amp; B_WRITE_AREA) != 0)</a>
<a name="ln512">			kernelProtection |= B_KERNEL_WRITE_AREA;</a>
<a name="ln513"> </a>
<a name="ln514">		return kernelProtection;</a>
<a name="ln515">	}</a>
<a name="ln516"> </a>
<a name="ln517">	return protection | B_KERNEL_READ_AREA</a>
<a name="ln518">		| (protection &amp; B_WRITE_AREA ? B_KERNEL_WRITE_AREA : 0);</a>
<a name="ln519">}</a>
<a name="ln520"> </a>
<a name="ln521"> </a>
<a name="ln522">/*!	The caller must have reserved enough pages the translation map</a>
<a name="ln523">	implementation might need to map this page.</a>
<a name="ln524">	The page's cache must be locked.</a>
<a name="ln525">*/</a>
<a name="ln526">static status_t</a>
<a name="ln527">map_page(VMArea* area, vm_page* page, addr_t address, uint32 protection,</a>
<a name="ln528">	vm_page_reservation* reservation)</a>
<a name="ln529">{</a>
<a name="ln530">	VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln531"> </a>
<a name="ln532">	bool wasMapped = page-&gt;IsMapped();</a>
<a name="ln533"> </a>
<a name="ln534">	if (area-&gt;wiring == B_NO_LOCK) {</a>
<a name="ln535">		DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln536"> </a>
<a name="ln537">		bool isKernelSpace = area-&gt;address_space == VMAddressSpace::Kernel();</a>
<a name="ln538">		vm_page_mapping* mapping = (vm_page_mapping*)object_cache_alloc(</a>
<a name="ln539">			gPageMappingsObjectCache,</a>
<a name="ln540">			CACHE_DONT_WAIT_FOR_MEMORY</a>
<a name="ln541">				| (isKernelSpace ? CACHE_DONT_LOCK_KERNEL_SPACE : 0));</a>
<a name="ln542">		if (mapping == NULL)</a>
<a name="ln543">			return B_NO_MEMORY;</a>
<a name="ln544"> </a>
<a name="ln545">		mapping-&gt;page = page;</a>
<a name="ln546">		mapping-&gt;area = area;</a>
<a name="ln547"> </a>
<a name="ln548">		map-&gt;Lock();</a>
<a name="ln549"> </a>
<a name="ln550">		map-&gt;Map(address, page-&gt;physical_page_number * B_PAGE_SIZE, protection,</a>
<a name="ln551">			area-&gt;MemoryType(), reservation);</a>
<a name="ln552"> </a>
<a name="ln553">		// insert mapping into lists</a>
<a name="ln554">		if (!page-&gt;IsMapped())</a>
<a name="ln555">			atomic_add(&amp;gMappedPagesCount, 1);</a>
<a name="ln556"> </a>
<a name="ln557">		page-&gt;mappings.Add(mapping);</a>
<a name="ln558">		area-&gt;mappings.Add(mapping);</a>
<a name="ln559"> </a>
<a name="ln560">		map-&gt;Unlock();</a>
<a name="ln561">	} else {</a>
<a name="ln562">		DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln563"> </a>
<a name="ln564">		map-&gt;Lock();</a>
<a name="ln565">		map-&gt;Map(address, page-&gt;physical_page_number * B_PAGE_SIZE, protection,</a>
<a name="ln566">			area-&gt;MemoryType(), reservation);</a>
<a name="ln567">		map-&gt;Unlock();</a>
<a name="ln568"> </a>
<a name="ln569">		increment_page_wired_count(page);</a>
<a name="ln570">	}</a>
<a name="ln571"> </a>
<a name="ln572">	if (!wasMapped) {</a>
<a name="ln573">		// The page is mapped now, so we must not remain in the cached queue.</a>
<a name="ln574">		// It also makes sense to move it from the inactive to the active, since</a>
<a name="ln575">		// otherwise the page daemon wouldn't come to keep track of it (in idle</a>
<a name="ln576">		// mode) -- if the page isn't touched, it will be deactivated after a</a>
<a name="ln577">		// full iteration through the queue at the latest.</a>
<a name="ln578">		if (page-&gt;State() == PAGE_STATE_CACHED</a>
<a name="ln579">				|| page-&gt;State() == PAGE_STATE_INACTIVE) {</a>
<a name="ln580">			vm_page_set_state(page, PAGE_STATE_ACTIVE);</a>
<a name="ln581">		}</a>
<a name="ln582">	}</a>
<a name="ln583"> </a>
<a name="ln584">	return B_OK;</a>
<a name="ln585">}</a>
<a name="ln586"> </a>
<a name="ln587"> </a>
<a name="ln588">/*!	If \a preserveModified is \c true, the caller must hold the lock of the</a>
<a name="ln589">	page's cache.</a>
<a name="ln590">*/</a>
<a name="ln591">static inline bool</a>
<a name="ln592">unmap_page(VMArea* area, addr_t virtualAddress)</a>
<a name="ln593">{</a>
<a name="ln594">	return area-&gt;address_space-&gt;TranslationMap()-&gt;UnmapPage(area,</a>
<a name="ln595">		virtualAddress, true);</a>
<a name="ln596">}</a>
<a name="ln597"> </a>
<a name="ln598"> </a>
<a name="ln599">/*!	If \a preserveModified is \c true, the caller must hold the lock of all</a>
<a name="ln600">	mapped pages' caches.</a>
<a name="ln601">*/</a>
<a name="ln602">static inline void</a>
<a name="ln603">unmap_pages(VMArea* area, addr_t base, size_t size)</a>
<a name="ln604">{</a>
<a name="ln605">	area-&gt;address_space-&gt;TranslationMap()-&gt;UnmapPages(area, base, size, true);</a>
<a name="ln606">}</a>
<a name="ln607"> </a>
<a name="ln608"> </a>
<a name="ln609">/*!	Cuts a piece out of an area. If the given cut range covers the complete</a>
<a name="ln610">	area, it is deleted. If it covers the beginning or the end, the area is</a>
<a name="ln611">	resized accordingly. If the range covers some part in the middle of the</a>
<a name="ln612">	area, it is split in two; in this case the second area is returned via</a>
<a name="ln613">	\a _secondArea (the variable is left untouched in the other cases).</a>
<a name="ln614">	The address space must be write locked.</a>
<a name="ln615">	The caller must ensure that no part of the given range is wired.</a>
<a name="ln616">*/</a>
<a name="ln617">static status_t</a>
<a name="ln618">cut_area(VMAddressSpace* addressSpace, VMArea* area, addr_t address,</a>
<a name="ln619">	addr_t lastAddress, VMArea** _secondArea, bool kernel)</a>
<a name="ln620">{</a>
<a name="ln621">	// Does the cut range intersect with the area at all?</a>
<a name="ln622">	addr_t areaLast = area-&gt;Base() + (area-&gt;Size() - 1);</a>
<a name="ln623">	if (area-&gt;Base() &gt; lastAddress || areaLast &lt; address)</a>
<a name="ln624">		return B_OK;</a>
<a name="ln625"> </a>
<a name="ln626">	// Is the area fully covered?</a>
<a name="ln627">	if (area-&gt;Base() &gt;= address &amp;&amp; areaLast &lt;= lastAddress) {</a>
<a name="ln628">		delete_area(addressSpace, area, false);</a>
<a name="ln629">		return B_OK;</a>
<a name="ln630">	}</a>
<a name="ln631"> </a>
<a name="ln632">	int priority;</a>
<a name="ln633">	uint32 allocationFlags;</a>
<a name="ln634">	if (addressSpace == VMAddressSpace::Kernel()) {</a>
<a name="ln635">		priority = VM_PRIORITY_SYSTEM;</a>
<a name="ln636">		allocationFlags = HEAP_DONT_WAIT_FOR_MEMORY</a>
<a name="ln637">			| HEAP_DONT_LOCK_KERNEL_SPACE;</a>
<a name="ln638">	} else {</a>
<a name="ln639">		priority = VM_PRIORITY_USER;</a>
<a name="ln640">		allocationFlags = 0;</a>
<a name="ln641">	}</a>
<a name="ln642"> </a>
<a name="ln643">	VMCache* cache = vm_area_get_locked_cache(area);</a>
<a name="ln644">	VMCacheChainLocker cacheChainLocker(cache);</a>
<a name="ln645">	cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln646"> </a>
<a name="ln647">	// Cut the end only?</a>
<a name="ln648">	if (areaLast &lt;= lastAddress) {</a>
<a name="ln649">		size_t oldSize = area-&gt;Size();</a>
<a name="ln650">		size_t newSize = address - area-&gt;Base();</a>
<a name="ln651"> </a>
<a name="ln652">		status_t error = addressSpace-&gt;ShrinkAreaTail(area, newSize,</a>
<a name="ln653">			allocationFlags);</a>
<a name="ln654">		if (error != B_OK)</a>
<a name="ln655">			return error;</a>
<a name="ln656"> </a>
<a name="ln657">		// unmap pages</a>
<a name="ln658">		unmap_pages(area, address, oldSize - newSize);</a>
<a name="ln659"> </a>
<a name="ln660">		// If no one else uses the area's cache, we can resize it, too.</a>
<a name="ln661">		if (cache-&gt;areas == area &amp;&amp; area-&gt;cache_next == NULL</a>
<a name="ln662">			&amp;&amp; cache-&gt;consumers.IsEmpty()</a>
<a name="ln663">			&amp;&amp; cache-&gt;type == CACHE_TYPE_RAM) {</a>
<a name="ln664">			// Since VMCache::Resize() can temporarily drop the lock, we must</a>
<a name="ln665">			// unlock all lower caches to prevent locking order inversion.</a>
<a name="ln666">			cacheChainLocker.Unlock(cache);</a>
<a name="ln667">			cache-&gt;Resize(cache-&gt;virtual_base + newSize, priority);</a>
<a name="ln668">			cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln669">		}</a>
<a name="ln670"> </a>
<a name="ln671">		return B_OK;</a>
<a name="ln672">	}</a>
<a name="ln673"> </a>
<a name="ln674">	// Cut the beginning only?</a>
<a name="ln675">	if (area-&gt;Base() &gt;= address) {</a>
<a name="ln676">		addr_t oldBase = area-&gt;Base();</a>
<a name="ln677">		addr_t newBase = lastAddress + 1;</a>
<a name="ln678">		size_t newSize = areaLast - lastAddress;</a>
<a name="ln679"> </a>
<a name="ln680">		// unmap pages</a>
<a name="ln681">		unmap_pages(area, oldBase, newBase - oldBase);</a>
<a name="ln682"> </a>
<a name="ln683">		// resize the area</a>
<a name="ln684">		status_t error = addressSpace-&gt;ShrinkAreaHead(area, newSize,</a>
<a name="ln685">			allocationFlags);</a>
<a name="ln686">		if (error != B_OK)</a>
<a name="ln687">			return error;</a>
<a name="ln688"> </a>
<a name="ln689">		// TODO: If no one else uses the area's cache, we should resize it, too!</a>
<a name="ln690"> </a>
<a name="ln691">		area-&gt;cache_offset += newBase - oldBase;</a>
<a name="ln692"> </a>
<a name="ln693">		return B_OK;</a>
<a name="ln694">	}</a>
<a name="ln695"> </a>
<a name="ln696">	// The tough part -- cut a piece out of the middle of the area.</a>
<a name="ln697">	// We do that by shrinking the area to the begin section and creating a</a>
<a name="ln698">	// new area for the end section.</a>
<a name="ln699"> </a>
<a name="ln700">	addr_t firstNewSize = address - area-&gt;Base();</a>
<a name="ln701">	addr_t secondBase = lastAddress + 1;</a>
<a name="ln702">	addr_t secondSize = areaLast - lastAddress;</a>
<a name="ln703"> </a>
<a name="ln704">	// unmap pages</a>
<a name="ln705">	unmap_pages(area, address, area-&gt;Size() - firstNewSize);</a>
<a name="ln706"> </a>
<a name="ln707">	// resize the area</a>
<a name="ln708">	addr_t oldSize = area-&gt;Size();</a>
<a name="ln709">	status_t error = addressSpace-&gt;ShrinkAreaTail(area, firstNewSize,</a>
<a name="ln710">		allocationFlags);</a>
<a name="ln711">	if (error != B_OK)</a>
<a name="ln712">		return error;</a>
<a name="ln713"> </a>
<a name="ln714">	// TODO: If no one else uses the area's cache, we might want to create a</a>
<a name="ln715">	// new cache for the second area, transfer the concerned pages from the</a>
<a name="ln716">	// first cache to it and resize the first cache.</a>
<a name="ln717"> </a>
<a name="ln718">	// map the second area</a>
<a name="ln719">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln720">	addressRestrictions.address = (void*)secondBase;</a>
<a name="ln721">	addressRestrictions.address_specification = B_EXACT_ADDRESS;</a>
<a name="ln722">	VMArea* secondArea;</a>
<a name="ln723">	error = map_backing_store(addressSpace, cache,</a>
<a name="ln724">		area-&gt;cache_offset + (secondBase - area-&gt;Base()), area-&gt;name,</a>
<a name="ln725">		secondSize, area-&gt;wiring, area-&gt;protection, REGION_NO_PRIVATE_MAP, 0,</a>
<a name="ln726">		&amp;addressRestrictions, kernel, &amp;secondArea, NULL);</a>
<a name="ln727">	if (error != B_OK) {</a>
<a name="ln728">		addressSpace-&gt;ShrinkAreaTail(area, oldSize, allocationFlags);</a>
<a name="ln729">		return error;</a>
<a name="ln730">	}</a>
<a name="ln731"> </a>
<a name="ln732">	// We need a cache reference for the new area.</a>
<a name="ln733">	cache-&gt;AcquireRefLocked();</a>
<a name="ln734"> </a>
<a name="ln735">	if (_secondArea != NULL)</a>
<a name="ln736">		*_secondArea = secondArea;</a>
<a name="ln737"> </a>
<a name="ln738">	return B_OK;</a>
<a name="ln739">}</a>
<a name="ln740"> </a>
<a name="ln741"> </a>
<a name="ln742">/*!	Deletes all areas in the given address range.</a>
<a name="ln743">	The address space must be write-locked.</a>
<a name="ln744">	The caller must ensure that no part of the given range is wired.</a>
<a name="ln745">*/</a>
<a name="ln746">static status_t</a>
<a name="ln747">unmap_address_range(VMAddressSpace* addressSpace, addr_t address, addr_t size,</a>
<a name="ln748">	bool kernel)</a>
<a name="ln749">{</a>
<a name="ln750">	size = PAGE_ALIGN(size);</a>
<a name="ln751">	addr_t lastAddress = address + (size - 1);</a>
<a name="ln752"> </a>
<a name="ln753">	// Check, whether the caller is allowed to modify the concerned areas.</a>
<a name="ln754">	if (!kernel) {</a>
<a name="ln755">		for (VMAddressSpace::AreaIterator it = addressSpace-&gt;GetAreaIterator();</a>
<a name="ln756">				VMArea* area = it.Next();) {</a>
<a name="ln757">			addr_t areaLast = area-&gt;Base() + (area-&gt;Size() - 1);</a>
<a name="ln758">			if (area-&gt;Base() &lt; lastAddress &amp;&amp; address &lt; areaLast) {</a>
<a name="ln759">				if (area-&gt;address_space == VMAddressSpace::Kernel()) {</a>
<a name="ln760">					dprintf(&quot;unmap_address_range: team %&quot; B_PRId32 &quot; tried to &quot;</a>
<a name="ln761">						&quot;unmap range of kernel area %&quot; B_PRId32 &quot; (%s)\n&quot;,</a>
<a name="ln762">						team_get_current_team_id(), area-&gt;id, area-&gt;name);</a>
<a name="ln763">					return B_NOT_ALLOWED;</a>
<a name="ln764">				}</a>
<a name="ln765">			}</a>
<a name="ln766">		}</a>
<a name="ln767">	}</a>
<a name="ln768"> </a>
<a name="ln769">	for (VMAddressSpace::AreaIterator it = addressSpace-&gt;GetAreaIterator();</a>
<a name="ln770">			VMArea* area = it.Next();) {</a>
<a name="ln771">		addr_t areaLast = area-&gt;Base() + (area-&gt;Size() - 1);</a>
<a name="ln772">		if (area-&gt;Base() &lt; lastAddress &amp;&amp; address &lt; areaLast) {</a>
<a name="ln773">			status_t error = cut_area(addressSpace, area, address,</a>
<a name="ln774">				lastAddress, NULL, kernel);</a>
<a name="ln775">			if (error != B_OK)</a>
<a name="ln776">				return error;</a>
<a name="ln777">				// Failing after already messing with areas is ugly, but we</a>
<a name="ln778">				// can't do anything about it.</a>
<a name="ln779">		}</a>
<a name="ln780">	}</a>
<a name="ln781"> </a>
<a name="ln782">	return B_OK;</a>
<a name="ln783">}</a>
<a name="ln784"> </a>
<a name="ln785"> </a>
<a name="ln786">/*! You need to hold the lock of the cache and the write lock of the address</a>
<a name="ln787">	space when calling this function.</a>
<a name="ln788">	Note, that in case of error your cache will be temporarily unlocked.</a>
<a name="ln789">	If \a addressSpec is \c B_EXACT_ADDRESS and the</a>
<a name="ln790">	\c CREATE_AREA_UNMAP_ADDRESS_RANGE flag is specified, the caller must ensure</a>
<a name="ln791">	that no part of the specified address range (base \c *_virtualAddress, size</a>
<a name="ln792">	\a size) is wired.</a>
<a name="ln793">*/</a>
<a name="ln794">static status_t</a>
<a name="ln795">map_backing_store(VMAddressSpace* addressSpace, VMCache* cache, off_t offset,</a>
<a name="ln796">	const char* areaName, addr_t size, int wiring, int protection, int mapping,</a>
<a name="ln797">	uint32 flags, const virtual_address_restrictions* addressRestrictions,</a>
<a name="ln798">	bool kernel, VMArea** _area, void** _virtualAddress)</a>
<a name="ln799">{</a>
<a name="ln800">	TRACE((&quot;map_backing_store: aspace %p, cache %p, virtual %p, offset 0x%&quot;</a>
<a name="ln801">		B_PRIx64 &quot;, size %&quot; B_PRIuADDR &quot;, addressSpec %&quot; B_PRIu32 &quot;, wiring %d&quot;</a>
<a name="ln802">		&quot;, protection %d, area %p, areaName '%s'\n&quot;, addressSpace, cache,</a>
<a name="ln803">		addressRestrictions-&gt;address, offset, size,</a>
<a name="ln804">		addressRestrictions-&gt;address_specification, wiring, protection,</a>
<a name="ln805">		_area, areaName));</a>
<a name="ln806">	cache-&gt;AssertLocked();</a>
<a name="ln807"> </a>
<a name="ln808">	if (size == 0) {</a>
<a name="ln809">#if KDEBUG</a>
<a name="ln810">		panic(&quot;map_backing_store(): called with size=0 for area '%s'!&quot;,</a>
<a name="ln811">			areaName);</a>
<a name="ln812">#endif</a>
<a name="ln813">		return B_BAD_VALUE;</a>
<a name="ln814">	}</a>
<a name="ln815"> </a>
<a name="ln816">	uint32 allocationFlags = HEAP_DONT_WAIT_FOR_MEMORY</a>
<a name="ln817">		| HEAP_DONT_LOCK_KERNEL_SPACE;</a>
<a name="ln818">	int priority;</a>
<a name="ln819">	if (addressSpace != VMAddressSpace::Kernel()) {</a>
<a name="ln820">		priority = VM_PRIORITY_USER;</a>
<a name="ln821">	} else if ((flags &amp; CREATE_AREA_PRIORITY_VIP) != 0) {</a>
<a name="ln822">		priority = VM_PRIORITY_VIP;</a>
<a name="ln823">		allocationFlags |= HEAP_PRIORITY_VIP;</a>
<a name="ln824">	} else</a>
<a name="ln825">		priority = VM_PRIORITY_SYSTEM;</a>
<a name="ln826"> </a>
<a name="ln827">	VMArea* area = addressSpace-&gt;CreateArea(areaName, wiring, protection,</a>
<a name="ln828">		allocationFlags);</a>
<a name="ln829">	if (area == NULL)</a>
<a name="ln830">		return B_NO_MEMORY;</a>
<a name="ln831"> </a>
<a name="ln832">	status_t status;</a>
<a name="ln833"> </a>
<a name="ln834">	// if this is a private map, we need to create a new cache</a>
<a name="ln835">	// to handle the private copies of pages as they are written to</a>
<a name="ln836">	VMCache* sourceCache = cache;</a>
<a name="ln837">	if (mapping == REGION_PRIVATE_MAP) {</a>
<a name="ln838">		VMCache* newCache;</a>
<a name="ln839"> </a>
<a name="ln840">		// create an anonymous cache</a>
<a name="ln841">		status = VMCacheFactory::CreateAnonymousCache(newCache,</a>
<a name="ln842">			(protection &amp; B_STACK_AREA) != 0</a>
<a name="ln843">				|| (protection &amp; B_OVERCOMMITTING_AREA) != 0, 0,</a>
<a name="ln844">			cache-&gt;GuardSize() / B_PAGE_SIZE, true, VM_PRIORITY_USER);</a>
<a name="ln845">		if (status != B_OK)</a>
<a name="ln846">			goto err1;</a>
<a name="ln847"> </a>
<a name="ln848">		newCache-&gt;Lock();</a>
<a name="ln849">		newCache-&gt;temporary = 1;</a>
<a name="ln850">		newCache-&gt;virtual_base = offset;</a>
<a name="ln851">		newCache-&gt;virtual_end = offset + size;</a>
<a name="ln852"> </a>
<a name="ln853">		cache-&gt;AddConsumer(newCache);</a>
<a name="ln854"> </a>
<a name="ln855">		cache = newCache;</a>
<a name="ln856">	}</a>
<a name="ln857"> </a>
<a name="ln858">	if ((flags &amp; CREATE_AREA_DONT_COMMIT_MEMORY) == 0) {</a>
<a name="ln859">		status = cache-&gt;SetMinimalCommitment(size, priority);</a>
<a name="ln860">		if (status != B_OK)</a>
<a name="ln861">			goto err2;</a>
<a name="ln862">	}</a>
<a name="ln863"> </a>
<a name="ln864">	// check to see if this address space has entered DELETE state</a>
<a name="ln865">	if (addressSpace-&gt;IsBeingDeleted()) {</a>
<a name="ln866">		// okay, someone is trying to delete this address space now, so we can't</a>
<a name="ln867">		// insert the area, so back out</a>
<a name="ln868">		status = B_BAD_TEAM_ID;</a>
<a name="ln869">		goto err2;</a>
<a name="ln870">	}</a>
<a name="ln871"> </a>
<a name="ln872">	if (addressRestrictions-&gt;address_specification == B_EXACT_ADDRESS</a>
<a name="ln873">			&amp;&amp; (flags &amp; CREATE_AREA_UNMAP_ADDRESS_RANGE) != 0) {</a>
<a name="ln874">		status = unmap_address_range(addressSpace,</a>
<a name="ln875">			(addr_t)addressRestrictions-&gt;address, size, kernel);</a>
<a name="ln876">		if (status != B_OK)</a>
<a name="ln877">			goto err2;</a>
<a name="ln878">	}</a>
<a name="ln879"> </a>
<a name="ln880">	status = addressSpace-&gt;InsertArea(area, size, addressRestrictions,</a>
<a name="ln881">		allocationFlags, _virtualAddress);</a>
<a name="ln882">	if (status == B_NO_MEMORY</a>
<a name="ln883">			&amp;&amp; addressRestrictions-&gt;address_specification == B_ANY_KERNEL_ADDRESS) {</a>
<a name="ln884">		// TODO: At present, there is no way to notify the low_resource monitor</a>
<a name="ln885">		// that kernel addresss space is fragmented, nor does it check for this</a>
<a name="ln886">		// automatically. Due to how many locks are held, we cannot wait here</a>
<a name="ln887">		// for space to be freed up, but it would be good to at least notify</a>
<a name="ln888">		// that we tried and failed to allocate some amount.</a>
<a name="ln889">	}</a>
<a name="ln890">	if (status != B_OK)</a>
<a name="ln891">		goto err2;</a>
<a name="ln892"> </a>
<a name="ln893">	// attach the cache to the area</a>
<a name="ln894">	area-&gt;cache = cache;</a>
<a name="ln895">	area-&gt;cache_offset = offset;</a>
<a name="ln896"> </a>
<a name="ln897">	// point the cache back to the area</a>
<a name="ln898">	cache-&gt;InsertAreaLocked(area);</a>
<a name="ln899">	if (mapping == REGION_PRIVATE_MAP)</a>
<a name="ln900">		cache-&gt;Unlock();</a>
<a name="ln901"> </a>
<a name="ln902">	// insert the area in the global area hash table</a>
<a name="ln903">	VMAreaHash::Insert(area);</a>
<a name="ln904"> </a>
<a name="ln905">	// grab a ref to the address space (the area holds this)</a>
<a name="ln906">	addressSpace-&gt;Get();</a>
<a name="ln907"> </a>
<a name="ln908">//	ktrace_printf(&quot;map_backing_store: cache: %p (source: %p), \&quot;%s\&quot; -&gt; %p&quot;,</a>
<a name="ln909">//		cache, sourceCache, areaName, area);</a>
<a name="ln910"> </a>
<a name="ln911">	*_area = area;</a>
<a name="ln912">	return B_OK;</a>
<a name="ln913"> </a>
<a name="ln914">err2:</a>
<a name="ln915">	if (mapping == REGION_PRIVATE_MAP) {</a>
<a name="ln916">		// We created this cache, so we must delete it again. Note, that we</a>
<a name="ln917">		// need to temporarily unlock the source cache or we'll otherwise</a>
<a name="ln918">		// deadlock, since VMCache::_RemoveConsumer() will try to lock it, too.</a>
<a name="ln919">		sourceCache-&gt;Unlock();</a>
<a name="ln920">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln921">		sourceCache-&gt;Lock();</a>
<a name="ln922">	}</a>
<a name="ln923">err1:</a>
<a name="ln924">	addressSpace-&gt;DeleteArea(area, allocationFlags);</a>
<a name="ln925">	return status;</a>
<a name="ln926">}</a>
<a name="ln927"> </a>
<a name="ln928"> </a>
<a name="ln929">/*!	Equivalent to wait_if_area_range_is_wired(area, area-&gt;Base(), area-&gt;Size(),</a>
<a name="ln930">	  locker1, locker2).</a>
<a name="ln931">*/</a>
<a name="ln932">template&lt;typename LockerType1, typename LockerType2&gt;</a>
<a name="ln933">static inline bool</a>
<a name="ln934">wait_if_area_is_wired(VMArea* area, LockerType1* locker1, LockerType2* locker2)</a>
<a name="ln935">{</a>
<a name="ln936">	area-&gt;cache-&gt;AssertLocked();</a>
<a name="ln937"> </a>
<a name="ln938">	VMAreaUnwiredWaiter waiter;</a>
<a name="ln939">	if (!area-&gt;AddWaiterIfWired(&amp;waiter))</a>
<a name="ln940">		return false;</a>
<a name="ln941"> </a>
<a name="ln942">	// unlock everything and wait</a>
<a name="ln943">	if (locker1 != NULL)</a>
<a name="ln944">		locker1-&gt;Unlock();</a>
<a name="ln945">	if (locker2 != NULL)</a>
<a name="ln946">		locker2-&gt;Unlock();</a>
<a name="ln947"> </a>
<a name="ln948">	waiter.waitEntry.Wait();</a>
<a name="ln949"> </a>
<a name="ln950">	return true;</a>
<a name="ln951">}</a>
<a name="ln952"> </a>
<a name="ln953"> </a>
<a name="ln954">/*!	Checks whether the given area has any wired ranges intersecting with the</a>
<a name="ln955">	specified range and waits, if so.</a>
<a name="ln956"> </a>
<a name="ln957">	When it has to wait, the function calls \c Unlock() on both \a locker1</a>
<a name="ln958">	and \a locker2, if given.</a>
<a name="ln959">	The area's top cache must be locked and must be unlocked as a side effect</a>
<a name="ln960">	of calling \c Unlock() on either \a locker1 or \a locker2.</a>
<a name="ln961"> </a>
<a name="ln962">	If the function does not have to wait it does not modify or unlock any</a>
<a name="ln963">	object.</a>
<a name="ln964"> </a>
<a name="ln965">	\param area The area to be checked.</a>
<a name="ln966">	\param base The base address of the range to check.</a>
<a name="ln967">	\param size The size of the address range to check.</a>
<a name="ln968">	\param locker1 An object to be unlocked when before starting to wait (may</a>
<a name="ln969">		be \c NULL).</a>
<a name="ln970">	\param locker2 An object to be unlocked when before starting to wait (may</a>
<a name="ln971">		be \c NULL).</a>
<a name="ln972">	\return \c true, if the function had to wait, \c false otherwise.</a>
<a name="ln973">*/</a>
<a name="ln974">template&lt;typename LockerType1, typename LockerType2&gt;</a>
<a name="ln975">static inline bool</a>
<a name="ln976">wait_if_area_range_is_wired(VMArea* area, addr_t base, size_t size,</a>
<a name="ln977">	LockerType1* locker1, LockerType2* locker2)</a>
<a name="ln978">{</a>
<a name="ln979">	area-&gt;cache-&gt;AssertLocked();</a>
<a name="ln980"> </a>
<a name="ln981">	VMAreaUnwiredWaiter waiter;</a>
<a name="ln982">	if (!area-&gt;AddWaiterIfWired(&amp;waiter, base, size))</a>
<a name="ln983">		return false;</a>
<a name="ln984"> </a>
<a name="ln985">	// unlock everything and wait</a>
<a name="ln986">	if (locker1 != NULL)</a>
<a name="ln987">		locker1-&gt;Unlock();</a>
<a name="ln988">	if (locker2 != NULL)</a>
<a name="ln989">		locker2-&gt;Unlock();</a>
<a name="ln990"> </a>
<a name="ln991">	waiter.waitEntry.Wait();</a>
<a name="ln992"> </a>
<a name="ln993">	return true;</a>
<a name="ln994">}</a>
<a name="ln995"> </a>
<a name="ln996"> </a>
<a name="ln997">/*!	Checks whether the given address space has any wired ranges intersecting</a>
<a name="ln998">	with the specified range and waits, if so.</a>
<a name="ln999"> </a>
<a name="ln1000">	Similar to wait_if_area_range_is_wired(), with the following differences:</a>
<a name="ln1001">	- All areas intersecting with the range are checked (respectively all until</a>
<a name="ln1002">	  one is found that contains a wired range intersecting with the given</a>
<a name="ln1003">	  range).</a>
<a name="ln1004">	- The given address space must at least be read-locked and must be unlocked</a>
<a name="ln1005">	  when \c Unlock() is called on \a locker.</a>
<a name="ln1006">	- None of the areas' caches are allowed to be locked.</a>
<a name="ln1007">*/</a>
<a name="ln1008">template&lt;typename LockerType&gt;</a>
<a name="ln1009">static inline bool</a>
<a name="ln1010">wait_if_address_range_is_wired(VMAddressSpace* addressSpace, addr_t base,</a>
<a name="ln1011">	size_t size, LockerType* locker)</a>
<a name="ln1012">{</a>
<a name="ln1013">	addr_t end = base + size - 1;</a>
<a name="ln1014">	for (VMAddressSpace::AreaIterator it = addressSpace-&gt;GetAreaIterator();</a>
<a name="ln1015">			VMArea* area = it.Next();) {</a>
<a name="ln1016">		// TODO: Introduce a VMAddressSpace method to get a close iterator!</a>
<a name="ln1017">		if (area-&gt;Base() &gt; end)</a>
<a name="ln1018">			return false;</a>
<a name="ln1019"> </a>
<a name="ln1020">		if (base &gt;= area-&gt;Base() + area-&gt;Size() - 1)</a>
<a name="ln1021">			continue;</a>
<a name="ln1022"> </a>
<a name="ln1023">		AreaCacheLocker cacheLocker(vm_area_get_locked_cache(area));</a>
<a name="ln1024"> </a>
<a name="ln1025">		if (wait_if_area_range_is_wired(area, base, size, locker, &amp;cacheLocker))</a>
<a name="ln1026">			return true;</a>
<a name="ln1027">	}</a>
<a name="ln1028"> </a>
<a name="ln1029">	return false;</a>
<a name="ln1030">}</a>
<a name="ln1031"> </a>
<a name="ln1032"> </a>
<a name="ln1033">/*!	Prepares an area to be used for vm_set_kernel_area_debug_protection().</a>
<a name="ln1034">	It must be called in a situation where the kernel address space may be</a>
<a name="ln1035">	locked.</a>
<a name="ln1036">*/</a>
<a name="ln1037">status_t</a>
<a name="ln1038">vm_prepare_kernel_area_debug_protection(area_id id, void** cookie)</a>
<a name="ln1039">{</a>
<a name="ln1040">	AddressSpaceReadLocker locker;</a>
<a name="ln1041">	VMArea* area;</a>
<a name="ln1042">	status_t status = locker.SetFromArea(id, area);</a>
<a name="ln1043">	if (status != B_OK)</a>
<a name="ln1044">		return status;</a>
<a name="ln1045"> </a>
<a name="ln1046">	if (area-&gt;page_protections == NULL) {</a>
<a name="ln1047">		status = allocate_area_page_protections(area);</a>
<a name="ln1048">		if (status != B_OK)</a>
<a name="ln1049">			return status;</a>
<a name="ln1050">	}</a>
<a name="ln1051"> </a>
<a name="ln1052">	*cookie = (void*)area;</a>
<a name="ln1053">	return B_OK;</a>
<a name="ln1054">}</a>
<a name="ln1055"> </a>
<a name="ln1056"> </a>
<a name="ln1057">/*!	This is a debug helper function that can only be used with very specific</a>
<a name="ln1058">	use cases.</a>
<a name="ln1059">	Sets protection for the given address range to the protection specified.</a>
<a name="ln1060">	If \a protection is 0 then the involved pages will be marked non-present</a>
<a name="ln1061">	in the translation map to cause a fault on access. The pages aren't</a>
<a name="ln1062">	actually unmapped however so that they can be marked present again with</a>
<a name="ln1063">	additional calls to this function. For this to work the area must be</a>
<a name="ln1064">	fully locked in memory so that the pages aren't otherwise touched.</a>
<a name="ln1065">	This function does not lock the kernel address space and needs to be</a>
<a name="ln1066">	supplied with a \a cookie retrieved from a successful call to</a>
<a name="ln1067">	vm_prepare_kernel_area_debug_protection().</a>
<a name="ln1068">*/</a>
<a name="ln1069">status_t</a>
<a name="ln1070">vm_set_kernel_area_debug_protection(void* cookie, void* _address, size_t size,</a>
<a name="ln1071">	uint32 protection)</a>
<a name="ln1072">{</a>
<a name="ln1073">	// check address range</a>
<a name="ln1074">	addr_t address = (addr_t)_address;</a>
<a name="ln1075">	size = PAGE_ALIGN(size);</a>
<a name="ln1076"> </a>
<a name="ln1077">	if ((address % B_PAGE_SIZE) != 0</a>
<a name="ln1078">		|| (addr_t)address + size &lt; (addr_t)address</a>
<a name="ln1079">		|| !IS_KERNEL_ADDRESS(address)</a>
<a name="ln1080">		|| !IS_KERNEL_ADDRESS((addr_t)address + size)) {</a>
<a name="ln1081">		return B_BAD_VALUE;</a>
<a name="ln1082">	}</a>
<a name="ln1083"> </a>
<a name="ln1084">	// Translate the kernel protection to user protection as we only store that.</a>
<a name="ln1085">	if ((protection &amp; B_KERNEL_READ_AREA) != 0)</a>
<a name="ln1086">		protection |= B_READ_AREA;</a>
<a name="ln1087">	if ((protection &amp; B_KERNEL_WRITE_AREA) != 0)</a>
<a name="ln1088">		protection |= B_WRITE_AREA;</a>
<a name="ln1089"> </a>
<a name="ln1090">	VMAddressSpace* addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln1091">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln1092">	VMArea* area = (VMArea*)cookie;</a>
<a name="ln1093"> </a>
<a name="ln1094">	addr_t offset = address - area-&gt;Base();</a>
<a name="ln1095">	if (area-&gt;Size() - offset &lt; size) {</a>
<a name="ln1096">		panic(&quot;protect range not fully within supplied area&quot;);</a>
<a name="ln1097">		return B_BAD_VALUE;</a>
<a name="ln1098">	}</a>
<a name="ln1099"> </a>
<a name="ln1100">	if (area-&gt;page_protections == NULL) {</a>
<a name="ln1101">		panic(&quot;area has no page protections&quot;);</a>
<a name="ln1102">		return B_BAD_VALUE;</a>
<a name="ln1103">	}</a>
<a name="ln1104"> </a>
<a name="ln1105">	// Invalidate the mapping entries so any access to them will fault or</a>
<a name="ln1106">	// restore the mapping entries unchanged so that lookup will success again.</a>
<a name="ln1107">	map-&gt;Lock();</a>
<a name="ln1108">	map-&gt;DebugMarkRangePresent(address, address + size, protection != 0);</a>
<a name="ln1109">	map-&gt;Unlock();</a>
<a name="ln1110"> </a>
<a name="ln1111">	// And set the proper page protections so that the fault case will actually</a>
<a name="ln1112">	// fail and not simply try to map a new page.</a>
<a name="ln1113">	for (addr_t pageAddress = address; pageAddress &lt; address + size;</a>
<a name="ln1114">			pageAddress += B_PAGE_SIZE) {</a>
<a name="ln1115">		set_area_page_protection(area, pageAddress, protection);</a>
<a name="ln1116">	}</a>
<a name="ln1117"> </a>
<a name="ln1118">	return B_OK;</a>
<a name="ln1119">}</a>
<a name="ln1120"> </a>
<a name="ln1121"> </a>
<a name="ln1122">status_t</a>
<a name="ln1123">vm_block_address_range(const char* name, void* address, addr_t size)</a>
<a name="ln1124">{</a>
<a name="ln1125">	if (!arch_vm_supports_protection(0))</a>
<a name="ln1126">		return B_NOT_SUPPORTED;</a>
<a name="ln1127"> </a>
<a name="ln1128">	AddressSpaceWriteLocker locker;</a>
<a name="ln1129">	status_t status = locker.SetTo(VMAddressSpace::KernelID());</a>
<a name="ln1130">	if (status != B_OK)</a>
<a name="ln1131">		return status;</a>
<a name="ln1132"> </a>
<a name="ln1133">	VMAddressSpace* addressSpace = locker.AddressSpace();</a>
<a name="ln1134"> </a>
<a name="ln1135">	// create an anonymous cache</a>
<a name="ln1136">	VMCache* cache;</a>
<a name="ln1137">	status = VMCacheFactory::CreateAnonymousCache(cache, false, 0, 0, false,</a>
<a name="ln1138">		VM_PRIORITY_SYSTEM);</a>
<a name="ln1139">	if (status != B_OK)</a>
<a name="ln1140">		return status;</a>
<a name="ln1141"> </a>
<a name="ln1142">	cache-&gt;temporary = 1;</a>
<a name="ln1143">	cache-&gt;virtual_end = size;</a>
<a name="ln1144">	cache-&gt;Lock();</a>
<a name="ln1145"> </a>
<a name="ln1146">	VMArea* area;</a>
<a name="ln1147">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1148">	addressRestrictions.address = address;</a>
<a name="ln1149">	addressRestrictions.address_specification = B_EXACT_ADDRESS;</a>
<a name="ln1150">	status = map_backing_store(addressSpace, cache, 0, name, size,</a>
<a name="ln1151">		B_ALREADY_WIRED, 0, REGION_NO_PRIVATE_MAP, 0, &amp;addressRestrictions,</a>
<a name="ln1152">		true, &amp;area, NULL);</a>
<a name="ln1153">	if (status != B_OK) {</a>
<a name="ln1154">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln1155">		return status;</a>
<a name="ln1156">	}</a>
<a name="ln1157"> </a>
<a name="ln1158">	cache-&gt;Unlock();</a>
<a name="ln1159">	area-&gt;cache_type = CACHE_TYPE_RAM;</a>
<a name="ln1160">	return area-&gt;id;</a>
<a name="ln1161">}</a>
<a name="ln1162"> </a>
<a name="ln1163"> </a>
<a name="ln1164">status_t</a>
<a name="ln1165">vm_unreserve_address_range(team_id team, void* address, addr_t size)</a>
<a name="ln1166">{</a>
<a name="ln1167">	AddressSpaceWriteLocker locker(team);</a>
<a name="ln1168">	if (!locker.IsLocked())</a>
<a name="ln1169">		return B_BAD_TEAM_ID;</a>
<a name="ln1170"> </a>
<a name="ln1171">	VMAddressSpace* addressSpace = locker.AddressSpace();</a>
<a name="ln1172">	return addressSpace-&gt;UnreserveAddressRange((addr_t)address, size,</a>
<a name="ln1173">		addressSpace == VMAddressSpace::Kernel()</a>
<a name="ln1174">			? HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE : 0);</a>
<a name="ln1175">}</a>
<a name="ln1176"> </a>
<a name="ln1177"> </a>
<a name="ln1178">status_t</a>
<a name="ln1179">vm_reserve_address_range(team_id team, void** _address, uint32 addressSpec,</a>
<a name="ln1180">	addr_t size, uint32 flags)</a>
<a name="ln1181">{</a>
<a name="ln1182">	if (size == 0)</a>
<a name="ln1183">		return B_BAD_VALUE;</a>
<a name="ln1184"> </a>
<a name="ln1185">	AddressSpaceWriteLocker locker(team);</a>
<a name="ln1186">	if (!locker.IsLocked())</a>
<a name="ln1187">		return B_BAD_TEAM_ID;</a>
<a name="ln1188"> </a>
<a name="ln1189">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1190">	addressRestrictions.address = *_address;</a>
<a name="ln1191">	addressRestrictions.address_specification = addressSpec;</a>
<a name="ln1192">	VMAddressSpace* addressSpace = locker.AddressSpace();</a>
<a name="ln1193">	return addressSpace-&gt;ReserveAddressRange(size, &amp;addressRestrictions, flags,</a>
<a name="ln1194">		addressSpace == VMAddressSpace::Kernel()</a>
<a name="ln1195">			? HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE : 0,</a>
<a name="ln1196">		_address);</a>
<a name="ln1197">}</a>
<a name="ln1198"> </a>
<a name="ln1199"> </a>
<a name="ln1200">area_id</a>
<a name="ln1201">vm_create_anonymous_area(team_id team, const char *name, addr_t size,</a>
<a name="ln1202">	uint32 wiring, uint32 protection, uint32 flags, addr_t guardSize,</a>
<a name="ln1203">	const virtual_address_restrictions* virtualAddressRestrictions,</a>
<a name="ln1204">	const physical_address_restrictions* physicalAddressRestrictions,</a>
<a name="ln1205">	bool kernel, void** _address)</a>
<a name="ln1206">{</a>
<a name="ln1207">	VMArea* area;</a>
<a name="ln1208">	VMCache* cache;</a>
<a name="ln1209">	vm_page* page = NULL;</a>
<a name="ln1210">	bool isStack = (protection &amp; B_STACK_AREA) != 0;</a>
<a name="ln1211">	page_num_t guardPages;</a>
<a name="ln1212">	bool canOvercommit = false;</a>
<a name="ln1213">	uint32 pageAllocFlags = (flags &amp; CREATE_AREA_DONT_CLEAR) == 0</a>
<a name="ln1214">		? VM_PAGE_ALLOC_CLEAR : 0;</a>
<a name="ln1215"> </a>
<a name="ln1216">	TRACE((&quot;create_anonymous_area [%&quot; B_PRId32 &quot;] %s: size 0x%&quot; B_PRIxADDR &quot;\n&quot;,</a>
<a name="ln1217">		team, name, size));</a>
<a name="ln1218"> </a>
<a name="ln1219">	size = PAGE_ALIGN(size);</a>
<a name="ln1220">	guardSize = PAGE_ALIGN(guardSize);</a>
<a name="ln1221">	guardPages = guardSize / B_PAGE_SIZE;</a>
<a name="ln1222"> </a>
<a name="ln1223">	if (size == 0 || size &lt; guardSize)</a>
<a name="ln1224">		return B_BAD_VALUE;</a>
<a name="ln1225">	if (!arch_vm_supports_protection(protection))</a>
<a name="ln1226">		return B_NOT_SUPPORTED;</a>
<a name="ln1227"> </a>
<a name="ln1228">	if (isStack || (protection &amp; B_OVERCOMMITTING_AREA) != 0)</a>
<a name="ln1229">		canOvercommit = true;</a>
<a name="ln1230"> </a>
<a name="ln1231">#ifdef DEBUG_KERNEL_STACKS</a>
<a name="ln1232">	if ((protection &amp; B_KERNEL_STACK_AREA) != 0)</a>
<a name="ln1233">		isStack = true;</a>
<a name="ln1234">#endif</a>
<a name="ln1235"> </a>
<a name="ln1236">	// check parameters</a>
<a name="ln1237">	switch (virtualAddressRestrictions-&gt;address_specification) {</a>
<a name="ln1238">		case B_ANY_ADDRESS:</a>
<a name="ln1239">		case B_EXACT_ADDRESS:</a>
<a name="ln1240">		case B_BASE_ADDRESS:</a>
<a name="ln1241">		case B_ANY_KERNEL_ADDRESS:</a>
<a name="ln1242">		case B_ANY_KERNEL_BLOCK_ADDRESS:</a>
<a name="ln1243">		case B_RANDOMIZED_ANY_ADDRESS:</a>
<a name="ln1244">		case B_RANDOMIZED_BASE_ADDRESS:</a>
<a name="ln1245">			break;</a>
<a name="ln1246"> </a>
<a name="ln1247">		default:</a>
<a name="ln1248">			return B_BAD_VALUE;</a>
<a name="ln1249">	}</a>
<a name="ln1250"> </a>
<a name="ln1251">	// If low or high physical address restrictions are given, we force</a>
<a name="ln1252">	// B_CONTIGUOUS wiring, since only then we'll use</a>
<a name="ln1253">	// vm_page_allocate_page_run() which deals with those restrictions.</a>
<a name="ln1254">	if (physicalAddressRestrictions-&gt;low_address != 0</a>
<a name="ln1255">		|| physicalAddressRestrictions-&gt;high_address != 0) {</a>
<a name="ln1256">		wiring = B_CONTIGUOUS;</a>
<a name="ln1257">	}</a>
<a name="ln1258"> </a>
<a name="ln1259">	physical_address_restrictions stackPhysicalRestrictions;</a>
<a name="ln1260">	bool doReserveMemory = false;</a>
<a name="ln1261">	switch (wiring) {</a>
<a name="ln1262">		case B_NO_LOCK:</a>
<a name="ln1263">			break;</a>
<a name="ln1264">		case B_FULL_LOCK:</a>
<a name="ln1265">		case B_LAZY_LOCK:</a>
<a name="ln1266">		case B_CONTIGUOUS:</a>
<a name="ln1267">			doReserveMemory = true;</a>
<a name="ln1268">			break;</a>
<a name="ln1269">		case B_ALREADY_WIRED:</a>
<a name="ln1270">			break;</a>
<a name="ln1271">		case B_LOMEM:</a>
<a name="ln1272">			stackPhysicalRestrictions = *physicalAddressRestrictions;</a>
<a name="ln1273">			stackPhysicalRestrictions.high_address = 16 * 1024 * 1024;</a>
<a name="ln1274">			physicalAddressRestrictions = &amp;stackPhysicalRestrictions;</a>
<a name="ln1275">			wiring = B_CONTIGUOUS;</a>
<a name="ln1276">			doReserveMemory = true;</a>
<a name="ln1277">			break;</a>
<a name="ln1278">		case B_32_BIT_FULL_LOCK:</a>
<a name="ln1279">			if (B_HAIKU_PHYSICAL_BITS &lt;= 32</a>
<a name="ln1280">				|| (uint64)vm_page_max_address() &lt; (uint64)1 &lt;&lt; 32) {</a>
<a name="ln1281">				wiring = B_FULL_LOCK;</a>
<a name="ln1282">				doReserveMemory = true;</a>
<a name="ln1283">				break;</a>
<a name="ln1284">			}</a>
<a name="ln1285">			// TODO: We don't really support this mode efficiently. Just fall</a>
<a name="ln1286">			// through for now ...</a>
<a name="ln1287">		case B_32_BIT_CONTIGUOUS:</a>
<a name="ln1288">			#if B_HAIKU_PHYSICAL_BITS &gt; 32</a>
<a name="ln1289">				if (vm_page_max_address() &gt;= (phys_addr_t)1 &lt;&lt; 32) {</a>
<a name="ln1290">					stackPhysicalRestrictions = *physicalAddressRestrictions;</a>
<a name="ln1291">					stackPhysicalRestrictions.high_address</a>
<a name="ln1292">						= (phys_addr_t)1 &lt;&lt; 32;</a>
<a name="ln1293">					physicalAddressRestrictions = &amp;stackPhysicalRestrictions;</a>
<a name="ln1294">				}</a>
<a name="ln1295">			#endif</a>
<a name="ln1296">			wiring = B_CONTIGUOUS;</a>
<a name="ln1297">			doReserveMemory = true;</a>
<a name="ln1298">			break;</a>
<a name="ln1299">		default:</a>
<a name="ln1300">			return B_BAD_VALUE;</a>
<a name="ln1301">	}</a>
<a name="ln1302"> </a>
<a name="ln1303">	// Optimization: For a single-page contiguous allocation without low/high</a>
<a name="ln1304">	// memory restriction B_FULL_LOCK wiring suffices.</a>
<a name="ln1305">	if (wiring == B_CONTIGUOUS &amp;&amp; size == B_PAGE_SIZE</a>
<a name="ln1306">		&amp;&amp; physicalAddressRestrictions-&gt;low_address == 0</a>
<a name="ln1307">		&amp;&amp; physicalAddressRestrictions-&gt;high_address == 0) {</a>
<a name="ln1308">		wiring = B_FULL_LOCK;</a>
<a name="ln1309">	}</a>
<a name="ln1310"> </a>
<a name="ln1311">	// For full lock or contiguous areas we're also going to map the pages and</a>
<a name="ln1312">	// thus need to reserve pages for the mapping backend upfront.</a>
<a name="ln1313">	addr_t reservedMapPages = 0;</a>
<a name="ln1314">	if (wiring == B_FULL_LOCK || wiring == B_CONTIGUOUS) {</a>
<a name="ln1315">		AddressSpaceWriteLocker locker;</a>
<a name="ln1316">		status_t status = locker.SetTo(team);</a>
<a name="ln1317">		if (status != B_OK)</a>
<a name="ln1318">			return status;</a>
<a name="ln1319"> </a>
<a name="ln1320">		VMTranslationMap* map = locker.AddressSpace()-&gt;TranslationMap();</a>
<a name="ln1321">		reservedMapPages = map-&gt;MaxPagesNeededToMap(0, size - 1);</a>
<a name="ln1322">	}</a>
<a name="ln1323"> </a>
<a name="ln1324">	int priority;</a>
<a name="ln1325">	if (team != VMAddressSpace::KernelID())</a>
<a name="ln1326">		priority = VM_PRIORITY_USER;</a>
<a name="ln1327">	else if ((flags &amp; CREATE_AREA_PRIORITY_VIP) != 0)</a>
<a name="ln1328">		priority = VM_PRIORITY_VIP;</a>
<a name="ln1329">	else</a>
<a name="ln1330">		priority = VM_PRIORITY_SYSTEM;</a>
<a name="ln1331"> </a>
<a name="ln1332">	// Reserve memory before acquiring the address space lock. This reduces the</a>
<a name="ln1333">	// chances of failure, since while holding the write lock to the address</a>
<a name="ln1334">	// space (if it is the kernel address space that is), the low memory handler</a>
<a name="ln1335">	// won't be able to free anything for us.</a>
<a name="ln1336">	addr_t reservedMemory = 0;</a>
<a name="ln1337">	if (doReserveMemory) {</a>
<a name="ln1338">		bigtime_t timeout = (flags &amp; CREATE_AREA_DONT_WAIT) != 0 ? 0 : 1000000;</a>
<a name="ln1339">		if (vm_try_reserve_memory(size, priority, timeout) != B_OK)</a>
<a name="ln1340">			return B_NO_MEMORY;</a>
<a name="ln1341">		reservedMemory = size;</a>
<a name="ln1342">		// TODO: We don't reserve the memory for the pages for the page</a>
<a name="ln1343">		// directories/tables. We actually need to do since we currently don't</a>
<a name="ln1344">		// reclaim them (and probably can't reclaim all of them anyway). Thus</a>
<a name="ln1345">		// there are actually less physical pages than there should be, which</a>
<a name="ln1346">		// can get the VM into trouble in low memory situations.</a>
<a name="ln1347">	}</a>
<a name="ln1348"> </a>
<a name="ln1349">	AddressSpaceWriteLocker locker;</a>
<a name="ln1350">	VMAddressSpace* addressSpace;</a>
<a name="ln1351">	status_t status;</a>
<a name="ln1352"> </a>
<a name="ln1353">	// For full lock areas reserve the pages before locking the address</a>
<a name="ln1354">	// space. E.g. block caches can't release their memory while we hold the</a>
<a name="ln1355">	// address space lock.</a>
<a name="ln1356">	page_num_t reservedPages = reservedMapPages;</a>
<a name="ln1357">	if (wiring == B_FULL_LOCK)</a>
<a name="ln1358">		reservedPages += size / B_PAGE_SIZE;</a>
<a name="ln1359"> </a>
<a name="ln1360">	vm_page_reservation reservation;</a>
<a name="ln1361">	if (reservedPages &gt; 0) {</a>
<a name="ln1362">		if ((flags &amp; CREATE_AREA_DONT_WAIT) != 0) {</a>
<a name="ln1363">			if (!vm_page_try_reserve_pages(&amp;reservation, reservedPages,</a>
<a name="ln1364">					priority)) {</a>
<a name="ln1365">				reservedPages = 0;</a>
<a name="ln1366">				status = B_WOULD_BLOCK;</a>
<a name="ln1367">				goto err0;</a>
<a name="ln1368">			}</a>
<a name="ln1369">		} else</a>
<a name="ln1370">			vm_page_reserve_pages(&amp;reservation, reservedPages, priority);</a>
<a name="ln1371">	}</a>
<a name="ln1372"> </a>
<a name="ln1373">	if (wiring == B_CONTIGUOUS) {</a>
<a name="ln1374">		// we try to allocate the page run here upfront as this may easily</a>
<a name="ln1375">		// fail for obvious reasons</a>
<a name="ln1376">		page = vm_page_allocate_page_run(PAGE_STATE_WIRED | pageAllocFlags,</a>
<a name="ln1377">			size / B_PAGE_SIZE, physicalAddressRestrictions, priority);</a>
<a name="ln1378">		if (page == NULL) {</a>
<a name="ln1379">			status = B_NO_MEMORY;</a>
<a name="ln1380">			goto err0;</a>
<a name="ln1381">		}</a>
<a name="ln1382">	}</a>
<a name="ln1383"> </a>
<a name="ln1384">	// Lock the address space and, if B_EXACT_ADDRESS and</a>
<a name="ln1385">	// CREATE_AREA_UNMAP_ADDRESS_RANGE were specified, ensure the address range</a>
<a name="ln1386">	// is not wired.</a>
<a name="ln1387">	do {</a>
<a name="ln1388">		status = locker.SetTo(team);</a>
<a name="ln1389">		if (status != B_OK)</a>
<a name="ln1390">			goto err1;</a>
<a name="ln1391"> </a>
<a name="ln1392">		addressSpace = locker.AddressSpace();</a>
<a name="ln1393">	} while (virtualAddressRestrictions-&gt;address_specification</a>
<a name="ln1394">			== B_EXACT_ADDRESS</a>
<a name="ln1395">		&amp;&amp; (flags &amp; CREATE_AREA_UNMAP_ADDRESS_RANGE) != 0</a>
<a name="ln1396">		&amp;&amp; wait_if_address_range_is_wired(addressSpace,</a>
<a name="ln1397">			(addr_t)virtualAddressRestrictions-&gt;address, size, &amp;locker));</a>
<a name="ln1398"> </a>
<a name="ln1399">	// create an anonymous cache</a>
<a name="ln1400">	// if it's a stack, make sure that two pages are available at least</a>
<a name="ln1401">	status = VMCacheFactory::CreateAnonymousCache(cache, canOvercommit,</a>
<a name="ln1402">		isStack ? (min_c(2, size / B_PAGE_SIZE - guardPages)) : 0, guardPages,</a>
<a name="ln1403">		wiring == B_NO_LOCK, priority);</a>
<a name="ln1404">	if (status != B_OK)</a>
<a name="ln1405">		goto err1;</a>
<a name="ln1406"> </a>
<a name="ln1407">	cache-&gt;temporary = 1;</a>
<a name="ln1408">	cache-&gt;virtual_end = size;</a>
<a name="ln1409">	cache-&gt;committed_size = reservedMemory;</a>
<a name="ln1410">		// TODO: This should be done via a method.</a>
<a name="ln1411">	reservedMemory = 0;</a>
<a name="ln1412"> </a>
<a name="ln1413">	cache-&gt;Lock();</a>
<a name="ln1414"> </a>
<a name="ln1415">	status = map_backing_store(addressSpace, cache, 0, name, size, wiring,</a>
<a name="ln1416">		protection, REGION_NO_PRIVATE_MAP, flags, virtualAddressRestrictions,</a>
<a name="ln1417">		kernel, &amp;area, _address);</a>
<a name="ln1418"> </a>
<a name="ln1419">	if (status != B_OK) {</a>
<a name="ln1420">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln1421">		goto err1;</a>
<a name="ln1422">	}</a>
<a name="ln1423"> </a>
<a name="ln1424">	locker.DegradeToReadLock();</a>
<a name="ln1425"> </a>
<a name="ln1426">	switch (wiring) {</a>
<a name="ln1427">		case B_NO_LOCK:</a>
<a name="ln1428">		case B_LAZY_LOCK:</a>
<a name="ln1429">			// do nothing - the pages are mapped in as needed</a>
<a name="ln1430">			break;</a>
<a name="ln1431"> </a>
<a name="ln1432">		case B_FULL_LOCK:</a>
<a name="ln1433">		{</a>
<a name="ln1434">			// Allocate and map all pages for this area</a>
<a name="ln1435"> </a>
<a name="ln1436">			off_t offset = 0;</a>
<a name="ln1437">			for (addr_t address = area-&gt;Base();</a>
<a name="ln1438">					address &lt; area-&gt;Base() + (area-&gt;Size() - 1);</a>
<a name="ln1439">					address += B_PAGE_SIZE, offset += B_PAGE_SIZE) {</a>
<a name="ln1440">#ifdef DEBUG_KERNEL_STACKS</a>
<a name="ln1441">#	ifdef STACK_GROWS_DOWNWARDS</a>
<a name="ln1442">				if (isStack &amp;&amp; address &lt; area-&gt;Base()</a>
<a name="ln1443">						+ KERNEL_STACK_GUARD_PAGES * B_PAGE_SIZE)</a>
<a name="ln1444">#	else</a>
<a name="ln1445">				if (isStack &amp;&amp; address &gt;= area-&gt;Base() + area-&gt;Size()</a>
<a name="ln1446">						- KERNEL_STACK_GUARD_PAGES * B_PAGE_SIZE)</a>
<a name="ln1447">#	endif</a>
<a name="ln1448">					continue;</a>
<a name="ln1449">#endif</a>
<a name="ln1450">				vm_page* page = vm_page_allocate_page(&amp;reservation,</a>
<a name="ln1451">					PAGE_STATE_WIRED | pageAllocFlags);</a>
<a name="ln1452">				cache-&gt;InsertPage(page, offset);</a>
<a name="ln1453">				map_page(area, page, address, protection, &amp;reservation);</a>
<a name="ln1454"> </a>
<a name="ln1455">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1456">			}</a>
<a name="ln1457"> </a>
<a name="ln1458">			break;</a>
<a name="ln1459">		}</a>
<a name="ln1460"> </a>
<a name="ln1461">		case B_ALREADY_WIRED:</a>
<a name="ln1462">		{</a>
<a name="ln1463">			// The pages should already be mapped. This is only really useful</a>
<a name="ln1464">			// during boot time. Find the appropriate vm_page objects and stick</a>
<a name="ln1465">			// them in the cache object.</a>
<a name="ln1466">			VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln1467">			off_t offset = 0;</a>
<a name="ln1468"> </a>
<a name="ln1469">			if (!gKernelStartup)</a>
<a name="ln1470">				panic(&quot;ALREADY_WIRED flag used outside kernel startup\n&quot;);</a>
<a name="ln1471"> </a>
<a name="ln1472">			map-&gt;Lock();</a>
<a name="ln1473"> </a>
<a name="ln1474">			for (addr_t virtualAddress = area-&gt;Base();</a>
<a name="ln1475">					virtualAddress &lt; area-&gt;Base() + (area-&gt;Size() - 1);</a>
<a name="ln1476">					virtualAddress += B_PAGE_SIZE, offset += B_PAGE_SIZE) {</a>
<a name="ln1477">				phys_addr_t physicalAddress;</a>
<a name="ln1478">				uint32 flags;</a>
<a name="ln1479">				status = map-&gt;Query(virtualAddress, &amp;physicalAddress, &amp;flags);</a>
<a name="ln1480">				if (status &lt; B_OK) {</a>
<a name="ln1481">					panic(&quot;looking up mapping failed for va 0x%lx\n&quot;,</a>
<a name="ln1482">						virtualAddress);</a>
<a name="ln1483">				}</a>
<a name="ln1484">				page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);</a>
<a name="ln1485">				if (page == NULL) {</a>
<a name="ln1486">					panic(&quot;looking up page failed for pa %#&quot; B_PRIxPHYSADDR</a>
<a name="ln1487">						&quot;\n&quot;, physicalAddress);</a>
<a name="ln1488">				}</a>
<a name="ln1489"> </a>
<a name="ln1490">				DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln1491"> </a>
<a name="ln1492">				cache-&gt;InsertPage(page, offset);</a>
<a name="ln1493">				increment_page_wired_count(page);</a>
<a name="ln1494">				vm_page_set_state(page, PAGE_STATE_WIRED);</a>
<a name="ln1495">				page-&gt;busy = false;</a>
<a name="ln1496"> </a>
<a name="ln1497">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1498">			}</a>
<a name="ln1499"> </a>
<a name="ln1500">			map-&gt;Unlock();</a>
<a name="ln1501">			break;</a>
<a name="ln1502">		}</a>
<a name="ln1503"> </a>
<a name="ln1504">		case B_CONTIGUOUS:</a>
<a name="ln1505">		{</a>
<a name="ln1506">			// We have already allocated our continuous pages run, so we can now</a>
<a name="ln1507">			// just map them in the address space</a>
<a name="ln1508">			VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln1509">			phys_addr_t physicalAddress</a>
<a name="ln1510">				= (phys_addr_t)page-&gt;physical_page_number * B_PAGE_SIZE;</a>
<a name="ln1511">			addr_t virtualAddress = area-&gt;Base();</a>
<a name="ln1512">			off_t offset = 0;</a>
<a name="ln1513"> </a>
<a name="ln1514">			map-&gt;Lock();</a>
<a name="ln1515"> </a>
<a name="ln1516">			for (virtualAddress = area-&gt;Base(); virtualAddress &lt; area-&gt;Base()</a>
<a name="ln1517">					+ (area-&gt;Size() - 1); virtualAddress += B_PAGE_SIZE,</a>
<a name="ln1518">					offset += B_PAGE_SIZE, physicalAddress += B_PAGE_SIZE) {</a>
<a name="ln1519">				page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);</a>
<a name="ln1520">				if (page == NULL)</a>
<a name="ln1521">					panic(&quot;couldn't lookup physical page just allocated\n&quot;);</a>
<a name="ln1522"> </a>
<a name="ln1523">				status = map-&gt;Map(virtualAddress, physicalAddress, protection,</a>
<a name="ln1524">					area-&gt;MemoryType(), &amp;reservation);</a>
<a name="ln1525">				if (status &lt; B_OK)</a>
<a name="ln1526">					panic(&quot;couldn't map physical page in page run\n&quot;);</a>
<a name="ln1527"> </a>
<a name="ln1528">				cache-&gt;InsertPage(page, offset);</a>
<a name="ln1529">				increment_page_wired_count(page);</a>
<a name="ln1530"> </a>
<a name="ln1531">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1532">			}</a>
<a name="ln1533"> </a>
<a name="ln1534">			map-&gt;Unlock();</a>
<a name="ln1535">			break;</a>
<a name="ln1536">		}</a>
<a name="ln1537"> </a>
<a name="ln1538">		default:</a>
<a name="ln1539">			break;</a>
<a name="ln1540">	}</a>
<a name="ln1541"> </a>
<a name="ln1542">	cache-&gt;Unlock();</a>
<a name="ln1543"> </a>
<a name="ln1544">	if (reservedPages &gt; 0)</a>
<a name="ln1545">		vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln1546"> </a>
<a name="ln1547">	TRACE((&quot;vm_create_anonymous_area: done\n&quot;));</a>
<a name="ln1548"> </a>
<a name="ln1549">	area-&gt;cache_type = CACHE_TYPE_RAM;</a>
<a name="ln1550">	return area-&gt;id;</a>
<a name="ln1551"> </a>
<a name="ln1552">err1:</a>
<a name="ln1553">	if (wiring == B_CONTIGUOUS) {</a>
<a name="ln1554">		// we had reserved the area space upfront...</a>
<a name="ln1555">		phys_addr_t pageNumber = page-&gt;physical_page_number;</a>
<a name="ln1556">		int32 i;</a>
<a name="ln1557">		for (i = size / B_PAGE_SIZE; i-- &gt; 0; pageNumber++) {</a>
<a name="ln1558">			page = vm_lookup_page(pageNumber);</a>
<a name="ln1559">			if (page == NULL)</a>
<a name="ln1560">				panic(&quot;couldn't lookup physical page just allocated\n&quot;);</a>
<a name="ln1561"> </a>
<a name="ln1562">			vm_page_set_state(page, PAGE_STATE_FREE);</a>
<a name="ln1563">		}</a>
<a name="ln1564">	}</a>
<a name="ln1565"> </a>
<a name="ln1566">err0:</a>
<a name="ln1567">	if (reservedPages &gt; 0)</a>
<a name="ln1568">		vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln1569">	if (reservedMemory &gt; 0)</a>
<a name="ln1570">		vm_unreserve_memory(reservedMemory);</a>
<a name="ln1571"> </a>
<a name="ln1572">	return status;</a>
<a name="ln1573">}</a>
<a name="ln1574"> </a>
<a name="ln1575"> </a>
<a name="ln1576">area_id</a>
<a name="ln1577">vm_map_physical_memory(team_id team, const char* name, void** _address,</a>
<a name="ln1578">	uint32 addressSpec, addr_t size, uint32 protection,</a>
<a name="ln1579">	phys_addr_t physicalAddress, bool alreadyWired)</a>
<a name="ln1580">{</a>
<a name="ln1581">	VMArea* area;</a>
<a name="ln1582">	VMCache* cache;</a>
<a name="ln1583">	addr_t mapOffset;</a>
<a name="ln1584"> </a>
<a name="ln1585">	TRACE((&quot;vm_map_physical_memory(aspace = %&quot; B_PRId32 &quot;, \&quot;%s\&quot;, virtual = %p&quot;</a>
<a name="ln1586">		&quot;, spec = %&quot; B_PRIu32 &quot;, size = %&quot; B_PRIxADDR &quot;, protection = %&quot;</a>
<a name="ln1587">		B_PRIu32 &quot;, phys = %#&quot; B_PRIxPHYSADDR &quot;)\n&quot;, team, name, *_address,</a>
<a name="ln1588">		addressSpec, size, protection, physicalAddress));</a>
<a name="ln1589"> </a>
<a name="ln1590">	if (!arch_vm_supports_protection(protection))</a>
<a name="ln1591">		return B_NOT_SUPPORTED;</a>
<a name="ln1592"> </a>
<a name="ln1593">	AddressSpaceWriteLocker locker(team);</a>
<a name="ln1594">	if (!locker.IsLocked())</a>
<a name="ln1595">		return B_BAD_TEAM_ID;</a>
<a name="ln1596"> </a>
<a name="ln1597">	// if the physical address is somewhat inside a page,</a>
<a name="ln1598">	// move the actual area down to align on a page boundary</a>
<a name="ln1599">	mapOffset = physicalAddress % B_PAGE_SIZE;</a>
<a name="ln1600">	size += mapOffset;</a>
<a name="ln1601">	physicalAddress -= mapOffset;</a>
<a name="ln1602"> </a>
<a name="ln1603">	size = PAGE_ALIGN(size);</a>
<a name="ln1604"> </a>
<a name="ln1605">	// create a device cache</a>
<a name="ln1606">	status_t status = VMCacheFactory::CreateDeviceCache(cache, physicalAddress);</a>
<a name="ln1607">	if (status != B_OK)</a>
<a name="ln1608">		return status;</a>
<a name="ln1609"> </a>
<a name="ln1610">	cache-&gt;virtual_end = size;</a>
<a name="ln1611"> </a>
<a name="ln1612">	cache-&gt;Lock();</a>
<a name="ln1613"> </a>
<a name="ln1614">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1615">	addressRestrictions.address = *_address;</a>
<a name="ln1616">	addressRestrictions.address_specification = addressSpec &amp; ~B_MTR_MASK;</a>
<a name="ln1617">	status = map_backing_store(locker.AddressSpace(), cache, 0, name, size,</a>
<a name="ln1618">		B_FULL_LOCK, protection, REGION_NO_PRIVATE_MAP, 0, &amp;addressRestrictions,</a>
<a name="ln1619">		true, &amp;area, _address);</a>
<a name="ln1620"> </a>
<a name="ln1621">	if (status &lt; B_OK)</a>
<a name="ln1622">		cache-&gt;ReleaseRefLocked();</a>
<a name="ln1623"> </a>
<a name="ln1624">	cache-&gt;Unlock();</a>
<a name="ln1625"> </a>
<a name="ln1626">	if (status == B_OK) {</a>
<a name="ln1627">		// set requested memory type -- use uncached, if not given</a>
<a name="ln1628">		uint32 memoryType = addressSpec &amp; B_MTR_MASK;</a>
<a name="ln1629">		if (memoryType == 0)</a>
<a name="ln1630">			memoryType = B_MTR_UC;</a>
<a name="ln1631"> </a>
<a name="ln1632">		area-&gt;SetMemoryType(memoryType);</a>
<a name="ln1633"> </a>
<a name="ln1634">		status = arch_vm_set_memory_type(area, physicalAddress, memoryType);</a>
<a name="ln1635">		if (status != B_OK)</a>
<a name="ln1636">			delete_area(locker.AddressSpace(), area, false);</a>
<a name="ln1637">	}</a>
<a name="ln1638"> </a>
<a name="ln1639">	if (status != B_OK)</a>
<a name="ln1640">		return status;</a>
<a name="ln1641"> </a>
<a name="ln1642">	VMTranslationMap* map = locker.AddressSpace()-&gt;TranslationMap();</a>
<a name="ln1643"> </a>
<a name="ln1644">	if (alreadyWired) {</a>
<a name="ln1645">		// The area is already mapped, but possibly not with the right</a>
<a name="ln1646">		// memory type.</a>
<a name="ln1647">		map-&gt;Lock();</a>
<a name="ln1648">		map-&gt;ProtectArea(area, area-&gt;protection);</a>
<a name="ln1649">		map-&gt;Unlock();</a>
<a name="ln1650">	} else {</a>
<a name="ln1651">		// Map the area completely.</a>
<a name="ln1652"> </a>
<a name="ln1653">		// reserve pages needed for the mapping</a>
<a name="ln1654">		size_t reservePages = map-&gt;MaxPagesNeededToMap(area-&gt;Base(),</a>
<a name="ln1655">			area-&gt;Base() + (size - 1));</a>
<a name="ln1656">		vm_page_reservation reservation;</a>
<a name="ln1657">		vm_page_reserve_pages(&amp;reservation, reservePages,</a>
<a name="ln1658">			team == VMAddressSpace::KernelID()</a>
<a name="ln1659">				? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln1660"> </a>
<a name="ln1661">		map-&gt;Lock();</a>
<a name="ln1662"> </a>
<a name="ln1663">		for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {</a>
<a name="ln1664">			map-&gt;Map(area-&gt;Base() + offset, physicalAddress + offset,</a>
<a name="ln1665">				protection, area-&gt;MemoryType(), &amp;reservation);</a>
<a name="ln1666">		}</a>
<a name="ln1667"> </a>
<a name="ln1668">		map-&gt;Unlock();</a>
<a name="ln1669"> </a>
<a name="ln1670">		vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln1671">	}</a>
<a name="ln1672"> </a>
<a name="ln1673">	// modify the pointer returned to be offset back into the new area</a>
<a name="ln1674">	// the same way the physical address in was offset</a>
<a name="ln1675">	*_address = (void*)((addr_t)*_address + mapOffset);</a>
<a name="ln1676"> </a>
<a name="ln1677">	area-&gt;cache_type = CACHE_TYPE_DEVICE;</a>
<a name="ln1678">	return area-&gt;id;</a>
<a name="ln1679">}</a>
<a name="ln1680"> </a>
<a name="ln1681"> </a>
<a name="ln1682">/*!	Don't use!</a>
<a name="ln1683">	TODO: This function was introduced to map physical page vecs to</a>
<a name="ln1684">	contiguous virtual memory in IOBuffer::GetNextVirtualVec(). It does</a>
<a name="ln1685">	use a device cache and does not track vm_page::wired_count!</a>
<a name="ln1686">*/</a>
<a name="ln1687">area_id</a>
<a name="ln1688">vm_map_physical_memory_vecs(team_id team, const char* name, void** _address,</a>
<a name="ln1689">	uint32 addressSpec, addr_t* _size, uint32 protection,</a>
<a name="ln1690">	struct generic_io_vec* vecs, uint32 vecCount)</a>
<a name="ln1691">{</a>
<a name="ln1692">	TRACE((&quot;vm_map_physical_memory_vecs(team = %&quot; B_PRId32 &quot;, \&quot;%s\&quot;, virtual &quot;</a>
<a name="ln1693">		&quot;= %p, spec = %&quot; B_PRIu32 &quot;, _size = %p, protection = %&quot; B_PRIu32 &quot;, &quot;</a>
<a name="ln1694">		&quot;vecs = %p, vecCount = %&quot; B_PRIu32 &quot;)\n&quot;, team, name, *_address,</a>
<a name="ln1695">		addressSpec, _size, protection, vecs, vecCount));</a>
<a name="ln1696"> </a>
<a name="ln1697">	if (!arch_vm_supports_protection(protection)</a>
<a name="ln1698">		|| (addressSpec &amp; B_MTR_MASK) != 0) {</a>
<a name="ln1699">		return B_NOT_SUPPORTED;</a>
<a name="ln1700">	}</a>
<a name="ln1701"> </a>
<a name="ln1702">	AddressSpaceWriteLocker locker(team);</a>
<a name="ln1703">	if (!locker.IsLocked())</a>
<a name="ln1704">		return B_BAD_TEAM_ID;</a>
<a name="ln1705"> </a>
<a name="ln1706">	if (vecCount == 0)</a>
<a name="ln1707">		return B_BAD_VALUE;</a>
<a name="ln1708"> </a>
<a name="ln1709">	addr_t size = 0;</a>
<a name="ln1710">	for (uint32 i = 0; i &lt; vecCount; i++) {</a>
<a name="ln1711">		if (vecs[i].base % B_PAGE_SIZE != 0</a>
<a name="ln1712">			|| vecs[i].length % B_PAGE_SIZE != 0) {</a>
<a name="ln1713">			return B_BAD_VALUE;</a>
<a name="ln1714">		}</a>
<a name="ln1715"> </a>
<a name="ln1716">		size += vecs[i].length;</a>
<a name="ln1717">	}</a>
<a name="ln1718"> </a>
<a name="ln1719">	// create a device cache</a>
<a name="ln1720">	VMCache* cache;</a>
<a name="ln1721">	status_t result = VMCacheFactory::CreateDeviceCache(cache, vecs[0].base);</a>
<a name="ln1722">	if (result != B_OK)</a>
<a name="ln1723">		return result;</a>
<a name="ln1724"> </a>
<a name="ln1725">	cache-&gt;virtual_end = size;</a>
<a name="ln1726"> </a>
<a name="ln1727">	cache-&gt;Lock();</a>
<a name="ln1728"> </a>
<a name="ln1729">	VMArea* area;</a>
<a name="ln1730">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1731">	addressRestrictions.address = *_address;</a>
<a name="ln1732">	addressRestrictions.address_specification = addressSpec &amp; ~B_MTR_MASK;</a>
<a name="ln1733">	result = map_backing_store(locker.AddressSpace(), cache, 0, name,</a>
<a name="ln1734">		size, B_FULL_LOCK, protection, REGION_NO_PRIVATE_MAP, 0,</a>
<a name="ln1735">		&amp;addressRestrictions, true, &amp;area, _address);</a>
<a name="ln1736"> </a>
<a name="ln1737">	if (result != B_OK)</a>
<a name="ln1738">		cache-&gt;ReleaseRefLocked();</a>
<a name="ln1739"> </a>
<a name="ln1740">	cache-&gt;Unlock();</a>
<a name="ln1741"> </a>
<a name="ln1742">	if (result != B_OK)</a>
<a name="ln1743">		return result;</a>
<a name="ln1744"> </a>
<a name="ln1745">	VMTranslationMap* map = locker.AddressSpace()-&gt;TranslationMap();</a>
<a name="ln1746">	size_t reservePages = map-&gt;MaxPagesNeededToMap(area-&gt;Base(),</a>
<a name="ln1747">		area-&gt;Base() + (size - 1));</a>
<a name="ln1748"> </a>
<a name="ln1749">	vm_page_reservation reservation;</a>
<a name="ln1750">	vm_page_reserve_pages(&amp;reservation, reservePages,</a>
<a name="ln1751">			team == VMAddressSpace::KernelID()</a>
<a name="ln1752">				? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln1753">	map-&gt;Lock();</a>
<a name="ln1754"> </a>
<a name="ln1755">	uint32 vecIndex = 0;</a>
<a name="ln1756">	size_t vecOffset = 0;</a>
<a name="ln1757">	for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {</a>
<a name="ln1758">		while (vecOffset &gt;= vecs[vecIndex].length &amp;&amp; vecIndex &lt; vecCount) {</a>
<a name="ln1759">			vecOffset = 0;</a>
<a name="ln1760">			vecIndex++;</a>
<a name="ln1761">		}</a>
<a name="ln1762"> </a>
<a name="ln1763">		if (vecIndex &gt;= vecCount)</a>
<a name="ln1764">			break;</a>
<a name="ln1765"> </a>
<a name="ln1766">		map-&gt;Map(area-&gt;Base() + offset, vecs[vecIndex].base + vecOffset,</a>
<a name="ln1767">			protection, area-&gt;MemoryType(), &amp;reservation);</a>
<a name="ln1768"> </a>
<a name="ln1769">		vecOffset += B_PAGE_SIZE;</a>
<a name="ln1770">	}</a>
<a name="ln1771"> </a>
<a name="ln1772">	map-&gt;Unlock();</a>
<a name="ln1773">	vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln1774"> </a>
<a name="ln1775">	if (_size != NULL)</a>
<a name="ln1776">		*_size = size;</a>
<a name="ln1777"> </a>
<a name="ln1778">	area-&gt;cache_type = CACHE_TYPE_DEVICE;</a>
<a name="ln1779">	return area-&gt;id;</a>
<a name="ln1780">}</a>
<a name="ln1781"> </a>
<a name="ln1782"> </a>
<a name="ln1783">area_id</a>
<a name="ln1784">vm_create_null_area(team_id team, const char* name, void** address,</a>
<a name="ln1785">	uint32 addressSpec, addr_t size, uint32 flags)</a>
<a name="ln1786">{</a>
<a name="ln1787">	size = PAGE_ALIGN(size);</a>
<a name="ln1788"> </a>
<a name="ln1789">	// Lock the address space and, if B_EXACT_ADDRESS and</a>
<a name="ln1790">	// CREATE_AREA_UNMAP_ADDRESS_RANGE were specified, ensure the address range</a>
<a name="ln1791">	// is not wired.</a>
<a name="ln1792">	AddressSpaceWriteLocker locker;</a>
<a name="ln1793">	do {</a>
<a name="ln1794">		if (locker.SetTo(team) != B_OK)</a>
<a name="ln1795">			return B_BAD_TEAM_ID;</a>
<a name="ln1796">	} while (addressSpec == B_EXACT_ADDRESS</a>
<a name="ln1797">		&amp;&amp; (flags &amp; CREATE_AREA_UNMAP_ADDRESS_RANGE) != 0</a>
<a name="ln1798">		&amp;&amp; wait_if_address_range_is_wired(locker.AddressSpace(),</a>
<a name="ln1799">			(addr_t)*address, size, &amp;locker));</a>
<a name="ln1800"> </a>
<a name="ln1801">	// create a null cache</a>
<a name="ln1802">	int priority = (flags &amp; CREATE_AREA_PRIORITY_VIP) != 0</a>
<a name="ln1803">		? VM_PRIORITY_VIP : VM_PRIORITY_SYSTEM;</a>
<a name="ln1804">	VMCache* cache;</a>
<a name="ln1805">	status_t status = VMCacheFactory::CreateNullCache(priority, cache);</a>
<a name="ln1806">	if (status != B_OK)</a>
<a name="ln1807">		return status;</a>
<a name="ln1808"> </a>
<a name="ln1809">	cache-&gt;temporary = 1;</a>
<a name="ln1810">	cache-&gt;virtual_end = size;</a>
<a name="ln1811"> </a>
<a name="ln1812">	cache-&gt;Lock();</a>
<a name="ln1813"> </a>
<a name="ln1814">	VMArea* area;</a>
<a name="ln1815">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1816">	addressRestrictions.address = *address;</a>
<a name="ln1817">	addressRestrictions.address_specification = addressSpec;</a>
<a name="ln1818">	status = map_backing_store(locker.AddressSpace(), cache, 0, name, size,</a>
<a name="ln1819">		B_LAZY_LOCK, B_KERNEL_READ_AREA, REGION_NO_PRIVATE_MAP, flags,</a>
<a name="ln1820">		&amp;addressRestrictions, true, &amp;area, address);</a>
<a name="ln1821"> </a>
<a name="ln1822">	if (status &lt; B_OK) {</a>
<a name="ln1823">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln1824">		return status;</a>
<a name="ln1825">	}</a>
<a name="ln1826"> </a>
<a name="ln1827">	cache-&gt;Unlock();</a>
<a name="ln1828"> </a>
<a name="ln1829">	area-&gt;cache_type = CACHE_TYPE_NULL;</a>
<a name="ln1830">	return area-&gt;id;</a>
<a name="ln1831">}</a>
<a name="ln1832"> </a>
<a name="ln1833"> </a>
<a name="ln1834">/*!	Creates the vnode cache for the specified \a vnode.</a>
<a name="ln1835">	The vnode has to be marked busy when calling this function.</a>
<a name="ln1836">*/</a>
<a name="ln1837">status_t</a>
<a name="ln1838">vm_create_vnode_cache(struct vnode* vnode, struct VMCache** cache)</a>
<a name="ln1839">{</a>
<a name="ln1840">	return VMCacheFactory::CreateVnodeCache(*cache, vnode);</a>
<a name="ln1841">}</a>
<a name="ln1842"> </a>
<a name="ln1843"> </a>
<a name="ln1844">/*!	\a cache must be locked. The area's address space must be read-locked.</a>
<a name="ln1845">*/</a>
<a name="ln1846">static void</a>
<a name="ln1847">pre_map_area_pages(VMArea* area, VMCache* cache,</a>
<a name="ln1848">	vm_page_reservation* reservation)</a>
<a name="ln1849">{</a>
<a name="ln1850">	addr_t baseAddress = area-&gt;Base();</a>
<a name="ln1851">	addr_t cacheOffset = area-&gt;cache_offset;</a>
<a name="ln1852">	page_num_t firstPage = cacheOffset / B_PAGE_SIZE;</a>
<a name="ln1853">	page_num_t endPage = firstPage + area-&gt;Size() / B_PAGE_SIZE;</a>
<a name="ln1854"> </a>
<a name="ln1855">	for (VMCachePagesTree::Iterator it</a>
<a name="ln1856">				= cache-&gt;pages.GetIterator(firstPage, true, true);</a>
<a name="ln1857">			vm_page* page = it.Next();) {</a>
<a name="ln1858">		if (page-&gt;cache_offset &gt;= endPage)</a>
<a name="ln1859">			break;</a>
<a name="ln1860"> </a>
<a name="ln1861">		// skip busy and inactive pages</a>
<a name="ln1862">		if (page-&gt;busy || page-&gt;usage_count == 0)</a>
<a name="ln1863">			continue;</a>
<a name="ln1864"> </a>
<a name="ln1865">		DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln1866">		map_page(area, page,</a>
<a name="ln1867">			baseAddress + (page-&gt;cache_offset * B_PAGE_SIZE - cacheOffset),</a>
<a name="ln1868">			B_READ_AREA | B_KERNEL_READ_AREA, reservation);</a>
<a name="ln1869">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1870">	}</a>
<a name="ln1871">}</a>
<a name="ln1872"> </a>
<a name="ln1873"> </a>
<a name="ln1874">/*!	Will map the file specified by \a fd to an area in memory.</a>
<a name="ln1875">	The file will be mirrored beginning at the specified \a offset. The</a>
<a name="ln1876">	\a offset and \a size arguments have to be page aligned.</a>
<a name="ln1877">*/</a>
<a name="ln1878">static area_id</a>
<a name="ln1879">_vm_map_file(team_id team, const char* name, void** _address,</a>
<a name="ln1880">	uint32 addressSpec, size_t size, uint32 protection, uint32 mapping,</a>
<a name="ln1881">	bool unmapAddressRange, int fd, off_t offset, bool kernel)</a>
<a name="ln1882">{</a>
<a name="ln1883">	// TODO: for binary files, we want to make sure that they get the</a>
<a name="ln1884">	//	copy of a file at a given time, ie. later changes should not</a>
<a name="ln1885">	//	make it into the mapped copy -- this will need quite some changes</a>
<a name="ln1886">	//	to be done in a nice way</a>
<a name="ln1887">	TRACE((&quot;_vm_map_file(fd = %d, offset = %&quot; B_PRIdOFF &quot;, size = %lu, mapping &quot;</a>
<a name="ln1888">		&quot;%&quot; B_PRIu32 &quot;)\n&quot;, fd, offset, size, mapping));</a>
<a name="ln1889"> </a>
<a name="ln1890">	offset = ROUNDDOWN(offset, B_PAGE_SIZE);</a>
<a name="ln1891">	size = PAGE_ALIGN(size);</a>
<a name="ln1892"> </a>
<a name="ln1893">	if (mapping == REGION_NO_PRIVATE_MAP)</a>
<a name="ln1894">		protection |= B_SHARED_AREA;</a>
<a name="ln1895">	if (addressSpec != B_EXACT_ADDRESS)</a>
<a name="ln1896">		unmapAddressRange = false;</a>
<a name="ln1897"> </a>
<a name="ln1898">	if (fd &lt; 0) {</a>
<a name="ln1899">		uint32 flags = unmapAddressRange ? CREATE_AREA_UNMAP_ADDRESS_RANGE : 0;</a>
<a name="ln1900">		virtual_address_restrictions virtualRestrictions = {};</a>
<a name="ln1901">		virtualRestrictions.address = *_address;</a>
<a name="ln1902">		virtualRestrictions.address_specification = addressSpec;</a>
<a name="ln1903">		physical_address_restrictions physicalRestrictions = {};</a>
<a name="ln1904">		return vm_create_anonymous_area(team, name, size, B_NO_LOCK, protection,</a>
<a name="ln1905">			flags, 0, &amp;virtualRestrictions, &amp;physicalRestrictions, kernel,</a>
<a name="ln1906">			_address);</a>
<a name="ln1907">	}</a>
<a name="ln1908"> </a>
<a name="ln1909">	// get the open flags of the FD</a>
<a name="ln1910">	file_descriptor* descriptor = get_fd(get_current_io_context(kernel), fd);</a>
<a name="ln1911">	if (descriptor == NULL)</a>
<a name="ln1912">		return EBADF;</a>
<a name="ln1913">	int32 openMode = descriptor-&gt;open_mode;</a>
<a name="ln1914">	put_fd(descriptor);</a>
<a name="ln1915"> </a>
<a name="ln1916">	// The FD must open for reading at any rate. For shared mapping with write</a>
<a name="ln1917">	// access, additionally the FD must be open for writing.</a>
<a name="ln1918">	if ((openMode &amp; O_ACCMODE) == O_WRONLY</a>
<a name="ln1919">		|| (mapping == REGION_NO_PRIVATE_MAP</a>
<a name="ln1920">			&amp;&amp; (protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0</a>
<a name="ln1921">			&amp;&amp; (openMode &amp; O_ACCMODE) == O_RDONLY)) {</a>
<a name="ln1922">		return EACCES;</a>
<a name="ln1923">	}</a>
<a name="ln1924"> </a>
<a name="ln1925">	// get the vnode for the object, this also grabs a ref to it</a>
<a name="ln1926">	struct vnode* vnode = NULL;</a>
<a name="ln1927">	status_t status = vfs_get_vnode_from_fd(fd, kernel, &amp;vnode);</a>
<a name="ln1928">	if (status &lt; B_OK)</a>
<a name="ln1929">		return status;</a>
<a name="ln1930">	CObjectDeleter&lt;struct vnode&gt; vnodePutter(vnode, vfs_put_vnode);</a>
<a name="ln1931"> </a>
<a name="ln1932">	// If we're going to pre-map pages, we need to reserve the pages needed by</a>
<a name="ln1933">	// the mapping backend upfront.</a>
<a name="ln1934">	page_num_t reservedPreMapPages = 0;</a>
<a name="ln1935">	vm_page_reservation reservation;</a>
<a name="ln1936">	if ((protection &amp; B_READ_AREA) != 0) {</a>
<a name="ln1937">		AddressSpaceWriteLocker locker;</a>
<a name="ln1938">		status = locker.SetTo(team);</a>
<a name="ln1939">		if (status != B_OK)</a>
<a name="ln1940">			return status;</a>
<a name="ln1941"> </a>
<a name="ln1942">		VMTranslationMap* map = locker.AddressSpace()-&gt;TranslationMap();</a>
<a name="ln1943">		reservedPreMapPages = map-&gt;MaxPagesNeededToMap(0, size - 1);</a>
<a name="ln1944"> </a>
<a name="ln1945">		locker.Unlock();</a>
<a name="ln1946"> </a>
<a name="ln1947">		vm_page_reserve_pages(&amp;reservation, reservedPreMapPages,</a>
<a name="ln1948">			team == VMAddressSpace::KernelID()</a>
<a name="ln1949">				? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln1950">	}</a>
<a name="ln1951"> </a>
<a name="ln1952">	struct PageUnreserver {</a>
<a name="ln1953">		PageUnreserver(vm_page_reservation* reservation)</a>
<a name="ln1954">			:</a>
<a name="ln1955">			fReservation(reservation)</a>
<a name="ln1956">		{</a>
<a name="ln1957">		}</a>
<a name="ln1958"> </a>
<a name="ln1959">		~PageUnreserver()</a>
<a name="ln1960">		{</a>
<a name="ln1961">			if (fReservation != NULL)</a>
<a name="ln1962">				vm_page_unreserve_pages(fReservation);</a>
<a name="ln1963">		}</a>
<a name="ln1964"> </a>
<a name="ln1965">		vm_page_reservation* fReservation;</a>
<a name="ln1966">	} pageUnreserver(reservedPreMapPages &gt; 0 ? &amp;reservation : NULL);</a>
<a name="ln1967"> </a>
<a name="ln1968">	// Lock the address space and, if the specified address range shall be</a>
<a name="ln1969">	// unmapped, ensure it is not wired.</a>
<a name="ln1970">	AddressSpaceWriteLocker locker;</a>
<a name="ln1971">	do {</a>
<a name="ln1972">		if (locker.SetTo(team) != B_OK)</a>
<a name="ln1973">			return B_BAD_TEAM_ID;</a>
<a name="ln1974">	} while (unmapAddressRange</a>
<a name="ln1975">		&amp;&amp; wait_if_address_range_is_wired(locker.AddressSpace(),</a>
<a name="ln1976">			(addr_t)*_address, size, &amp;locker));</a>
<a name="ln1977"> </a>
<a name="ln1978">	// TODO: this only works for file systems that use the file cache</a>
<a name="ln1979">	VMCache* cache;</a>
<a name="ln1980">	status = vfs_get_vnode_cache(vnode, &amp;cache, false);</a>
<a name="ln1981">	if (status &lt; B_OK)</a>
<a name="ln1982">		return status;</a>
<a name="ln1983"> </a>
<a name="ln1984">	cache-&gt;Lock();</a>
<a name="ln1985"> </a>
<a name="ln1986">	VMArea* area;</a>
<a name="ln1987">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln1988">	addressRestrictions.address = *_address;</a>
<a name="ln1989">	addressRestrictions.address_specification = addressSpec;</a>
<a name="ln1990">	status = map_backing_store(locker.AddressSpace(), cache, offset, name, size,</a>
<a name="ln1991">		0, protection, mapping,</a>
<a name="ln1992">		unmapAddressRange ? CREATE_AREA_UNMAP_ADDRESS_RANGE : 0,</a>
<a name="ln1993">		&amp;addressRestrictions, kernel, &amp;area, _address);</a>
<a name="ln1994"> </a>
<a name="ln1995">	if (status != B_OK || mapping == REGION_PRIVATE_MAP) {</a>
<a name="ln1996">		// map_backing_store() cannot know we no longer need the ref</a>
<a name="ln1997">		cache-&gt;ReleaseRefLocked();</a>
<a name="ln1998">	}</a>
<a name="ln1999"> </a>
<a name="ln2000">	if (status == B_OK &amp;&amp; (protection &amp; B_READ_AREA) != 0)</a>
<a name="ln2001">		pre_map_area_pages(area, cache, &amp;reservation);</a>
<a name="ln2002"> </a>
<a name="ln2003">	cache-&gt;Unlock();</a>
<a name="ln2004"> </a>
<a name="ln2005">	if (status == B_OK) {</a>
<a name="ln2006">		// TODO: this probably deserves a smarter solution, ie. don't always</a>
<a name="ln2007">		// prefetch stuff, and also, probably don't trigger it at this place.</a>
<a name="ln2008">		cache_prefetch_vnode(vnode, offset, min_c(size, 10LL * 1024 * 1024));</a>
<a name="ln2009">			// prefetches at max 10 MB starting from &quot;offset&quot;</a>
<a name="ln2010">	}</a>
<a name="ln2011"> </a>
<a name="ln2012">	if (status != B_OK)</a>
<a name="ln2013">		return status;</a>
<a name="ln2014"> </a>
<a name="ln2015">	area-&gt;cache_type = CACHE_TYPE_VNODE;</a>
<a name="ln2016">	return area-&gt;id;</a>
<a name="ln2017">}</a>
<a name="ln2018"> </a>
<a name="ln2019"> </a>
<a name="ln2020">area_id</a>
<a name="ln2021">vm_map_file(team_id aid, const char* name, void** address, uint32 addressSpec,</a>
<a name="ln2022">	addr_t size, uint32 protection, uint32 mapping, bool unmapAddressRange,</a>
<a name="ln2023">	int fd, off_t offset)</a>
<a name="ln2024">{</a>
<a name="ln2025">	if (!arch_vm_supports_protection(protection))</a>
<a name="ln2026">		return B_NOT_SUPPORTED;</a>
<a name="ln2027"> </a>
<a name="ln2028">	return _vm_map_file(aid, name, address, addressSpec, size, protection,</a>
<a name="ln2029">		mapping, unmapAddressRange, fd, offset, true);</a>
<a name="ln2030">}</a>
<a name="ln2031"> </a>
<a name="ln2032"> </a>
<a name="ln2033">VMCache*</a>
<a name="ln2034">vm_area_get_locked_cache(VMArea* area)</a>
<a name="ln2035">{</a>
<a name="ln2036">	rw_lock_read_lock(&amp;sAreaCacheLock);</a>
<a name="ln2037"> </a>
<a name="ln2038">	while (true) {</a>
<a name="ln2039">		VMCache* cache = area-&gt;cache;</a>
<a name="ln2040"> </a>
<a name="ln2041">		if (!cache-&gt;SwitchFromReadLock(&amp;sAreaCacheLock)) {</a>
<a name="ln2042">			// cache has been deleted</a>
<a name="ln2043">			rw_lock_read_lock(&amp;sAreaCacheLock);</a>
<a name="ln2044">			continue;</a>
<a name="ln2045">		}</a>
<a name="ln2046"> </a>
<a name="ln2047">		rw_lock_read_lock(&amp;sAreaCacheLock);</a>
<a name="ln2048"> </a>
<a name="ln2049">		if (cache == area-&gt;cache) {</a>
<a name="ln2050">			cache-&gt;AcquireRefLocked();</a>
<a name="ln2051">			rw_lock_read_unlock(&amp;sAreaCacheLock);</a>
<a name="ln2052">			return cache;</a>
<a name="ln2053">		}</a>
<a name="ln2054"> </a>
<a name="ln2055">		// the cache changed in the meantime</a>
<a name="ln2056">		cache-&gt;Unlock();</a>
<a name="ln2057">	}</a>
<a name="ln2058">}</a>
<a name="ln2059"> </a>
<a name="ln2060"> </a>
<a name="ln2061">void</a>
<a name="ln2062">vm_area_put_locked_cache(VMCache* cache)</a>
<a name="ln2063">{</a>
<a name="ln2064">	cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2065">}</a>
<a name="ln2066"> </a>
<a name="ln2067"> </a>
<a name="ln2068">area_id</a>
<a name="ln2069">vm_clone_area(team_id team, const char* name, void** address,</a>
<a name="ln2070">	uint32 addressSpec, uint32 protection, uint32 mapping, area_id sourceID,</a>
<a name="ln2071">	bool kernel)</a>
<a name="ln2072">{</a>
<a name="ln2073">	VMArea* newArea = NULL;</a>
<a name="ln2074">	VMArea* sourceArea;</a>
<a name="ln2075"> </a>
<a name="ln2076">	// Check whether the source area exists and is cloneable. If so, mark it</a>
<a name="ln2077">	// B_SHARED_AREA, so that we don't get problems with copy-on-write.</a>
<a name="ln2078">	{</a>
<a name="ln2079">		AddressSpaceWriteLocker locker;</a>
<a name="ln2080">		status_t status = locker.SetFromArea(sourceID, sourceArea);</a>
<a name="ln2081">		if (status != B_OK)</a>
<a name="ln2082">			return status;</a>
<a name="ln2083"> </a>
<a name="ln2084">		sourceArea-&gt;protection |= B_SHARED_AREA;</a>
<a name="ln2085">		protection |= B_SHARED_AREA;</a>
<a name="ln2086">	}</a>
<a name="ln2087"> </a>
<a name="ln2088">	// Now lock both address spaces and actually do the cloning.</a>
<a name="ln2089"> </a>
<a name="ln2090">	MultiAddressSpaceLocker locker;</a>
<a name="ln2091">	VMAddressSpace* sourceAddressSpace;</a>
<a name="ln2092">	status_t status = locker.AddArea(sourceID, false, &amp;sourceAddressSpace);</a>
<a name="ln2093">	if (status != B_OK)</a>
<a name="ln2094">		return status;</a>
<a name="ln2095"> </a>
<a name="ln2096">	VMAddressSpace* targetAddressSpace;</a>
<a name="ln2097">	status = locker.AddTeam(team, true, &amp;targetAddressSpace);</a>
<a name="ln2098">	if (status != B_OK)</a>
<a name="ln2099">		return status;</a>
<a name="ln2100"> </a>
<a name="ln2101">	status = locker.Lock();</a>
<a name="ln2102">	if (status != B_OK)</a>
<a name="ln2103">		return status;</a>
<a name="ln2104"> </a>
<a name="ln2105">	sourceArea = lookup_area(sourceAddressSpace, sourceID);</a>
<a name="ln2106">	if (sourceArea == NULL)</a>
<a name="ln2107">		return B_BAD_VALUE;</a>
<a name="ln2108"> </a>
<a name="ln2109">	VMCache* cache = vm_area_get_locked_cache(sourceArea);</a>
<a name="ln2110"> </a>
<a name="ln2111">	if (!kernel &amp;&amp; sourceAddressSpace == VMAddressSpace::Kernel()</a>
<a name="ln2112">		&amp;&amp; targetAddressSpace != VMAddressSpace::Kernel()</a>
<a name="ln2113">		&amp;&amp; !(sourceArea-&gt;protection &amp; B_USER_CLONEABLE_AREA)) {</a>
<a name="ln2114">		// kernel areas must not be cloned in userland, unless explicitly</a>
<a name="ln2115">		// declared user-cloneable upon construction</a>
<a name="ln2116">#if KDEBUG</a>
<a name="ln2117">		panic(&quot;attempting to clone kernel area \&quot;%s\&quot; (%&quot; B_PRId32 &quot;)!&quot;,</a>
<a name="ln2118">			sourceArea-&gt;name, sourceID);</a>
<a name="ln2119">#endif</a>
<a name="ln2120">		status = B_NOT_ALLOWED;</a>
<a name="ln2121">	} else if (sourceArea-&gt;cache_type == CACHE_TYPE_NULL) {</a>
<a name="ln2122">		status = B_NOT_ALLOWED;</a>
<a name="ln2123">	} else {</a>
<a name="ln2124">		virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln2125">		addressRestrictions.address = *address;</a>
<a name="ln2126">		addressRestrictions.address_specification = addressSpec;</a>
<a name="ln2127">		status = map_backing_store(targetAddressSpace, cache,</a>
<a name="ln2128">			sourceArea-&gt;cache_offset, name, sourceArea-&gt;Size(),</a>
<a name="ln2129">			sourceArea-&gt;wiring, protection, mapping, 0, &amp;addressRestrictions,</a>
<a name="ln2130">			kernel, &amp;newArea, address);</a>
<a name="ln2131">	}</a>
<a name="ln2132">	if (status == B_OK &amp;&amp; mapping != REGION_PRIVATE_MAP) {</a>
<a name="ln2133">		// If the mapping is REGION_PRIVATE_MAP, map_backing_store() needed</a>
<a name="ln2134">		// to create a new cache, and has therefore already acquired a reference</a>
<a name="ln2135">		// to the source cache - but otherwise it has no idea that we need</a>
<a name="ln2136">		// one.</a>
<a name="ln2137">		cache-&gt;AcquireRefLocked();</a>
<a name="ln2138">	}</a>
<a name="ln2139">	if (status == B_OK &amp;&amp; newArea-&gt;wiring == B_FULL_LOCK) {</a>
<a name="ln2140">		// we need to map in everything at this point</a>
<a name="ln2141">		if (sourceArea-&gt;cache_type == CACHE_TYPE_DEVICE) {</a>
<a name="ln2142">			// we don't have actual pages to map but a physical area</a>
<a name="ln2143">			VMTranslationMap* map</a>
<a name="ln2144">				= sourceArea-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2145">			map-&gt;Lock();</a>
<a name="ln2146"> </a>
<a name="ln2147">			phys_addr_t physicalAddress;</a>
<a name="ln2148">			uint32 oldProtection;</a>
<a name="ln2149">			map-&gt;Query(sourceArea-&gt;Base(), &amp;physicalAddress, &amp;oldProtection);</a>
<a name="ln2150"> </a>
<a name="ln2151">			map-&gt;Unlock();</a>
<a name="ln2152"> </a>
<a name="ln2153">			map = targetAddressSpace-&gt;TranslationMap();</a>
<a name="ln2154">			size_t reservePages = map-&gt;MaxPagesNeededToMap(newArea-&gt;Base(),</a>
<a name="ln2155">				newArea-&gt;Base() + (newArea-&gt;Size() - 1));</a>
<a name="ln2156"> </a>
<a name="ln2157">			vm_page_reservation reservation;</a>
<a name="ln2158">			vm_page_reserve_pages(&amp;reservation, reservePages,</a>
<a name="ln2159">				targetAddressSpace == VMAddressSpace::Kernel()</a>
<a name="ln2160">					? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln2161">			map-&gt;Lock();</a>
<a name="ln2162"> </a>
<a name="ln2163">			for (addr_t offset = 0; offset &lt; newArea-&gt;Size();</a>
<a name="ln2164">					offset += B_PAGE_SIZE) {</a>
<a name="ln2165">				map-&gt;Map(newArea-&gt;Base() + offset, physicalAddress + offset,</a>
<a name="ln2166">					protection, newArea-&gt;MemoryType(), &amp;reservation);</a>
<a name="ln2167">			}</a>
<a name="ln2168"> </a>
<a name="ln2169">			map-&gt;Unlock();</a>
<a name="ln2170">			vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln2171">		} else {</a>
<a name="ln2172">			VMTranslationMap* map = targetAddressSpace-&gt;TranslationMap();</a>
<a name="ln2173">			size_t reservePages = map-&gt;MaxPagesNeededToMap(</a>
<a name="ln2174">				newArea-&gt;Base(), newArea-&gt;Base() + (newArea-&gt;Size() - 1));</a>
<a name="ln2175">			vm_page_reservation reservation;</a>
<a name="ln2176">			vm_page_reserve_pages(&amp;reservation, reservePages,</a>
<a name="ln2177">				targetAddressSpace == VMAddressSpace::Kernel()</a>
<a name="ln2178">					? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln2179"> </a>
<a name="ln2180">			// map in all pages from source</a>
<a name="ln2181">			for (VMCachePagesTree::Iterator it = cache-&gt;pages.GetIterator();</a>
<a name="ln2182">					vm_page* page  = it.Next();) {</a>
<a name="ln2183">				if (!page-&gt;busy) {</a>
<a name="ln2184">					DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2185">					map_page(newArea, page,</a>
<a name="ln2186">						newArea-&gt;Base() + ((page-&gt;cache_offset &lt;&lt; PAGE_SHIFT)</a>
<a name="ln2187">							- newArea-&gt;cache_offset),</a>
<a name="ln2188">						protection, &amp;reservation);</a>
<a name="ln2189">					DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2190">				}</a>
<a name="ln2191">			}</a>
<a name="ln2192">			// TODO: B_FULL_LOCK means that all pages are locked. We are not</a>
<a name="ln2193">			// ensuring that!</a>
<a name="ln2194"> </a>
<a name="ln2195">			vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln2196">		}</a>
<a name="ln2197">	}</a>
<a name="ln2198">	if (status == B_OK)</a>
<a name="ln2199">		newArea-&gt;cache_type = sourceArea-&gt;cache_type;</a>
<a name="ln2200"> </a>
<a name="ln2201">	vm_area_put_locked_cache(cache);</a>
<a name="ln2202"> </a>
<a name="ln2203">	if (status &lt; B_OK)</a>
<a name="ln2204">		return status;</a>
<a name="ln2205"> </a>
<a name="ln2206">	return newArea-&gt;id;</a>
<a name="ln2207">}</a>
<a name="ln2208"> </a>
<a name="ln2209"> </a>
<a name="ln2210">/*!	Deletes the specified area of the given address space.</a>
<a name="ln2211"> </a>
<a name="ln2212">	The address space must be write-locked.</a>
<a name="ln2213">	The caller must ensure that the area does not have any wired ranges.</a>
<a name="ln2214"> </a>
<a name="ln2215">	\param addressSpace The address space containing the area.</a>
<a name="ln2216">	\param area The area to be deleted.</a>
<a name="ln2217">	\param deletingAddressSpace \c true, if the address space is in the process</a>
<a name="ln2218">		of being deleted.</a>
<a name="ln2219">*/</a>
<a name="ln2220">static void</a>
<a name="ln2221">delete_area(VMAddressSpace* addressSpace, VMArea* area,</a>
<a name="ln2222">	bool deletingAddressSpace)</a>
<a name="ln2223">{</a>
<a name="ln2224">	ASSERT(!area-&gt;IsWired());</a>
<a name="ln2225"> </a>
<a name="ln2226">	VMAreaHash::Remove(area);</a>
<a name="ln2227"> </a>
<a name="ln2228">	// At this point the area is removed from the global hash table, but</a>
<a name="ln2229">	// still exists in the area list.</a>
<a name="ln2230"> </a>
<a name="ln2231">	// Unmap the virtual address space the area occupied.</a>
<a name="ln2232">	{</a>
<a name="ln2233">		// We need to lock the complete cache chain.</a>
<a name="ln2234">		VMCache* topCache = vm_area_get_locked_cache(area);</a>
<a name="ln2235">		VMCacheChainLocker cacheChainLocker(topCache);</a>
<a name="ln2236">		cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln2237"> </a>
<a name="ln2238">		// If the area's top cache is a temporary cache and the area is the only</a>
<a name="ln2239">		// one referencing it (besides us currently holding a second reference),</a>
<a name="ln2240">		// the unmapping code doesn't need to care about preserving the accessed</a>
<a name="ln2241">		// and dirty flags of the top cache page mappings.</a>
<a name="ln2242">		bool ignoreTopCachePageFlags</a>
<a name="ln2243">			= topCache-&gt;temporary &amp;&amp; topCache-&gt;RefCount() == 2;</a>
<a name="ln2244"> </a>
<a name="ln2245">		area-&gt;address_space-&gt;TranslationMap()-&gt;UnmapArea(area,</a>
<a name="ln2246">			deletingAddressSpace, ignoreTopCachePageFlags);</a>
<a name="ln2247">	}</a>
<a name="ln2248"> </a>
<a name="ln2249">	if (!area-&gt;cache-&gt;temporary)</a>
<a name="ln2250">		area-&gt;cache-&gt;WriteModified();</a>
<a name="ln2251"> </a>
<a name="ln2252">	uint32 allocationFlags = addressSpace == VMAddressSpace::Kernel()</a>
<a name="ln2253">		? HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE : 0;</a>
<a name="ln2254"> </a>
<a name="ln2255">	arch_vm_unset_memory_type(area);</a>
<a name="ln2256">	addressSpace-&gt;RemoveArea(area, allocationFlags);</a>
<a name="ln2257">	addressSpace-&gt;Put();</a>
<a name="ln2258"> </a>
<a name="ln2259">	area-&gt;cache-&gt;RemoveArea(area);</a>
<a name="ln2260">	area-&gt;cache-&gt;ReleaseRef();</a>
<a name="ln2261"> </a>
<a name="ln2262">	addressSpace-&gt;DeleteArea(area, allocationFlags);</a>
<a name="ln2263">}</a>
<a name="ln2264"> </a>
<a name="ln2265"> </a>
<a name="ln2266">status_t</a>
<a name="ln2267">vm_delete_area(team_id team, area_id id, bool kernel)</a>
<a name="ln2268">{</a>
<a name="ln2269">	TRACE((&quot;vm_delete_area(team = 0x%&quot; B_PRIx32 &quot;, area = 0x%&quot; B_PRIx32 &quot;)\n&quot;,</a>
<a name="ln2270">		team, id));</a>
<a name="ln2271"> </a>
<a name="ln2272">	// lock the address space and make sure the area isn't wired</a>
<a name="ln2273">	AddressSpaceWriteLocker locker;</a>
<a name="ln2274">	VMArea* area;</a>
<a name="ln2275">	AreaCacheLocker cacheLocker;</a>
<a name="ln2276"> </a>
<a name="ln2277">	do {</a>
<a name="ln2278">		status_t status = locker.SetFromArea(team, id, area);</a>
<a name="ln2279">		if (status != B_OK)</a>
<a name="ln2280">			return status;</a>
<a name="ln2281"> </a>
<a name="ln2282">		cacheLocker.SetTo(area);</a>
<a name="ln2283">	} while (wait_if_area_is_wired(area, &amp;locker, &amp;cacheLocker));</a>
<a name="ln2284"> </a>
<a name="ln2285">	cacheLocker.Unlock();</a>
<a name="ln2286"> </a>
<a name="ln2287">	// SetFromArea will have returned an error if the area's owning team is not</a>
<a name="ln2288">	// the same as the passed team, so we don't need to do those checks here.</a>
<a name="ln2289"> </a>
<a name="ln2290">	delete_area(locker.AddressSpace(), area, false);</a>
<a name="ln2291">	return B_OK;</a>
<a name="ln2292">}</a>
<a name="ln2293"> </a>
<a name="ln2294"> </a>
<a name="ln2295">/*!	Creates a new cache on top of given cache, moves all areas from</a>
<a name="ln2296">	the old cache to the new one, and changes the protection of all affected</a>
<a name="ln2297">	areas' pages to read-only. If requested, wired pages are moved up to the</a>
<a name="ln2298">	new cache and copies are added to the old cache in their place.</a>
<a name="ln2299">	Preconditions:</a>
<a name="ln2300">	- The given cache must be locked.</a>
<a name="ln2301">	- All of the cache's areas' address spaces must be read locked.</a>
<a name="ln2302">	- Either the cache must not have any wired ranges or a page reservation for</a>
<a name="ln2303">	  all wired pages must be provided, so they can be copied.</a>
<a name="ln2304"> </a>
<a name="ln2305">	\param lowerCache The cache on top of which a new cache shall be created.</a>
<a name="ln2306">	\param wiredPagesReservation If \c NULL there must not be any wired pages</a>
<a name="ln2307">		in \a lowerCache. Otherwise as many pages must be reserved as the cache</a>
<a name="ln2308">		has wired page. The wired pages are copied in this case.</a>
<a name="ln2309">*/</a>
<a name="ln2310">static status_t</a>
<a name="ln2311">vm_copy_on_write_area(VMCache* lowerCache,</a>
<a name="ln2312">	vm_page_reservation* wiredPagesReservation)</a>
<a name="ln2313">{</a>
<a name="ln2314">	VMCache* upperCache;</a>
<a name="ln2315"> </a>
<a name="ln2316">	TRACE((&quot;vm_copy_on_write_area(cache = %p)\n&quot;, lowerCache));</a>
<a name="ln2317"> </a>
<a name="ln2318">	// We need to separate the cache from its areas. The cache goes one level</a>
<a name="ln2319">	// deeper and we create a new cache inbetween.</a>
<a name="ln2320"> </a>
<a name="ln2321">	// create an anonymous cache</a>
<a name="ln2322">	status_t status = VMCacheFactory::CreateAnonymousCache(upperCache, false, 0,</a>
<a name="ln2323">		lowerCache-&gt;GuardSize() / B_PAGE_SIZE,</a>
<a name="ln2324">		dynamic_cast&lt;VMAnonymousNoSwapCache*&gt;(lowerCache) == NULL,</a>
<a name="ln2325">		VM_PRIORITY_USER);</a>
<a name="ln2326">	if (status != B_OK)</a>
<a name="ln2327">		return status;</a>
<a name="ln2328"> </a>
<a name="ln2329">	upperCache-&gt;Lock();</a>
<a name="ln2330"> </a>
<a name="ln2331">	upperCache-&gt;temporary = 1;</a>
<a name="ln2332">	upperCache-&gt;virtual_base = lowerCache-&gt;virtual_base;</a>
<a name="ln2333">	upperCache-&gt;virtual_end = lowerCache-&gt;virtual_end;</a>
<a name="ln2334"> </a>
<a name="ln2335">	// transfer the lower cache areas to the upper cache</a>
<a name="ln2336">	rw_lock_write_lock(&amp;sAreaCacheLock);</a>
<a name="ln2337">	upperCache-&gt;TransferAreas(lowerCache);</a>
<a name="ln2338">	rw_lock_write_unlock(&amp;sAreaCacheLock);</a>
<a name="ln2339"> </a>
<a name="ln2340">	lowerCache-&gt;AddConsumer(upperCache);</a>
<a name="ln2341"> </a>
<a name="ln2342">	// We now need to remap all pages from all of the cache's areas read-only,</a>
<a name="ln2343">	// so that a copy will be created on next write access. If there are wired</a>
<a name="ln2344">	// pages, we keep their protection, move them to the upper cache and create</a>
<a name="ln2345">	// copies for the lower cache.</a>
<a name="ln2346">	if (wiredPagesReservation != NULL) {</a>
<a name="ln2347">		// We need to handle wired pages -- iterate through the cache's pages.</a>
<a name="ln2348">		for (VMCachePagesTree::Iterator it = lowerCache-&gt;pages.GetIterator();</a>
<a name="ln2349">				vm_page* page = it.Next();) {</a>
<a name="ln2350">			if (page-&gt;WiredCount() &gt; 0) {</a>
<a name="ln2351">				// allocate a new page and copy the wired one</a>
<a name="ln2352">				vm_page* copiedPage = vm_page_allocate_page(</a>
<a name="ln2353">					wiredPagesReservation, PAGE_STATE_ACTIVE);</a>
<a name="ln2354"> </a>
<a name="ln2355">				vm_memcpy_physical_page(</a>
<a name="ln2356">					copiedPage-&gt;physical_page_number * B_PAGE_SIZE,</a>
<a name="ln2357">					page-&gt;physical_page_number * B_PAGE_SIZE);</a>
<a name="ln2358"> </a>
<a name="ln2359">				// move the wired page to the upper cache (note: removing is OK</a>
<a name="ln2360">				// with the SplayTree iterator) and insert the copy</a>
<a name="ln2361">				upperCache-&gt;MovePage(page);</a>
<a name="ln2362">				lowerCache-&gt;InsertPage(copiedPage,</a>
<a name="ln2363">					page-&gt;cache_offset * B_PAGE_SIZE);</a>
<a name="ln2364"> </a>
<a name="ln2365">				DEBUG_PAGE_ACCESS_END(copiedPage);</a>
<a name="ln2366">			} else {</a>
<a name="ln2367">				// Change the protection of this page in all areas.</a>
<a name="ln2368">				for (VMArea* tempArea = upperCache-&gt;areas; tempArea != NULL;</a>
<a name="ln2369">						tempArea = tempArea-&gt;cache_next) {</a>
<a name="ln2370">					// The area must be readable in the same way it was</a>
<a name="ln2371">					// previously writable.</a>
<a name="ln2372">					uint32 protection = B_KERNEL_READ_AREA;</a>
<a name="ln2373">					if ((tempArea-&gt;protection &amp; B_READ_AREA) != 0)</a>
<a name="ln2374">						protection |= B_READ_AREA;</a>
<a name="ln2375"> </a>
<a name="ln2376">					VMTranslationMap* map</a>
<a name="ln2377">						= tempArea-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2378">					map-&gt;Lock();</a>
<a name="ln2379">					map-&gt;ProtectPage(tempArea,</a>
<a name="ln2380">						virtual_page_address(tempArea, page), protection);</a>
<a name="ln2381">					map-&gt;Unlock();</a>
<a name="ln2382">				}</a>
<a name="ln2383">			}</a>
<a name="ln2384">		}</a>
<a name="ln2385">	} else {</a>
<a name="ln2386">		ASSERT(lowerCache-&gt;WiredPagesCount() == 0);</a>
<a name="ln2387"> </a>
<a name="ln2388">		// just change the protection of all areas</a>
<a name="ln2389">		for (VMArea* tempArea = upperCache-&gt;areas; tempArea != NULL;</a>
<a name="ln2390">				tempArea = tempArea-&gt;cache_next) {</a>
<a name="ln2391">			// The area must be readable in the same way it was previously</a>
<a name="ln2392">			// writable.</a>
<a name="ln2393">			uint32 protection = B_KERNEL_READ_AREA;</a>
<a name="ln2394">			if ((tempArea-&gt;protection &amp; B_READ_AREA) != 0)</a>
<a name="ln2395">				protection |= B_READ_AREA;</a>
<a name="ln2396"> </a>
<a name="ln2397">			VMTranslationMap* map = tempArea-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2398">			map-&gt;Lock();</a>
<a name="ln2399">			map-&gt;ProtectArea(tempArea, protection);</a>
<a name="ln2400">			map-&gt;Unlock();</a>
<a name="ln2401">		}</a>
<a name="ln2402">	}</a>
<a name="ln2403"> </a>
<a name="ln2404">	vm_area_put_locked_cache(upperCache);</a>
<a name="ln2405"> </a>
<a name="ln2406">	return B_OK;</a>
<a name="ln2407">}</a>
<a name="ln2408"> </a>
<a name="ln2409"> </a>
<a name="ln2410">area_id</a>
<a name="ln2411">vm_copy_area(team_id team, const char* name, void** _address,</a>
<a name="ln2412">	uint32 addressSpec, uint32 protection, area_id sourceID)</a>
<a name="ln2413">{</a>
<a name="ln2414">	bool writableCopy = (protection &amp; (B_KERNEL_WRITE_AREA | B_WRITE_AREA)) != 0;</a>
<a name="ln2415"> </a>
<a name="ln2416">	if ((protection &amp; B_KERNEL_PROTECTION) == 0) {</a>
<a name="ln2417">		// set the same protection for the kernel as for userland</a>
<a name="ln2418">		protection |= B_KERNEL_READ_AREA;</a>
<a name="ln2419">		if (writableCopy)</a>
<a name="ln2420">			protection |= B_KERNEL_WRITE_AREA;</a>
<a name="ln2421">	}</a>
<a name="ln2422"> </a>
<a name="ln2423">	// Do the locking: target address space, all address spaces associated with</a>
<a name="ln2424">	// the source cache, and the cache itself.</a>
<a name="ln2425">	MultiAddressSpaceLocker locker;</a>
<a name="ln2426">	VMAddressSpace* targetAddressSpace;</a>
<a name="ln2427">	VMCache* cache;</a>
<a name="ln2428">	VMArea* source;</a>
<a name="ln2429">	AreaCacheLocker cacheLocker;</a>
<a name="ln2430">	status_t status;</a>
<a name="ln2431">	bool sharedArea;</a>
<a name="ln2432"> </a>
<a name="ln2433">	page_num_t wiredPages = 0;</a>
<a name="ln2434">	vm_page_reservation wiredPagesReservation;</a>
<a name="ln2435"> </a>
<a name="ln2436">	bool restart;</a>
<a name="ln2437">	do {</a>
<a name="ln2438">		restart = false;</a>
<a name="ln2439"> </a>
<a name="ln2440">		locker.Unset();</a>
<a name="ln2441">		status = locker.AddTeam(team, true, &amp;targetAddressSpace);</a>
<a name="ln2442">		if (status == B_OK) {</a>
<a name="ln2443">			status = locker.AddAreaCacheAndLock(sourceID, false, false, source,</a>
<a name="ln2444">				&amp;cache);</a>
<a name="ln2445">		}</a>
<a name="ln2446">		if (status != B_OK)</a>
<a name="ln2447">			return status;</a>
<a name="ln2448"> </a>
<a name="ln2449">		cacheLocker.SetTo(cache, true);	// already locked</a>
<a name="ln2450"> </a>
<a name="ln2451">		sharedArea = (source-&gt;protection &amp; B_SHARED_AREA) != 0;</a>
<a name="ln2452"> </a>
<a name="ln2453">		page_num_t oldWiredPages = wiredPages;</a>
<a name="ln2454">		wiredPages = 0;</a>
<a name="ln2455"> </a>
<a name="ln2456">		// If the source area isn't shared, count the number of wired pages in</a>
<a name="ln2457">		// the cache and reserve as many pages.</a>
<a name="ln2458">		if (!sharedArea) {</a>
<a name="ln2459">			wiredPages = cache-&gt;WiredPagesCount();</a>
<a name="ln2460"> </a>
<a name="ln2461">			if (wiredPages &gt; oldWiredPages) {</a>
<a name="ln2462">				cacheLocker.Unlock();</a>
<a name="ln2463">				locker.Unlock();</a>
<a name="ln2464"> </a>
<a name="ln2465">				if (oldWiredPages &gt; 0)</a>
<a name="ln2466">					vm_page_unreserve_pages(&amp;wiredPagesReservation);</a>
<a name="ln2467"> </a>
<a name="ln2468">				vm_page_reserve_pages(&amp;wiredPagesReservation, wiredPages,</a>
<a name="ln2469">					VM_PRIORITY_USER);</a>
<a name="ln2470"> </a>
<a name="ln2471">				restart = true;</a>
<a name="ln2472">			}</a>
<a name="ln2473">		} else if (oldWiredPages &gt; 0)</a>
<a name="ln2474">			vm_page_unreserve_pages(&amp;wiredPagesReservation);</a>
<a name="ln2475">	} while (restart);</a>
<a name="ln2476"> </a>
<a name="ln2477">	// unreserve pages later</a>
<a name="ln2478">	struct PagesUnreserver {</a>
<a name="ln2479">		PagesUnreserver(vm_page_reservation* reservation)</a>
<a name="ln2480">			:</a>
<a name="ln2481">			fReservation(reservation)</a>
<a name="ln2482">		{</a>
<a name="ln2483">		}</a>
<a name="ln2484"> </a>
<a name="ln2485">		~PagesUnreserver()</a>
<a name="ln2486">		{</a>
<a name="ln2487">			if (fReservation != NULL)</a>
<a name="ln2488">				vm_page_unreserve_pages(fReservation);</a>
<a name="ln2489">		}</a>
<a name="ln2490"> </a>
<a name="ln2491">	private:</a>
<a name="ln2492">		vm_page_reservation*	fReservation;</a>
<a name="ln2493">	} pagesUnreserver(wiredPages &gt; 0 ? &amp;wiredPagesReservation : NULL);</a>
<a name="ln2494"> </a>
<a name="ln2495">	if (addressSpec == B_CLONE_ADDRESS) {</a>
<a name="ln2496">		addressSpec = B_EXACT_ADDRESS;</a>
<a name="ln2497">		*_address = (void*)source-&gt;Base();</a>
<a name="ln2498">	}</a>
<a name="ln2499"> </a>
<a name="ln2500">	// First, create a cache on top of the source area, respectively use the</a>
<a name="ln2501">	// existing one, if this is a shared area.</a>
<a name="ln2502"> </a>
<a name="ln2503">	VMArea* target;</a>
<a name="ln2504">	virtual_address_restrictions addressRestrictions = {};</a>
<a name="ln2505">	addressRestrictions.address = *_address;</a>
<a name="ln2506">	addressRestrictions.address_specification = addressSpec;</a>
<a name="ln2507">	status = map_backing_store(targetAddressSpace, cache, source-&gt;cache_offset,</a>
<a name="ln2508">		name, source-&gt;Size(), source-&gt;wiring, protection,</a>
<a name="ln2509">		sharedArea ? REGION_NO_PRIVATE_MAP : REGION_PRIVATE_MAP,</a>
<a name="ln2510">		writableCopy ? 0 : CREATE_AREA_DONT_COMMIT_MEMORY,</a>
<a name="ln2511">		&amp;addressRestrictions, true, &amp;target, _address);</a>
<a name="ln2512">	if (status &lt; B_OK)</a>
<a name="ln2513">		return status;</a>
<a name="ln2514"> </a>
<a name="ln2515">	if (sharedArea) {</a>
<a name="ln2516">		// The new area uses the old area's cache, but map_backing_store()</a>
<a name="ln2517">		// hasn't acquired a ref. So we have to do that now.</a>
<a name="ln2518">		cache-&gt;AcquireRefLocked();</a>
<a name="ln2519">	}</a>
<a name="ln2520"> </a>
<a name="ln2521">	// If the source area is writable, we need to move it one layer up as well</a>
<a name="ln2522"> </a>
<a name="ln2523">	if (!sharedArea) {</a>
<a name="ln2524">		if ((source-&gt;protection &amp; (B_KERNEL_WRITE_AREA | B_WRITE_AREA)) != 0) {</a>
<a name="ln2525">			// TODO: do something more useful if this fails!</a>
<a name="ln2526">			if (vm_copy_on_write_area(cache,</a>
<a name="ln2527">					wiredPages &gt; 0 ? &amp;wiredPagesReservation : NULL) &lt; B_OK) {</a>
<a name="ln2528">				panic(&quot;vm_copy_on_write_area() failed!\n&quot;);</a>
<a name="ln2529">			}</a>
<a name="ln2530">		}</a>
<a name="ln2531">	}</a>
<a name="ln2532"> </a>
<a name="ln2533">	// we return the ID of the newly created area</a>
<a name="ln2534">	return target-&gt;id;</a>
<a name="ln2535">}</a>
<a name="ln2536"> </a>
<a name="ln2537"> </a>
<a name="ln2538">status_t</a>
<a name="ln2539">vm_set_area_protection(team_id team, area_id areaID, uint32 newProtection,</a>
<a name="ln2540">	bool kernel)</a>
<a name="ln2541">{</a>
<a name="ln2542">	fix_protection(&amp;newProtection);</a>
<a name="ln2543"> </a>
<a name="ln2544">	TRACE((&quot;vm_set_area_protection(team = %#&quot; B_PRIx32 &quot;, area = %#&quot; B_PRIx32</a>
<a name="ln2545">		&quot;, protection = %#&quot; B_PRIx32 &quot;)\n&quot;, team, areaID, newProtection));</a>
<a name="ln2546"> </a>
<a name="ln2547">	if (!arch_vm_supports_protection(newProtection))</a>
<a name="ln2548">		return B_NOT_SUPPORTED;</a>
<a name="ln2549"> </a>
<a name="ln2550">	bool becomesWritable</a>
<a name="ln2551">		= (newProtection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0;</a>
<a name="ln2552"> </a>
<a name="ln2553">	// lock address spaces and cache</a>
<a name="ln2554">	MultiAddressSpaceLocker locker;</a>
<a name="ln2555">	VMCache* cache;</a>
<a name="ln2556">	VMArea* area;</a>
<a name="ln2557">	status_t status;</a>
<a name="ln2558">	AreaCacheLocker cacheLocker;</a>
<a name="ln2559">	bool isWritable;</a>
<a name="ln2560"> </a>
<a name="ln2561">	bool restart;</a>
<a name="ln2562">	do {</a>
<a name="ln2563">		restart = false;</a>
<a name="ln2564"> </a>
<a name="ln2565">		locker.Unset();</a>
<a name="ln2566">		status = locker.AddAreaCacheAndLock(areaID, true, false, area, &amp;cache);</a>
<a name="ln2567">		if (status != B_OK)</a>
<a name="ln2568">			return status;</a>
<a name="ln2569"> </a>
<a name="ln2570">		cacheLocker.SetTo(cache, true);	// already locked</a>
<a name="ln2571"> </a>
<a name="ln2572">		if (!kernel &amp;&amp; area-&gt;address_space == VMAddressSpace::Kernel()) {</a>
<a name="ln2573">			dprintf(&quot;vm_set_area_protection: team %&quot; B_PRId32 &quot; tried to &quot;</a>
<a name="ln2574">				&quot;set protection %#&quot; B_PRIx32 &quot; on kernel area %&quot; B_PRId32</a>
<a name="ln2575">				&quot; (%s)\n&quot;, team, newProtection, areaID, area-&gt;name);</a>
<a name="ln2576">			return B_NOT_ALLOWED;</a>
<a name="ln2577">		}</a>
<a name="ln2578"> </a>
<a name="ln2579">		if (area-&gt;protection == newProtection)</a>
<a name="ln2580">			return B_OK;</a>
<a name="ln2581"> </a>
<a name="ln2582">		if (team != VMAddressSpace::KernelID()</a>
<a name="ln2583">			&amp;&amp; area-&gt;address_space-&gt;ID() != team) {</a>
<a name="ln2584">			// unless you're the kernel, you are only allowed to set</a>
<a name="ln2585">			// the protection of your own areas</a>
<a name="ln2586">			return B_NOT_ALLOWED;</a>
<a name="ln2587">		}</a>
<a name="ln2588"> </a>
<a name="ln2589">		isWritable</a>
<a name="ln2590">			= (area-&gt;protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0;</a>
<a name="ln2591"> </a>
<a name="ln2592">		// Make sure the area (respectively, if we're going to call</a>
<a name="ln2593">		// vm_copy_on_write_area(), all areas of the cache) doesn't have any</a>
<a name="ln2594">		// wired ranges.</a>
<a name="ln2595">		if (!isWritable &amp;&amp; becomesWritable &amp;&amp; !cache-&gt;consumers.IsEmpty()) {</a>
<a name="ln2596">			for (VMArea* otherArea = cache-&gt;areas; otherArea != NULL;</a>
<a name="ln2597">					otherArea = otherArea-&gt;cache_next) {</a>
<a name="ln2598">				if (wait_if_area_is_wired(otherArea, &amp;locker, &amp;cacheLocker)) {</a>
<a name="ln2599">					restart = true;</a>
<a name="ln2600">					break;</a>
<a name="ln2601">				}</a>
<a name="ln2602">			}</a>
<a name="ln2603">		} else {</a>
<a name="ln2604">			if (wait_if_area_is_wired(area, &amp;locker, &amp;cacheLocker))</a>
<a name="ln2605">				restart = true;</a>
<a name="ln2606">		}</a>
<a name="ln2607">	} while (restart);</a>
<a name="ln2608"> </a>
<a name="ln2609">	bool changePageProtection = true;</a>
<a name="ln2610">	bool changeTopCachePagesOnly = false;</a>
<a name="ln2611"> </a>
<a name="ln2612">	if (isWritable &amp;&amp; !becomesWritable) {</a>
<a name="ln2613">		// writable -&gt; !writable</a>
<a name="ln2614"> </a>
<a name="ln2615">		if (cache-&gt;source != NULL &amp;&amp; cache-&gt;temporary) {</a>
<a name="ln2616">			if (cache-&gt;CountWritableAreas(area) == 0) {</a>
<a name="ln2617">				// Since this cache now lives from the pages in its source cache,</a>
<a name="ln2618">				// we can change the cache's commitment to take only those pages</a>
<a name="ln2619">				// into account that really are in this cache.</a>
<a name="ln2620"> </a>
<a name="ln2621">				status = cache-&gt;Commit(cache-&gt;page_count * B_PAGE_SIZE,</a>
<a name="ln2622">					team == VMAddressSpace::KernelID()</a>
<a name="ln2623">						? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln2624"> </a>
<a name="ln2625">				// TODO: we may be able to join with our source cache, if</a>
<a name="ln2626">				// count == 0</a>
<a name="ln2627">			}</a>
<a name="ln2628">		}</a>
<a name="ln2629"> </a>
<a name="ln2630">		// If only the writability changes, we can just remap the pages of the</a>
<a name="ln2631">		// top cache, since the pages of lower caches are mapped read-only</a>
<a name="ln2632">		// anyway. That's advantageous only, if the number of pages in the cache</a>
<a name="ln2633">		// is significantly smaller than the number of pages in the area,</a>
<a name="ln2634">		// though.</a>
<a name="ln2635">		if (newProtection</a>
<a name="ln2636">				== (area-&gt;protection &amp; ~(B_WRITE_AREA | B_KERNEL_WRITE_AREA))</a>
<a name="ln2637">			&amp;&amp; cache-&gt;page_count * 2 &lt; area-&gt;Size() / B_PAGE_SIZE) {</a>
<a name="ln2638">			changeTopCachePagesOnly = true;</a>
<a name="ln2639">		}</a>
<a name="ln2640">	} else if (!isWritable &amp;&amp; becomesWritable) {</a>
<a name="ln2641">		// !writable -&gt; writable</a>
<a name="ln2642"> </a>
<a name="ln2643">		if (!cache-&gt;consumers.IsEmpty()) {</a>
<a name="ln2644">			// There are consumers -- we have to insert a new cache. Fortunately</a>
<a name="ln2645">			// vm_copy_on_write_area() does everything that's needed.</a>
<a name="ln2646">			changePageProtection = false;</a>
<a name="ln2647">			status = vm_copy_on_write_area(cache, NULL);</a>
<a name="ln2648">		} else {</a>
<a name="ln2649">			// No consumers, so we don't need to insert a new one.</a>
<a name="ln2650">			if (cache-&gt;source != NULL &amp;&amp; cache-&gt;temporary) {</a>
<a name="ln2651">				// the cache's commitment must contain all possible pages</a>
<a name="ln2652">				status = cache-&gt;Commit(cache-&gt;virtual_end - cache-&gt;virtual_base,</a>
<a name="ln2653">					team == VMAddressSpace::KernelID()</a>
<a name="ln2654">						? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln2655">			}</a>
<a name="ln2656"> </a>
<a name="ln2657">			if (status == B_OK &amp;&amp; cache-&gt;source != NULL) {</a>
<a name="ln2658">				// There's a source cache, hence we can't just change all pages'</a>
<a name="ln2659">				// protection or we might allow writing into pages belonging to</a>
<a name="ln2660">				// a lower cache.</a>
<a name="ln2661">				changeTopCachePagesOnly = true;</a>
<a name="ln2662">			}</a>
<a name="ln2663">		}</a>
<a name="ln2664">	} else {</a>
<a name="ln2665">		// we don't have anything special to do in all other cases</a>
<a name="ln2666">	}</a>
<a name="ln2667"> </a>
<a name="ln2668">	if (status == B_OK) {</a>
<a name="ln2669">		// remap existing pages in this cache</a>
<a name="ln2670">		if (changePageProtection) {</a>
<a name="ln2671">			VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2672">			map-&gt;Lock();</a>
<a name="ln2673"> </a>
<a name="ln2674">			if (changeTopCachePagesOnly) {</a>
<a name="ln2675">				page_num_t firstPageOffset = area-&gt;cache_offset / B_PAGE_SIZE;</a>
<a name="ln2676">				page_num_t lastPageOffset</a>
<a name="ln2677">					= firstPageOffset + area-&gt;Size() / B_PAGE_SIZE;</a>
<a name="ln2678">				for (VMCachePagesTree::Iterator it = cache-&gt;pages.GetIterator();</a>
<a name="ln2679">						vm_page* page = it.Next();) {</a>
<a name="ln2680">					if (page-&gt;cache_offset &gt;= firstPageOffset</a>
<a name="ln2681">						&amp;&amp; page-&gt;cache_offset &lt;= lastPageOffset) {</a>
<a name="ln2682">						addr_t address = virtual_page_address(area, page);</a>
<a name="ln2683">						map-&gt;ProtectPage(area, address, newProtection);</a>
<a name="ln2684">					}</a>
<a name="ln2685">				}</a>
<a name="ln2686">			} else</a>
<a name="ln2687">				map-&gt;ProtectArea(area, newProtection);</a>
<a name="ln2688"> </a>
<a name="ln2689">			map-&gt;Unlock();</a>
<a name="ln2690">		}</a>
<a name="ln2691"> </a>
<a name="ln2692">		area-&gt;protection = newProtection;</a>
<a name="ln2693">	}</a>
<a name="ln2694"> </a>
<a name="ln2695">	return status;</a>
<a name="ln2696">}</a>
<a name="ln2697"> </a>
<a name="ln2698"> </a>
<a name="ln2699">status_t</a>
<a name="ln2700">vm_get_page_mapping(team_id team, addr_t vaddr, phys_addr_t* paddr)</a>
<a name="ln2701">{</a>
<a name="ln2702">	VMAddressSpace* addressSpace = VMAddressSpace::Get(team);</a>
<a name="ln2703">	if (addressSpace == NULL)</a>
<a name="ln2704">		return B_BAD_TEAM_ID;</a>
<a name="ln2705"> </a>
<a name="ln2706">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln2707"> </a>
<a name="ln2708">	map-&gt;Lock();</a>
<a name="ln2709">	uint32 dummyFlags;</a>
<a name="ln2710">	status_t status = map-&gt;Query(vaddr, paddr, &amp;dummyFlags);</a>
<a name="ln2711">	map-&gt;Unlock();</a>
<a name="ln2712"> </a>
<a name="ln2713">	addressSpace-&gt;Put();</a>
<a name="ln2714">	return status;</a>
<a name="ln2715">}</a>
<a name="ln2716"> </a>
<a name="ln2717"> </a>
<a name="ln2718">/*!	The page's cache must be locked.</a>
<a name="ln2719">*/</a>
<a name="ln2720">bool</a>
<a name="ln2721">vm_test_map_modification(vm_page* page)</a>
<a name="ln2722">{</a>
<a name="ln2723">	if (page-&gt;modified)</a>
<a name="ln2724">		return true;</a>
<a name="ln2725"> </a>
<a name="ln2726">	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();</a>
<a name="ln2727">	vm_page_mapping* mapping;</a>
<a name="ln2728">	while ((mapping = iterator.Next()) != NULL) {</a>
<a name="ln2729">		VMArea* area = mapping-&gt;area;</a>
<a name="ln2730">		VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2731"> </a>
<a name="ln2732">		phys_addr_t physicalAddress;</a>
<a name="ln2733">		uint32 flags;</a>
<a name="ln2734">		map-&gt;Lock();</a>
<a name="ln2735">		map-&gt;Query(virtual_page_address(area, page), &amp;physicalAddress, &amp;flags);</a>
<a name="ln2736">		map-&gt;Unlock();</a>
<a name="ln2737"> </a>
<a name="ln2738">		if ((flags &amp; PAGE_MODIFIED) != 0)</a>
<a name="ln2739">			return true;</a>
<a name="ln2740">	}</a>
<a name="ln2741"> </a>
<a name="ln2742">	return false;</a>
<a name="ln2743">}</a>
<a name="ln2744"> </a>
<a name="ln2745"> </a>
<a name="ln2746">/*!	The page's cache must be locked.</a>
<a name="ln2747">*/</a>
<a name="ln2748">void</a>
<a name="ln2749">vm_clear_map_flags(vm_page* page, uint32 flags)</a>
<a name="ln2750">{</a>
<a name="ln2751">	if ((flags &amp; PAGE_ACCESSED) != 0)</a>
<a name="ln2752">		page-&gt;accessed = false;</a>
<a name="ln2753">	if ((flags &amp; PAGE_MODIFIED) != 0)</a>
<a name="ln2754">		page-&gt;modified = false;</a>
<a name="ln2755"> </a>
<a name="ln2756">	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();</a>
<a name="ln2757">	vm_page_mapping* mapping;</a>
<a name="ln2758">	while ((mapping = iterator.Next()) != NULL) {</a>
<a name="ln2759">		VMArea* area = mapping-&gt;area;</a>
<a name="ln2760">		VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2761"> </a>
<a name="ln2762">		map-&gt;Lock();</a>
<a name="ln2763">		map-&gt;ClearFlags(virtual_page_address(area, page), flags);</a>
<a name="ln2764">		map-&gt;Unlock();</a>
<a name="ln2765">	}</a>
<a name="ln2766">}</a>
<a name="ln2767"> </a>
<a name="ln2768"> </a>
<a name="ln2769">/*!	Removes all mappings from a page.</a>
<a name="ln2770">	After you've called this function, the page is unmapped from memory and</a>
<a name="ln2771">	the page's \c accessed and \c modified flags have been updated according</a>
<a name="ln2772">	to the state of the mappings.</a>
<a name="ln2773">	The page's cache must be locked.</a>
<a name="ln2774">*/</a>
<a name="ln2775">void</a>
<a name="ln2776">vm_remove_all_page_mappings(vm_page* page)</a>
<a name="ln2777">{</a>
<a name="ln2778">	while (vm_page_mapping* mapping = page-&gt;mappings.Head()) {</a>
<a name="ln2779">		VMArea* area = mapping-&gt;area;</a>
<a name="ln2780">		VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2781">		addr_t address = virtual_page_address(area, page);</a>
<a name="ln2782">		map-&gt;UnmapPage(area, address, false);</a>
<a name="ln2783">	}</a>
<a name="ln2784">}</a>
<a name="ln2785"> </a>
<a name="ln2786"> </a>
<a name="ln2787">int32</a>
<a name="ln2788">vm_clear_page_mapping_accessed_flags(struct vm_page *page)</a>
<a name="ln2789">{</a>
<a name="ln2790">	int32 count = 0;</a>
<a name="ln2791"> </a>
<a name="ln2792">	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();</a>
<a name="ln2793">	vm_page_mapping* mapping;</a>
<a name="ln2794">	while ((mapping = iterator.Next()) != NULL) {</a>
<a name="ln2795">		VMArea* area = mapping-&gt;area;</a>
<a name="ln2796">		VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2797"> </a>
<a name="ln2798">		bool modified;</a>
<a name="ln2799">		if (map-&gt;ClearAccessedAndModified(area,</a>
<a name="ln2800">				virtual_page_address(area, page), false, modified)) {</a>
<a name="ln2801">			count++;</a>
<a name="ln2802">		}</a>
<a name="ln2803"> </a>
<a name="ln2804">		page-&gt;modified |= modified;</a>
<a name="ln2805">	}</a>
<a name="ln2806"> </a>
<a name="ln2807"> </a>
<a name="ln2808">	if (page-&gt;accessed) {</a>
<a name="ln2809">		count++;</a>
<a name="ln2810">		page-&gt;accessed = false;</a>
<a name="ln2811">	}</a>
<a name="ln2812"> </a>
<a name="ln2813">	return count;</a>
<a name="ln2814">}</a>
<a name="ln2815"> </a>
<a name="ln2816"> </a>
<a name="ln2817">/*!	Removes all mappings of a page and/or clears the accessed bits of the</a>
<a name="ln2818">	mappings.</a>
<a name="ln2819">	The function iterates through the page mappings and removes them until</a>
<a name="ln2820">	encountering one that has been accessed. From then on it will continue to</a>
<a name="ln2821">	iterate, but only clear the accessed flag of the mapping. The page's</a>
<a name="ln2822">	\c modified bit will be updated accordingly, the \c accessed bit will be</a>
<a name="ln2823">	cleared.</a>
<a name="ln2824">	\return The number of mapping accessed bits encountered, including the</a>
<a name="ln2825">		\c accessed bit of the page itself. If \c 0 is returned, all mappings</a>
<a name="ln2826">		of the page have been removed.</a>
<a name="ln2827">*/</a>
<a name="ln2828">int32</a>
<a name="ln2829">vm_remove_all_page_mappings_if_unaccessed(struct vm_page *page)</a>
<a name="ln2830">{</a>
<a name="ln2831">	ASSERT(page-&gt;WiredCount() == 0);</a>
<a name="ln2832"> </a>
<a name="ln2833">	if (page-&gt;accessed)</a>
<a name="ln2834">		return vm_clear_page_mapping_accessed_flags(page);</a>
<a name="ln2835"> </a>
<a name="ln2836">	while (vm_page_mapping* mapping = page-&gt;mappings.Head()) {</a>
<a name="ln2837">		VMArea* area = mapping-&gt;area;</a>
<a name="ln2838">		VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln2839">		addr_t address = virtual_page_address(area, page);</a>
<a name="ln2840">		bool modified = false;</a>
<a name="ln2841">		if (map-&gt;ClearAccessedAndModified(area, address, true, modified)) {</a>
<a name="ln2842">			page-&gt;accessed = true;</a>
<a name="ln2843">			page-&gt;modified |= modified;</a>
<a name="ln2844">			return vm_clear_page_mapping_accessed_flags(page);</a>
<a name="ln2845">		}</a>
<a name="ln2846">		page-&gt;modified |= modified;</a>
<a name="ln2847">	}</a>
<a name="ln2848"> </a>
<a name="ln2849">	return 0;</a>
<a name="ln2850">}</a>
<a name="ln2851"> </a>
<a name="ln2852"> </a>
<a name="ln2853">static int</a>
<a name="ln2854">display_mem(int argc, char** argv)</a>
<a name="ln2855">{</a>
<a name="ln2856">	bool physical = false;</a>
<a name="ln2857">	addr_t copyAddress;</a>
<a name="ln2858">	int32 displayWidth;</a>
<a name="ln2859">	int32 itemSize;</a>
<a name="ln2860">	int32 num = -1;</a>
<a name="ln2861">	addr_t address;</a>
<a name="ln2862">	int i = 1, j;</a>
<a name="ln2863"> </a>
<a name="ln2864">	if (argc &gt; 1 &amp;&amp; argv[1][0] == '-') {</a>
<a name="ln2865">		if (!strcmp(argv[1], &quot;-p&quot;) || !strcmp(argv[1], &quot;--physical&quot;)) {</a>
<a name="ln2866">			physical = true;</a>
<a name="ln2867">			i++;</a>
<a name="ln2868">		} else</a>
<a name="ln2869">			i = 99;</a>
<a name="ln2870">	}</a>
<a name="ln2871"> </a>
<a name="ln2872">	if (argc &lt; i + 1 || argc &gt; i + 2) {</a>
<a name="ln2873">		kprintf(&quot;usage: dl/dw/ds/db/string [-p|--physical] &lt;address&gt; [num]\n&quot;</a>
<a name="ln2874">			&quot;\tdl - 8 bytes\n&quot;</a>
<a name="ln2875">			&quot;\tdw - 4 bytes\n&quot;</a>
<a name="ln2876">			&quot;\tds - 2 bytes\n&quot;</a>
<a name="ln2877">			&quot;\tdb - 1 byte\n&quot;</a>
<a name="ln2878">			&quot;\tstring - a whole string\n&quot;</a>
<a name="ln2879">			&quot;  -p or --physical only allows memory from a single page to be &quot;</a>
<a name="ln2880">			&quot;displayed.\n&quot;);</a>
<a name="ln2881">		return 0;</a>
<a name="ln2882">	}</a>
<a name="ln2883"> </a>
<a name="ln2884">	address = parse_expression(argv[i]);</a>
<a name="ln2885"> </a>
<a name="ln2886">	if (argc &gt; i + 1)</a>
<a name="ln2887">		num = parse_expression(argv[i + 1]);</a>
<a name="ln2888"> </a>
<a name="ln2889">	// build the format string</a>
<a name="ln2890">	if (strcmp(argv[0], &quot;db&quot;) == 0) {</a>
<a name="ln2891">		itemSize = 1;</a>
<a name="ln2892">		displayWidth = 16;</a>
<a name="ln2893">	} else if (strcmp(argv[0], &quot;ds&quot;) == 0) {</a>
<a name="ln2894">		itemSize = 2;</a>
<a name="ln2895">		displayWidth = 8;</a>
<a name="ln2896">	} else if (strcmp(argv[0], &quot;dw&quot;) == 0) {</a>
<a name="ln2897">		itemSize = 4;</a>
<a name="ln2898">		displayWidth = 4;</a>
<a name="ln2899">	} else if (strcmp(argv[0], &quot;dl&quot;) == 0) {</a>
<a name="ln2900">		itemSize = 8;</a>
<a name="ln2901">		displayWidth = 2;</a>
<a name="ln2902">	} else if (strcmp(argv[0], &quot;string&quot;) == 0) {</a>
<a name="ln2903">		itemSize = 1;</a>
<a name="ln2904">		displayWidth = -1;</a>
<a name="ln2905">	} else {</a>
<a name="ln2906">		kprintf(&quot;display_mem called in an invalid way!\n&quot;);</a>
<a name="ln2907">		return 0;</a>
<a name="ln2908">	}</a>
<a name="ln2909"> </a>
<a name="ln2910">	if (num &lt;= 0)</a>
<a name="ln2911">		num = displayWidth;</a>
<a name="ln2912"> </a>
<a name="ln2913">	void* physicalPageHandle = NULL;</a>
<a name="ln2914"> </a>
<a name="ln2915">	if (physical) {</a>
<a name="ln2916">		int32 offset = address &amp; (B_PAGE_SIZE - 1);</a>
<a name="ln2917">		if (num * itemSize + offset &gt; B_PAGE_SIZE) {</a>
<a name="ln2918">			num = (B_PAGE_SIZE - offset) / itemSize;</a>
<a name="ln2919">			kprintf(&quot;NOTE: number of bytes has been cut to page size\n&quot;);</a>
<a name="ln2920">		}</a>
<a name="ln2921"> </a>
<a name="ln2922">		address = ROUNDDOWN(address, B_PAGE_SIZE);</a>
<a name="ln2923"> </a>
<a name="ln2924">		if (vm_get_physical_page_debug(address, &amp;copyAddress,</a>
<a name="ln2925">				&amp;physicalPageHandle) != B_OK) {</a>
<a name="ln2926">			kprintf(&quot;getting the hardware page failed.&quot;);</a>
<a name="ln2927">			return 0;</a>
<a name="ln2928">		}</a>
<a name="ln2929"> </a>
<a name="ln2930">		address += offset;</a>
<a name="ln2931">		copyAddress += offset;</a>
<a name="ln2932">	} else</a>
<a name="ln2933">		copyAddress = address;</a>
<a name="ln2934"> </a>
<a name="ln2935">	if (!strcmp(argv[0], &quot;string&quot;)) {</a>
<a name="ln2936">		kprintf(&quot;%p \&quot;&quot;, (char*)copyAddress);</a>
<a name="ln2937"> </a>
<a name="ln2938">		// string mode</a>
<a name="ln2939">		for (i = 0; true; i++) {</a>
<a name="ln2940">			char c;</a>
<a name="ln2941">			if (debug_memcpy(B_CURRENT_TEAM, &amp;c, (char*)copyAddress + i, 1)</a>
<a name="ln2942">					!= B_OK</a>
<a name="ln2943">				|| c == '\0') {</a>
<a name="ln2944">				break;</a>
<a name="ln2945">			}</a>
<a name="ln2946"> </a>
<a name="ln2947">			if (c == '\n')</a>
<a name="ln2948">				kprintf(&quot;\\n&quot;);</a>
<a name="ln2949">			else if (c == '\t')</a>
<a name="ln2950">				kprintf(&quot;\\t&quot;);</a>
<a name="ln2951">			else {</a>
<a name="ln2952">				if (!isprint(c))</a>
<a name="ln2953">					c = '.';</a>
<a name="ln2954"> </a>
<a name="ln2955">				kprintf(&quot;%c&quot;, c);</a>
<a name="ln2956">			}</a>
<a name="ln2957">		}</a>
<a name="ln2958"> </a>
<a name="ln2959">		kprintf(&quot;\&quot;\n&quot;);</a>
<a name="ln2960">	} else {</a>
<a name="ln2961">		// number mode</a>
<a name="ln2962">		for (i = 0; i &lt; num; i++) {</a>
<a name="ln2963">			uint64 value;</a>
<a name="ln2964"> </a>
<a name="ln2965">			if ((i % displayWidth) == 0) {</a>
<a name="ln2966">				int32 displayed = min_c(displayWidth, (num-i)) * itemSize;</a>
<a name="ln2967">				if (i != 0)</a>
<a name="ln2968">					kprintf(&quot;\n&quot;);</a>
<a name="ln2969"> </a>
<a name="ln2970">				kprintf(&quot;[0x%lx]  &quot;, address + i * itemSize);</a>
<a name="ln2971"> </a>
<a name="ln2972">				for (j = 0; j &lt; displayed; j++) {</a>
<a name="ln2973">					char c;</a>
<a name="ln2974">					if (debug_memcpy(B_CURRENT_TEAM, &amp;c,</a>
<a name="ln2975">							(char*)copyAddress + i * itemSize + j, 1) != B_OK) {</a>
<a name="ln2976">						displayed = j;</a>
<a name="ln2977">						break;</a>
<a name="ln2978">					}</a>
<a name="ln2979">					if (!isprint(c))</a>
<a name="ln2980">						c = '.';</a>
<a name="ln2981"> </a>
<a name="ln2982">					kprintf(&quot;%c&quot;, c);</a>
<a name="ln2983">				}</a>
<a name="ln2984">				if (num &gt; displayWidth) {</a>
<a name="ln2985">					// make sure the spacing in the last line is correct</a>
<a name="ln2986">					for (j = displayed; j &lt; displayWidth * itemSize; j++)</a>
<a name="ln2987">						kprintf(&quot; &quot;);</a>
<a name="ln2988">				}</a>
<a name="ln2989">				kprintf(&quot;  &quot;);</a>
<a name="ln2990">			}</a>
<a name="ln2991"> </a>
<a name="ln2992">			if (debug_memcpy(B_CURRENT_TEAM, &amp;value,</a>
<a name="ln2993">					(uint8*)copyAddress + i * itemSize, itemSize) != B_OK) {</a>
<a name="ln2994">				kprintf(&quot;read fault&quot;);</a>
<a name="ln2995">				break;</a>
<a name="ln2996">			}</a>
<a name="ln2997"> </a>
<a name="ln2998">			switch (itemSize) {</a>
<a name="ln2999">				case 1:</a>
<a name="ln3000">					kprintf(&quot; %02&quot; B_PRIx8, *(uint8*)&amp;value);</a>
<a name="ln3001">					break;</a>
<a name="ln3002">				case 2:</a>
<a name="ln3003">					kprintf(&quot; %04&quot; B_PRIx16, *(uint16*)&amp;value);</a>
<a name="ln3004">					break;</a>
<a name="ln3005">				case 4:</a>
<a name="ln3006">					kprintf(&quot; %08&quot; B_PRIx32, *(uint32*)&amp;value);</a>
<a name="ln3007">					break;</a>
<a name="ln3008">				case 8:</a>
<a name="ln3009">					kprintf(&quot; %016&quot; B_PRIx64, *(uint64*)&amp;value);</a>
<a name="ln3010">					break;</a>
<a name="ln3011">			}</a>
<a name="ln3012">		}</a>
<a name="ln3013"> </a>
<a name="ln3014">		kprintf(&quot;\n&quot;);</a>
<a name="ln3015">	}</a>
<a name="ln3016"> </a>
<a name="ln3017">	if (physical) {</a>
<a name="ln3018">		copyAddress = ROUNDDOWN(copyAddress, B_PAGE_SIZE);</a>
<a name="ln3019">		vm_put_physical_page_debug(copyAddress, physicalPageHandle);</a>
<a name="ln3020">	}</a>
<a name="ln3021">	return 0;</a>
<a name="ln3022">}</a>
<a name="ln3023"> </a>
<a name="ln3024"> </a>
<a name="ln3025">static void</a>
<a name="ln3026">dump_cache_tree_recursively(VMCache* cache, int level,</a>
<a name="ln3027">	VMCache* highlightCache)</a>
<a name="ln3028">{</a>
<a name="ln3029">	// print this cache</a>
<a name="ln3030">	for (int i = 0; i &lt; level; i++)</a>
<a name="ln3031">		kprintf(&quot;  &quot;);</a>
<a name="ln3032">	if (cache == highlightCache)</a>
<a name="ln3033">		kprintf(&quot;%p &lt;--\n&quot;, cache);</a>
<a name="ln3034">	else</a>
<a name="ln3035">		kprintf(&quot;%p\n&quot;, cache);</a>
<a name="ln3036"> </a>
<a name="ln3037">	// recursively print its consumers</a>
<a name="ln3038">	for (VMCache::ConsumerList::Iterator it = cache-&gt;consumers.GetIterator();</a>
<a name="ln3039">			VMCache* consumer = it.Next();) {</a>
<a name="ln3040">		dump_cache_tree_recursively(consumer, level + 1, highlightCache);</a>
<a name="ln3041">	}</a>
<a name="ln3042">}</a>
<a name="ln3043"> </a>
<a name="ln3044"> </a>
<a name="ln3045">static int</a>
<a name="ln3046">dump_cache_tree(int argc, char** argv)</a>
<a name="ln3047">{</a>
<a name="ln3048">	if (argc != 2 || !strcmp(argv[1], &quot;--help&quot;)) {</a>
<a name="ln3049">		kprintf(&quot;usage: %s &lt;address&gt;\n&quot;, argv[0]);</a>
<a name="ln3050">		return 0;</a>
<a name="ln3051">	}</a>
<a name="ln3052"> </a>
<a name="ln3053">	addr_t address = parse_expression(argv[1]);</a>
<a name="ln3054">	if (address == 0)</a>
<a name="ln3055">		return 0;</a>
<a name="ln3056"> </a>
<a name="ln3057">	VMCache* cache = (VMCache*)address;</a>
<a name="ln3058">	VMCache* root = cache;</a>
<a name="ln3059"> </a>
<a name="ln3060">	// find the root cache (the transitive source)</a>
<a name="ln3061">	while (root-&gt;source != NULL)</a>
<a name="ln3062">		root = root-&gt;source;</a>
<a name="ln3063"> </a>
<a name="ln3064">	dump_cache_tree_recursively(root, 0, cache);</a>
<a name="ln3065"> </a>
<a name="ln3066">	return 0;</a>
<a name="ln3067">}</a>
<a name="ln3068"> </a>
<a name="ln3069"> </a>
<a name="ln3070">const char*</a>
<a name="ln3071">vm_cache_type_to_string(int32 type)</a>
<a name="ln3072">{</a>
<a name="ln3073">	switch (type) {</a>
<a name="ln3074">		case CACHE_TYPE_RAM:</a>
<a name="ln3075">			return &quot;RAM&quot;;</a>
<a name="ln3076">		case CACHE_TYPE_DEVICE:</a>
<a name="ln3077">			return &quot;device&quot;;</a>
<a name="ln3078">		case CACHE_TYPE_VNODE:</a>
<a name="ln3079">			return &quot;vnode&quot;;</a>
<a name="ln3080">		case CACHE_TYPE_NULL:</a>
<a name="ln3081">			return &quot;null&quot;;</a>
<a name="ln3082"> </a>
<a name="ln3083">		default:</a>
<a name="ln3084">			return &quot;unknown&quot;;</a>
<a name="ln3085">	}</a>
<a name="ln3086">}</a>
<a name="ln3087"> </a>
<a name="ln3088"> </a>
<a name="ln3089">#if DEBUG_CACHE_LIST</a>
<a name="ln3090"> </a>
<a name="ln3091">static void</a>
<a name="ln3092">update_cache_info_recursively(VMCache* cache, cache_info&amp; info)</a>
<a name="ln3093">{</a>
<a name="ln3094">	info.page_count += cache-&gt;page_count;</a>
<a name="ln3095">	if (cache-&gt;type == CACHE_TYPE_RAM)</a>
<a name="ln3096">		info.committed += cache-&gt;committed_size;</a>
<a name="ln3097"> </a>
<a name="ln3098">	// recurse</a>
<a name="ln3099">	for (VMCache::ConsumerList::Iterator it = cache-&gt;consumers.GetIterator();</a>
<a name="ln3100">			VMCache* consumer = it.Next();) {</a>
<a name="ln3101">		update_cache_info_recursively(consumer, info);</a>
<a name="ln3102">	}</a>
<a name="ln3103">}</a>
<a name="ln3104"> </a>
<a name="ln3105"> </a>
<a name="ln3106">static int</a>
<a name="ln3107">cache_info_compare_page_count(const void* _a, const void* _b)</a>
<a name="ln3108">{</a>
<a name="ln3109">	const cache_info* a = (const cache_info*)_a;</a>
<a name="ln3110">	const cache_info* b = (const cache_info*)_b;</a>
<a name="ln3111">	if (a-&gt;page_count == b-&gt;page_count)</a>
<a name="ln3112">		return 0;</a>
<a name="ln3113">	return a-&gt;page_count &lt; b-&gt;page_count ? 1 : -1;</a>
<a name="ln3114">}</a>
<a name="ln3115"> </a>
<a name="ln3116"> </a>
<a name="ln3117">static int</a>
<a name="ln3118">cache_info_compare_committed(const void* _a, const void* _b)</a>
<a name="ln3119">{</a>
<a name="ln3120">	const cache_info* a = (const cache_info*)_a;</a>
<a name="ln3121">	const cache_info* b = (const cache_info*)_b;</a>
<a name="ln3122">	if (a-&gt;committed == b-&gt;committed)</a>
<a name="ln3123">		return 0;</a>
<a name="ln3124">	return a-&gt;committed &lt; b-&gt;committed ? 1 : -1;</a>
<a name="ln3125">}</a>
<a name="ln3126"> </a>
<a name="ln3127"> </a>
<a name="ln3128">static void</a>
<a name="ln3129">dump_caches_recursively(VMCache* cache, cache_info&amp; info, int level)</a>
<a name="ln3130">{</a>
<a name="ln3131">	for (int i = 0; i &lt; level; i++)</a>
<a name="ln3132">		kprintf(&quot;  &quot;);</a>
<a name="ln3133"> </a>
<a name="ln3134">	kprintf(&quot;%p: type: %s, base: %&quot; B_PRIdOFF &quot;, size: %&quot; B_PRIdOFF &quot;, &quot;</a>
<a name="ln3135">		&quot;pages: %&quot; B_PRIu32, cache, vm_cache_type_to_string(cache-&gt;type),</a>
<a name="ln3136">		cache-&gt;virtual_base, cache-&gt;virtual_end, cache-&gt;page_count);</a>
<a name="ln3137"> </a>
<a name="ln3138">	if (level == 0)</a>
<a name="ln3139">		kprintf(&quot;/%lu&quot;, info.page_count);</a>
<a name="ln3140"> </a>
<a name="ln3141">	if (cache-&gt;type == CACHE_TYPE_RAM || (level == 0 &amp;&amp; info.committed &gt; 0)) {</a>
<a name="ln3142">		kprintf(&quot;, committed: %&quot; B_PRIdOFF, cache-&gt;committed_size);</a>
<a name="ln3143"> </a>
<a name="ln3144">		if (level == 0)</a>
<a name="ln3145">			kprintf(&quot;/%lu&quot;, info.committed);</a>
<a name="ln3146">	}</a>
<a name="ln3147"> </a>
<a name="ln3148">	// areas</a>
<a name="ln3149">	if (cache-&gt;areas != NULL) {</a>
<a name="ln3150">		VMArea* area = cache-&gt;areas;</a>
<a name="ln3151">		kprintf(&quot;, areas: %&quot; B_PRId32 &quot; (%s, team: %&quot; B_PRId32 &quot;)&quot;, area-&gt;id,</a>
<a name="ln3152">			area-&gt;name, area-&gt;address_space-&gt;ID());</a>
<a name="ln3153"> </a>
<a name="ln3154">		while (area-&gt;cache_next != NULL) {</a>
<a name="ln3155">			area = area-&gt;cache_next;</a>
<a name="ln3156">			kprintf(&quot;, %&quot; B_PRId32, area-&gt;id);</a>
<a name="ln3157">		}</a>
<a name="ln3158">	}</a>
<a name="ln3159"> </a>
<a name="ln3160">	kputs(&quot;\n&quot;);</a>
<a name="ln3161"> </a>
<a name="ln3162">	// recurse</a>
<a name="ln3163">	for (VMCache::ConsumerList::Iterator it = cache-&gt;consumers.GetIterator();</a>
<a name="ln3164">			VMCache* consumer = it.Next();) {</a>
<a name="ln3165">		dump_caches_recursively(consumer, info, level + 1);</a>
<a name="ln3166">	}</a>
<a name="ln3167">}</a>
<a name="ln3168"> </a>
<a name="ln3169"> </a>
<a name="ln3170">static int</a>
<a name="ln3171">dump_caches(int argc, char** argv)</a>
<a name="ln3172">{</a>
<a name="ln3173">	if (sCacheInfoTable == NULL) {</a>
<a name="ln3174">		kprintf(&quot;No cache info table!\n&quot;);</a>
<a name="ln3175">		return 0;</a>
<a name="ln3176">	}</a>
<a name="ln3177"> </a>
<a name="ln3178">	bool sortByPageCount = true;</a>
<a name="ln3179"> </a>
<a name="ln3180">	for (int32 i = 1; i &lt; argc; i++) {</a>
<a name="ln3181">		if (strcmp(argv[i], &quot;-c&quot;) == 0) {</a>
<a name="ln3182">			sortByPageCount = false;</a>
<a name="ln3183">		} else {</a>
<a name="ln3184">			print_debugger_command_usage(argv[0]);</a>
<a name="ln3185">			return 0;</a>
<a name="ln3186">		}</a>
<a name="ln3187">	}</a>
<a name="ln3188"> </a>
<a name="ln3189">	uint32 totalCount = 0;</a>
<a name="ln3190">	uint32 rootCount = 0;</a>
<a name="ln3191">	off_t totalCommitted = 0;</a>
<a name="ln3192">	page_num_t totalPages = 0;</a>
<a name="ln3193"> </a>
<a name="ln3194">	VMCache* cache = gDebugCacheList;</a>
<a name="ln3195">	while (cache) {</a>
<a name="ln3196">		totalCount++;</a>
<a name="ln3197">		if (cache-&gt;source == NULL) {</a>
<a name="ln3198">			cache_info stackInfo;</a>
<a name="ln3199">			cache_info&amp; info = rootCount &lt; (uint32)kCacheInfoTableCount</a>
<a name="ln3200">				? sCacheInfoTable[rootCount] : stackInfo;</a>
<a name="ln3201">			rootCount++;</a>
<a name="ln3202">			info.cache = cache;</a>
<a name="ln3203">			info.page_count = 0;</a>
<a name="ln3204">			info.committed = 0;</a>
<a name="ln3205">			update_cache_info_recursively(cache, info);</a>
<a name="ln3206">			totalCommitted += info.committed;</a>
<a name="ln3207">			totalPages += info.page_count;</a>
<a name="ln3208">		}</a>
<a name="ln3209"> </a>
<a name="ln3210">		cache = cache-&gt;debug_next;</a>
<a name="ln3211">	}</a>
<a name="ln3212"> </a>
<a name="ln3213">	if (rootCount &lt;= (uint32)kCacheInfoTableCount) {</a>
<a name="ln3214">		qsort(sCacheInfoTable, rootCount, sizeof(cache_info),</a>
<a name="ln3215">			sortByPageCount</a>
<a name="ln3216">				? &amp;cache_info_compare_page_count</a>
<a name="ln3217">				: &amp;cache_info_compare_committed);</a>
<a name="ln3218">	}</a>
<a name="ln3219"> </a>
<a name="ln3220">	kprintf(&quot;total committed memory: %&quot; B_PRIdOFF &quot;, total used pages: %&quot;</a>
<a name="ln3221">		B_PRIuPHYSADDR &quot;\n&quot;, totalCommitted, totalPages);</a>
<a name="ln3222">	kprintf(&quot;%&quot; B_PRIu32 &quot; caches (%&quot; B_PRIu32 &quot; root caches), sorted by %s &quot;</a>
<a name="ln3223">		&quot;per cache tree...\n\n&quot;, totalCount, rootCount, sortByPageCount ?</a>
<a name="ln3224">			&quot;page count&quot; : &quot;committed size&quot;);</a>
<a name="ln3225"> </a>
<a name="ln3226">	if (rootCount &lt;= (uint32)kCacheInfoTableCount) {</a>
<a name="ln3227">		for (uint32 i = 0; i &lt; rootCount; i++) {</a>
<a name="ln3228">			cache_info&amp; info = sCacheInfoTable[i];</a>
<a name="ln3229">			dump_caches_recursively(info.cache, info, 0);</a>
<a name="ln3230">		}</a>
<a name="ln3231">	} else</a>
<a name="ln3232">		kprintf(&quot;Cache info table too small! Can't sort and print caches!\n&quot;);</a>
<a name="ln3233"> </a>
<a name="ln3234">	return 0;</a>
<a name="ln3235">}</a>
<a name="ln3236"> </a>
<a name="ln3237">#endif	// DEBUG_CACHE_LIST</a>
<a name="ln3238"> </a>
<a name="ln3239"> </a>
<a name="ln3240">static int</a>
<a name="ln3241">dump_cache(int argc, char** argv)</a>
<a name="ln3242">{</a>
<a name="ln3243">	VMCache* cache;</a>
<a name="ln3244">	bool showPages = false;</a>
<a name="ln3245">	int i = 1;</a>
<a name="ln3246"> </a>
<a name="ln3247">	if (argc &lt; 2 || !strcmp(argv[1], &quot;--help&quot;)) {</a>
<a name="ln3248">		kprintf(&quot;usage: %s [-ps] &lt;address&gt;\n&quot;</a>
<a name="ln3249">			&quot;  if -p is specified, all pages are shown, if -s is used\n&quot;</a>
<a name="ln3250">			&quot;  only the cache info is shown respectively.\n&quot;, argv[0]);</a>
<a name="ln3251">		return 0;</a>
<a name="ln3252">	}</a>
<a name="ln3253">	while (argv[i][0] == '-') {</a>
<a name="ln3254">		char* arg = argv[i] + 1;</a>
<a name="ln3255">		while (arg[0]) {</a>
<a name="ln3256">			if (arg[0] == 'p')</a>
<a name="ln3257">				showPages = true;</a>
<a name="ln3258">			arg++;</a>
<a name="ln3259">		}</a>
<a name="ln3260">		i++;</a>
<a name="ln3261">	}</a>
<a name="ln3262">	if (argv[i] == NULL) {</a>
<a name="ln3263">		kprintf(&quot;%s: invalid argument, pass address\n&quot;, argv[0]);</a>
<a name="ln3264">		return 0;</a>
<a name="ln3265">	}</a>
<a name="ln3266"> </a>
<a name="ln3267">	addr_t address = parse_expression(argv[i]);</a>
<a name="ln3268">	if (address == 0)</a>
<a name="ln3269">		return 0;</a>
<a name="ln3270"> </a>
<a name="ln3271">	cache = (VMCache*)address;</a>
<a name="ln3272"> </a>
<a name="ln3273">	cache-&gt;Dump(showPages);</a>
<a name="ln3274"> </a>
<a name="ln3275">	set_debug_variable(&quot;_sourceCache&quot;, (addr_t)cache-&gt;source);</a>
<a name="ln3276"> </a>
<a name="ln3277">	return 0;</a>
<a name="ln3278">}</a>
<a name="ln3279"> </a>
<a name="ln3280"> </a>
<a name="ln3281">static void</a>
<a name="ln3282">dump_area_struct(VMArea* area, bool mappings)</a>
<a name="ln3283">{</a>
<a name="ln3284">	kprintf(&quot;AREA: %p\n&quot;, area);</a>
<a name="ln3285">	kprintf(&quot;name:\t\t'%s'\n&quot;, area-&gt;name);</a>
<a name="ln3286">	kprintf(&quot;owner:\t\t0x%&quot; B_PRIx32 &quot;\n&quot;, area-&gt;address_space-&gt;ID());</a>
<a name="ln3287">	kprintf(&quot;id:\t\t0x%&quot; B_PRIx32 &quot;\n&quot;, area-&gt;id);</a>
<a name="ln3288">	kprintf(&quot;base:\t\t0x%lx\n&quot;, area-&gt;Base());</a>
<a name="ln3289">	kprintf(&quot;size:\t\t0x%lx\n&quot;, area-&gt;Size());</a>
<a name="ln3290">	kprintf(&quot;protection:\t0x%&quot; B_PRIx32 &quot;\n&quot;, area-&gt;protection);</a>
<a name="ln3291">	kprintf(&quot;wiring:\t\t0x%x\n&quot;, area-&gt;wiring);</a>
<a name="ln3292">	kprintf(&quot;memory_type:\t%#&quot; B_PRIx32 &quot;\n&quot;, area-&gt;MemoryType());</a>
<a name="ln3293">	kprintf(&quot;cache:\t\t%p\n&quot;, area-&gt;cache);</a>
<a name="ln3294">	kprintf(&quot;cache_type:\t%s\n&quot;, vm_cache_type_to_string(area-&gt;cache_type));</a>
<a name="ln3295">	kprintf(&quot;cache_offset:\t0x%&quot; B_PRIx64 &quot;\n&quot;, area-&gt;cache_offset);</a>
<a name="ln3296">	kprintf(&quot;cache_next:\t%p\n&quot;, area-&gt;cache_next);</a>
<a name="ln3297">	kprintf(&quot;cache_prev:\t%p\n&quot;, area-&gt;cache_prev);</a>
<a name="ln3298"> </a>
<a name="ln3299">	VMAreaMappings::Iterator iterator = area-&gt;mappings.GetIterator();</a>
<a name="ln3300">	if (mappings) {</a>
<a name="ln3301">		kprintf(&quot;page mappings:\n&quot;);</a>
<a name="ln3302">		while (iterator.HasNext()) {</a>
<a name="ln3303">			vm_page_mapping* mapping = iterator.Next();</a>
<a name="ln3304">			kprintf(&quot;  %p&quot;, mapping-&gt;page);</a>
<a name="ln3305">		}</a>
<a name="ln3306">		kprintf(&quot;\n&quot;);</a>
<a name="ln3307">	} else {</a>
<a name="ln3308">		uint32 count = 0;</a>
<a name="ln3309">		while (iterator.Next() != NULL) {</a>
<a name="ln3310">			count++;</a>
<a name="ln3311">		}</a>
<a name="ln3312">		kprintf(&quot;page mappings:\t%&quot; B_PRIu32 &quot;\n&quot;, count);</a>
<a name="ln3313">	}</a>
<a name="ln3314">}</a>
<a name="ln3315"> </a>
<a name="ln3316"> </a>
<a name="ln3317">static int</a>
<a name="ln3318">dump_area(int argc, char** argv)</a>
<a name="ln3319">{</a>
<a name="ln3320">	bool mappings = false;</a>
<a name="ln3321">	bool found = false;</a>
<a name="ln3322">	int32 index = 1;</a>
<a name="ln3323">	VMArea* area;</a>
<a name="ln3324">	addr_t num;</a>
<a name="ln3325"> </a>
<a name="ln3326">	if (argc &lt; 2 || !strcmp(argv[1], &quot;--help&quot;)) {</a>
<a name="ln3327">		kprintf(&quot;usage: area [-m] [id|contains|address|name] &lt;id|address|name&gt;\n&quot;</a>
<a name="ln3328">			&quot;All areas matching either id/address/name are listed. You can\n&quot;</a>
<a name="ln3329">			&quot;force to check only a specific item by prefixing the specifier\n&quot;</a>
<a name="ln3330">			&quot;with the id/contains/address/name keywords.\n&quot;</a>
<a name="ln3331">			&quot;-m shows the area's mappings as well.\n&quot;);</a>
<a name="ln3332">		return 0;</a>
<a name="ln3333">	}</a>
<a name="ln3334"> </a>
<a name="ln3335">	if (!strcmp(argv[1], &quot;-m&quot;)) {</a>
<a name="ln3336">		mappings = true;</a>
<a name="ln3337">		index++;</a>
<a name="ln3338">	}</a>
<a name="ln3339"> </a>
<a name="ln3340">	int32 mode = 0xf;</a>
<a name="ln3341">	if (!strcmp(argv[index], &quot;id&quot;))</a>
<a name="ln3342">		mode = 1;</a>
<a name="ln3343">	else if (!strcmp(argv[index], &quot;contains&quot;))</a>
<a name="ln3344">		mode = 2;</a>
<a name="ln3345">	else if (!strcmp(argv[index], &quot;name&quot;))</a>
<a name="ln3346">		mode = 4;</a>
<a name="ln3347">	else if (!strcmp(argv[index], &quot;address&quot;))</a>
<a name="ln3348">		mode = 0;</a>
<a name="ln3349">	if (mode != 0xf)</a>
<a name="ln3350">		index++;</a>
<a name="ln3351"> </a>
<a name="ln3352">	if (index &gt;= argc) {</a>
<a name="ln3353">		kprintf(&quot;No area specifier given.\n&quot;);</a>
<a name="ln3354">		return 0;</a>
<a name="ln3355">	}</a>
<a name="ln3356"> </a>
<a name="ln3357">	num = parse_expression(argv[index]);</a>
<a name="ln3358"> </a>
<a name="ln3359">	if (mode == 0) {</a>
<a name="ln3360">		dump_area_struct((struct VMArea*)num, mappings);</a>
<a name="ln3361">	} else {</a>
<a name="ln3362">		// walk through the area list, looking for the arguments as a name</a>
<a name="ln3363"> </a>
<a name="ln3364">		VMAreaHashTable::Iterator it = VMAreaHash::GetIterator();</a>
<a name="ln3365">		while ((area = it.Next()) != NULL) {</a>
<a name="ln3366">			if (((mode &amp; 4) != 0 &amp;&amp; area-&gt;name != NULL</a>
<a name="ln3367">					&amp;&amp; !strcmp(argv[index], area-&gt;name))</a>
<a name="ln3368">				|| (num != 0 &amp;&amp; (((mode &amp; 1) != 0 &amp;&amp; (addr_t)area-&gt;id == num)</a>
<a name="ln3369">					|| (((mode &amp; 2) != 0 &amp;&amp; area-&gt;Base() &lt;= num</a>
<a name="ln3370">						&amp;&amp; area-&gt;Base() + area-&gt;Size() &gt; num))))) {</a>
<a name="ln3371">				dump_area_struct(area, mappings);</a>
<a name="ln3372">				found = true;</a>
<a name="ln3373">			}</a>
<a name="ln3374">		}</a>
<a name="ln3375"> </a>
<a name="ln3376">		if (!found)</a>
<a name="ln3377">			kprintf(&quot;could not find area %s (%ld)\n&quot;, argv[index], num);</a>
<a name="ln3378">	}</a>
<a name="ln3379"> </a>
<a name="ln3380">	return 0;</a>
<a name="ln3381">}</a>
<a name="ln3382"> </a>
<a name="ln3383"> </a>
<a name="ln3384">static int</a>
<a name="ln3385">dump_area_list(int argc, char** argv)</a>
<a name="ln3386">{</a>
<a name="ln3387">	VMArea* area;</a>
<a name="ln3388">	const char* name = NULL;</a>
<a name="ln3389">	int32 id = 0;</a>
<a name="ln3390"> </a>
<a name="ln3391">	if (argc &gt; 1) {</a>
<a name="ln3392">		id = parse_expression(argv[1]);</a>
<a name="ln3393">		if (id == 0)</a>
<a name="ln3394">			name = argv[1];</a>
<a name="ln3395">	}</a>
<a name="ln3396"> </a>
<a name="ln3397">	kprintf(&quot;%-*s      id  %-*s    %-*sprotect lock  name\n&quot;,</a>
<a name="ln3398">		B_PRINTF_POINTER_WIDTH, &quot;addr&quot;, B_PRINTF_POINTER_WIDTH, &quot;base&quot;,</a>
<a name="ln3399">		B_PRINTF_POINTER_WIDTH, &quot;size&quot;);</a>
<a name="ln3400"> </a>
<a name="ln3401">	VMAreaHashTable::Iterator it = VMAreaHash::GetIterator();</a>
<a name="ln3402">	while ((area = it.Next()) != NULL) {</a>
<a name="ln3403">		if ((id != 0 &amp;&amp; area-&gt;address_space-&gt;ID() != id)</a>
<a name="ln3404">			|| (name != NULL &amp;&amp; strstr(area-&gt;name, name) == NULL))</a>
<a name="ln3405">			continue;</a>
<a name="ln3406"> </a>
<a name="ln3407">		kprintf(&quot;%p %5&quot; B_PRIx32 &quot;  %p  %p %4&quot; B_PRIx32 &quot; %4d  %s\n&quot;, area,</a>
<a name="ln3408">			area-&gt;id, (void*)area-&gt;Base(), (void*)area-&gt;Size(),</a>
<a name="ln3409">			area-&gt;protection, area-&gt;wiring, area-&gt;name);</a>
<a name="ln3410">	}</a>
<a name="ln3411">	return 0;</a>
<a name="ln3412">}</a>
<a name="ln3413"> </a>
<a name="ln3414"> </a>
<a name="ln3415">static int</a>
<a name="ln3416">dump_available_memory(int argc, char** argv)</a>
<a name="ln3417">{</a>
<a name="ln3418">	kprintf(&quot;Available memory: %&quot; B_PRIdOFF &quot;/%&quot; B_PRIuPHYSADDR &quot; bytes\n&quot;,</a>
<a name="ln3419">		sAvailableMemory, (phys_addr_t)vm_page_num_pages() * B_PAGE_SIZE);</a>
<a name="ln3420">	return 0;</a>
<a name="ln3421">}</a>
<a name="ln3422"> </a>
<a name="ln3423"> </a>
<a name="ln3424">static int</a>
<a name="ln3425">dump_mapping_info(int argc, char** argv)</a>
<a name="ln3426">{</a>
<a name="ln3427">	bool reverseLookup = false;</a>
<a name="ln3428">	bool pageLookup = false;</a>
<a name="ln3429"> </a>
<a name="ln3430">	int argi = 1;</a>
<a name="ln3431">	for (; argi &lt; argc &amp;&amp; argv[argi][0] == '-'; argi++) {</a>
<a name="ln3432">		const char* arg = argv[argi];</a>
<a name="ln3433">		if (strcmp(arg, &quot;-r&quot;) == 0) {</a>
<a name="ln3434">			reverseLookup = true;</a>
<a name="ln3435">		} else if (strcmp(arg, &quot;-p&quot;) == 0) {</a>
<a name="ln3436">			reverseLookup = true;</a>
<a name="ln3437">			pageLookup = true;</a>
<a name="ln3438">		} else {</a>
<a name="ln3439">			print_debugger_command_usage(argv[0]);</a>
<a name="ln3440">			return 0;</a>
<a name="ln3441">		}</a>
<a name="ln3442">	}</a>
<a name="ln3443"> </a>
<a name="ln3444">	// We need at least one argument, the address. Optionally a thread ID can be</a>
<a name="ln3445">	// specified.</a>
<a name="ln3446">	if (argi &gt;= argc || argi + 2 &lt; argc) {</a>
<a name="ln3447">		print_debugger_command_usage(argv[0]);</a>
<a name="ln3448">		return 0;</a>
<a name="ln3449">	}</a>
<a name="ln3450"> </a>
<a name="ln3451">	uint64 addressValue;</a>
<a name="ln3452">	if (!evaluate_debug_expression(argv[argi++], &amp;addressValue, false))</a>
<a name="ln3453">		return 0;</a>
<a name="ln3454"> </a>
<a name="ln3455">	Team* team = NULL;</a>
<a name="ln3456">	if (argi &lt; argc) {</a>
<a name="ln3457">		uint64 threadID;</a>
<a name="ln3458">		if (!evaluate_debug_expression(argv[argi++], &amp;threadID, false))</a>
<a name="ln3459">			return 0;</a>
<a name="ln3460"> </a>
<a name="ln3461">		Thread* thread = Thread::GetDebug(threadID);</a>
<a name="ln3462">		if (thread == NULL) {</a>
<a name="ln3463">			kprintf(&quot;Invalid thread/team ID \&quot;%s\&quot;\n&quot;, argv[argi - 1]);</a>
<a name="ln3464">			return 0;</a>
<a name="ln3465">		}</a>
<a name="ln3466"> </a>
<a name="ln3467">		team = thread-&gt;team;</a>
<a name="ln3468">	}</a>
<a name="ln3469"> </a>
<a name="ln3470">	if (reverseLookup) {</a>
<a name="ln3471">		phys_addr_t physicalAddress;</a>
<a name="ln3472">		if (pageLookup) {</a>
<a name="ln3473">			vm_page* page = (vm_page*)(addr_t)addressValue;</a>
<a name="ln3474">			physicalAddress = page-&gt;physical_page_number * B_PAGE_SIZE;</a>
<a name="ln3475">		} else {</a>
<a name="ln3476">			physicalAddress = (phys_addr_t)addressValue;</a>
<a name="ln3477">			physicalAddress -= physicalAddress % B_PAGE_SIZE;</a>
<a name="ln3478">		}</a>
<a name="ln3479"> </a>
<a name="ln3480">		kprintf(&quot;    Team     Virtual Address      Area\n&quot;);</a>
<a name="ln3481">		kprintf(&quot;--------------------------------------\n&quot;);</a>
<a name="ln3482"> </a>
<a name="ln3483">		struct Callback : VMTranslationMap::ReverseMappingInfoCallback {</a>
<a name="ln3484">			Callback()</a>
<a name="ln3485">				:</a>
<a name="ln3486">				fAddressSpace(NULL)</a>
<a name="ln3487">			{</a>
<a name="ln3488">			}</a>
<a name="ln3489"> </a>
<a name="ln3490">			void SetAddressSpace(VMAddressSpace* addressSpace)</a>
<a name="ln3491">			{</a>
<a name="ln3492">				fAddressSpace = addressSpace;</a>
<a name="ln3493">			}</a>
<a name="ln3494"> </a>
<a name="ln3495">			virtual bool HandleVirtualAddress(addr_t virtualAddress)</a>
<a name="ln3496">			{</a>
<a name="ln3497">				kprintf(&quot;%8&quot; B_PRId32 &quot;  %#18&quot; B_PRIxADDR, fAddressSpace-&gt;ID(),</a>
<a name="ln3498">					virtualAddress);</a>
<a name="ln3499">				if (VMArea* area = fAddressSpace-&gt;LookupArea(virtualAddress))</a>
<a name="ln3500">					kprintf(&quot;  %8&quot; B_PRId32 &quot; %s\n&quot;, area-&gt;id, area-&gt;name);</a>
<a name="ln3501">				else</a>
<a name="ln3502">					kprintf(&quot;\n&quot;);</a>
<a name="ln3503">				return false;</a>
<a name="ln3504">			}</a>
<a name="ln3505"> </a>
<a name="ln3506">		private:</a>
<a name="ln3507">			VMAddressSpace*	fAddressSpace;</a>
<a name="ln3508">		} callback;</a>
<a name="ln3509"> </a>
<a name="ln3510">		if (team != NULL) {</a>
<a name="ln3511">			// team specified -- get its address space</a>
<a name="ln3512">			VMAddressSpace* addressSpace = team-&gt;address_space;</a>
<a name="ln3513">			if (addressSpace == NULL) {</a>
<a name="ln3514">				kprintf(&quot;Failed to get address space!\n&quot;);</a>
<a name="ln3515">				return 0;</a>
<a name="ln3516">			}</a>
<a name="ln3517"> </a>
<a name="ln3518">			callback.SetAddressSpace(addressSpace);</a>
<a name="ln3519">			addressSpace-&gt;TranslationMap()-&gt;DebugGetReverseMappingInfo(</a>
<a name="ln3520">				physicalAddress, callback);</a>
<a name="ln3521">		} else {</a>
<a name="ln3522">			// no team specified -- iterate through all address spaces</a>
<a name="ln3523">			for (VMAddressSpace* addressSpace = VMAddressSpace::DebugFirst();</a>
<a name="ln3524">				addressSpace != NULL;</a>
<a name="ln3525">				addressSpace = VMAddressSpace::DebugNext(addressSpace)) {</a>
<a name="ln3526">				callback.SetAddressSpace(addressSpace);</a>
<a name="ln3527">				addressSpace-&gt;TranslationMap()-&gt;DebugGetReverseMappingInfo(</a>
<a name="ln3528">					physicalAddress, callback);</a>
<a name="ln3529">			}</a>
<a name="ln3530">		}</a>
<a name="ln3531">	} else {</a>
<a name="ln3532">		// get the address space</a>
<a name="ln3533">		addr_t virtualAddress = (addr_t)addressValue;</a>
<a name="ln3534">		virtualAddress -= virtualAddress % B_PAGE_SIZE;</a>
<a name="ln3535">		VMAddressSpace* addressSpace;</a>
<a name="ln3536">		if (IS_KERNEL_ADDRESS(virtualAddress)) {</a>
<a name="ln3537">			addressSpace = VMAddressSpace::Kernel();</a>
<a name="ln3538">		} else if (team != NULL) {</a>
<a name="ln3539">			addressSpace = team-&gt;address_space;</a>
<a name="ln3540">		} else {</a>
<a name="ln3541">			Thread* thread = debug_get_debugged_thread();</a>
<a name="ln3542">			if (thread == NULL || thread-&gt;team == NULL) {</a>
<a name="ln3543">				kprintf(&quot;Failed to get team!\n&quot;);</a>
<a name="ln3544">				return 0;</a>
<a name="ln3545">			}</a>
<a name="ln3546"> </a>
<a name="ln3547">			addressSpace = thread-&gt;team-&gt;address_space;</a>
<a name="ln3548">		}</a>
<a name="ln3549"> </a>
<a name="ln3550">		if (addressSpace == NULL) {</a>
<a name="ln3551">			kprintf(&quot;Failed to get address space!\n&quot;);</a>
<a name="ln3552">			return 0;</a>
<a name="ln3553">		}</a>
<a name="ln3554"> </a>
<a name="ln3555">		// let the translation map implementation do the job</a>
<a name="ln3556">		addressSpace-&gt;TranslationMap()-&gt;DebugPrintMappingInfo(virtualAddress);</a>
<a name="ln3557">	}</a>
<a name="ln3558"> </a>
<a name="ln3559">	return 0;</a>
<a name="ln3560">}</a>
<a name="ln3561"> </a>
<a name="ln3562"> </a>
<a name="ln3563">/*!	Deletes all areas and reserved regions in the given address space.</a>
<a name="ln3564"> </a>
<a name="ln3565">	The caller must ensure that none of the areas has any wired ranges.</a>
<a name="ln3566"> </a>
<a name="ln3567">	\param addressSpace The address space.</a>
<a name="ln3568">	\param deletingAddressSpace \c true, if the address space is in the process</a>
<a name="ln3569">		of being deleted.</a>
<a name="ln3570">*/</a>
<a name="ln3571">void</a>
<a name="ln3572">vm_delete_areas(struct VMAddressSpace* addressSpace, bool deletingAddressSpace)</a>
<a name="ln3573">{</a>
<a name="ln3574">	TRACE((&quot;vm_delete_areas: called on address space 0x%&quot; B_PRIx32 &quot;\n&quot;,</a>
<a name="ln3575">		addressSpace-&gt;ID()));</a>
<a name="ln3576"> </a>
<a name="ln3577">	addressSpace-&gt;WriteLock();</a>
<a name="ln3578"> </a>
<a name="ln3579">	// remove all reserved areas in this address space</a>
<a name="ln3580">	addressSpace-&gt;UnreserveAllAddressRanges(0);</a>
<a name="ln3581"> </a>
<a name="ln3582">	// delete all the areas in this address space</a>
<a name="ln3583">	while (VMArea* area = addressSpace-&gt;FirstArea()) {</a>
<a name="ln3584">		ASSERT(!area-&gt;IsWired());</a>
<a name="ln3585">		delete_area(addressSpace, area, deletingAddressSpace);</a>
<a name="ln3586">	}</a>
<a name="ln3587"> </a>
<a name="ln3588">	addressSpace-&gt;WriteUnlock();</a>
<a name="ln3589">}</a>
<a name="ln3590"> </a>
<a name="ln3591"> </a>
<a name="ln3592">static area_id</a>
<a name="ln3593">vm_area_for(addr_t address, bool kernel)</a>
<a name="ln3594">{</a>
<a name="ln3595">	team_id team;</a>
<a name="ln3596">	if (IS_USER_ADDRESS(address)) {</a>
<a name="ln3597">		// we try the user team address space, if any</a>
<a name="ln3598">		team = VMAddressSpace::CurrentID();</a>
<a name="ln3599">		if (team &lt; 0)</a>
<a name="ln3600">			return team;</a>
<a name="ln3601">	} else</a>
<a name="ln3602">		team = VMAddressSpace::KernelID();</a>
<a name="ln3603"> </a>
<a name="ln3604">	AddressSpaceReadLocker locker(team);</a>
<a name="ln3605">	if (!locker.IsLocked())</a>
<a name="ln3606">		return B_BAD_TEAM_ID;</a>
<a name="ln3607"> </a>
<a name="ln3608">	VMArea* area = locker.AddressSpace()-&gt;LookupArea(address);</a>
<a name="ln3609">	if (area != NULL) {</a>
<a name="ln3610">		if (!kernel &amp;&amp; (area-&gt;protection &amp; (B_READ_AREA | B_WRITE_AREA)) == 0)</a>
<a name="ln3611">			return B_ERROR;</a>
<a name="ln3612"> </a>
<a name="ln3613">		return area-&gt;id;</a>
<a name="ln3614">	}</a>
<a name="ln3615"> </a>
<a name="ln3616">	return B_ERROR;</a>
<a name="ln3617">}</a>
<a name="ln3618"> </a>
<a name="ln3619"> </a>
<a name="ln3620">/*!	Frees physical pages that were used during the boot process.</a>
<a name="ln3621">	\a end is inclusive.</a>
<a name="ln3622">*/</a>
<a name="ln3623">static void</a>
<a name="ln3624">unmap_and_free_physical_pages(VMTranslationMap* map, addr_t start, addr_t end)</a>
<a name="ln3625">{</a>
<a name="ln3626">	// free all physical pages in the specified range</a>
<a name="ln3627"> </a>
<a name="ln3628">	for (addr_t current = start; current &lt; end; current += B_PAGE_SIZE) {</a>
<a name="ln3629">		phys_addr_t physicalAddress;</a>
<a name="ln3630">		uint32 flags;</a>
<a name="ln3631"> </a>
<a name="ln3632">		if (map-&gt;Query(current, &amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln3633">			&amp;&amp; (flags &amp; PAGE_PRESENT) != 0) {</a>
<a name="ln3634">			vm_page* page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);</a>
<a name="ln3635">			if (page != NULL &amp;&amp; page-&gt;State() != PAGE_STATE_FREE</a>
<a name="ln3636">					 &amp;&amp; page-&gt;State() != PAGE_STATE_CLEAR</a>
<a name="ln3637">					 &amp;&amp; page-&gt;State() != PAGE_STATE_UNUSED) {</a>
<a name="ln3638">				DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln3639">				vm_page_set_state(page, PAGE_STATE_FREE);</a>
<a name="ln3640">			}</a>
<a name="ln3641">		}</a>
<a name="ln3642">	}</a>
<a name="ln3643"> </a>
<a name="ln3644">	// unmap the memory</a>
<a name="ln3645">	map-&gt;Unmap(start, end);</a>
<a name="ln3646">}</a>
<a name="ln3647"> </a>
<a name="ln3648"> </a>
<a name="ln3649">void</a>
<a name="ln3650">vm_free_unused_boot_loader_range(addr_t start, addr_t size)</a>
<a name="ln3651">{</a>
<a name="ln3652">	VMTranslationMap* map = VMAddressSpace::Kernel()-&gt;TranslationMap();</a>
<a name="ln3653">	addr_t end = start + (size - 1);</a>
<a name="ln3654">	addr_t lastEnd = start;</a>
<a name="ln3655"> </a>
<a name="ln3656">	TRACE((&quot;vm_free_unused_boot_loader_range(): asked to free %p - %p\n&quot;,</a>
<a name="ln3657">		(void*)start, (void*)end));</a>
<a name="ln3658"> </a>
<a name="ln3659">	// The areas are sorted in virtual address space order, so</a>
<a name="ln3660">	// we just have to find the holes between them that fall</a>
<a name="ln3661">	// into the area we should dispose</a>
<a name="ln3662"> </a>
<a name="ln3663">	map-&gt;Lock();</a>
<a name="ln3664"> </a>
<a name="ln3665">	for (VMAddressSpace::AreaIterator it</a>
<a name="ln3666">				= VMAddressSpace::Kernel()-&gt;GetAreaIterator();</a>
<a name="ln3667">			VMArea* area = it.Next();) {</a>
<a name="ln3668">		addr_t areaStart = area-&gt;Base();</a>
<a name="ln3669">		addr_t areaEnd = areaStart + (area-&gt;Size() - 1);</a>
<a name="ln3670"> </a>
<a name="ln3671">		if (areaEnd &lt; start)</a>
<a name="ln3672">			continue;</a>
<a name="ln3673"> </a>
<a name="ln3674">		if (areaStart &gt; end) {</a>
<a name="ln3675">			// we are done, the area is already beyond of what we have to free</a>
<a name="ln3676">			break;</a>
<a name="ln3677">		}</a>
<a name="ln3678"> </a>
<a name="ln3679">		if (areaStart &gt; lastEnd) {</a>
<a name="ln3680">			// this is something we can free</a>
<a name="ln3681">			TRACE((&quot;free boot range: get rid of %p - %p\n&quot;, (void*)lastEnd,</a>
<a name="ln3682">				(void*)areaStart));</a>
<a name="ln3683">			unmap_and_free_physical_pages(map, lastEnd, areaStart - 1);</a>
<a name="ln3684">		}</a>
<a name="ln3685"> </a>
<a name="ln3686">		if (areaEnd &gt;= end) {</a>
<a name="ln3687">			lastEnd = areaEnd;</a>
<a name="ln3688">				// no +1 to prevent potential overflow</a>
<a name="ln3689">			break;</a>
<a name="ln3690">		}</a>
<a name="ln3691"> </a>
<a name="ln3692">		lastEnd = areaEnd + 1;</a>
<a name="ln3693">	}</a>
<a name="ln3694"> </a>
<a name="ln3695">	if (lastEnd &lt; end) {</a>
<a name="ln3696">		// we can also get rid of some space at the end of the area</a>
<a name="ln3697">		TRACE((&quot;free boot range: also remove %p - %p\n&quot;, (void*)lastEnd,</a>
<a name="ln3698">			(void*)end));</a>
<a name="ln3699">		unmap_and_free_physical_pages(map, lastEnd, end);</a>
<a name="ln3700">	}</a>
<a name="ln3701"> </a>
<a name="ln3702">	map-&gt;Unlock();</a>
<a name="ln3703">}</a>
<a name="ln3704"> </a>
<a name="ln3705"> </a>
<a name="ln3706">static void</a>
<a name="ln3707">create_preloaded_image_areas(struct preloaded_image* _image)</a>
<a name="ln3708">{</a>
<a name="ln3709">	preloaded_elf_image* image = static_cast&lt;preloaded_elf_image*&gt;(_image);</a>
<a name="ln3710">	char name[B_OS_NAME_LENGTH];</a>
<a name="ln3711">	void* address;</a>
<a name="ln3712">	int32 length;</a>
<a name="ln3713"> </a>
<a name="ln3714">	// use file name to create a good area name</a>
<a name="ln3715">	char* fileName = strrchr(image-&gt;name, '/');</a>
<a name="ln3716">	if (fileName == NULL)</a>
<a name="ln3717">		fileName = image-&gt;name;</a>
<a name="ln3718">	else</a>
<a name="ln3719">		fileName++;</a>
<a name="ln3720"> </a>
<a name="ln3721">	length = strlen(fileName);</a>
<a name="ln3722">	// make sure there is enough space for the suffix</a>
<a name="ln3723">	if (length &gt; 25)</a>
<a name="ln3724">		length = 25;</a>
<a name="ln3725"> </a>
<a name="ln3726">	memcpy(name, fileName, length);</a>
<a name="ln3727">	strcpy(name + length, &quot;_text&quot;);</a>
<a name="ln3728">	address = (void*)ROUNDDOWN(image-&gt;text_region.start, B_PAGE_SIZE);</a>
<a name="ln3729">	image-&gt;text_region.id = create_area(name, &amp;address, B_EXACT_ADDRESS,</a>
<a name="ln3730">		PAGE_ALIGN(image-&gt;text_region.size), B_ALREADY_WIRED,</a>
<a name="ln3731">		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln3732">		// this will later be remapped read-only/executable by the</a>
<a name="ln3733">		// ELF initialization code</a>
<a name="ln3734"> </a>
<a name="ln3735">	strcpy(name + length, &quot;_data&quot;);</a>
<a name="ln3736">	address = (void*)ROUNDDOWN(image-&gt;data_region.start, B_PAGE_SIZE);</a>
<a name="ln3737">	image-&gt;data_region.id = create_area(name, &amp;address, B_EXACT_ADDRESS,</a>
<a name="ln3738">		PAGE_ALIGN(image-&gt;data_region.size), B_ALREADY_WIRED,</a>
<a name="ln3739">		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln3740">}</a>
<a name="ln3741"> </a>
<a name="ln3742"> </a>
<a name="ln3743">/*!	Frees all previously kernel arguments areas from the kernel_args structure.</a>
<a name="ln3744">	Any boot loader resources contained in that arguments must not be accessed</a>
<a name="ln3745">	anymore past this point.</a>
<a name="ln3746">*/</a>
<a name="ln3747">void</a>
<a name="ln3748">vm_free_kernel_args(kernel_args* args)</a>
<a name="ln3749">{</a>
<a name="ln3750">	uint32 i;</a>
<a name="ln3751"> </a>
<a name="ln3752">	TRACE((&quot;vm_free_kernel_args()\n&quot;));</a>
<a name="ln3753"> </a>
<a name="ln3754">	for (i = 0; i &lt; args-&gt;num_kernel_args_ranges; i++) {</a>
<a name="ln3755">		area_id area = area_for((void*)(addr_t)args-&gt;kernel_args_range[i].start);</a>
<a name="ln3756">		if (area &gt;= B_OK)</a>
<a name="ln3757">			delete_area(area);</a>
<a name="ln3758">	}</a>
<a name="ln3759">}</a>
<a name="ln3760"> </a>
<a name="ln3761"> </a>
<a name="ln3762">static void</a>
<a name="ln3763">allocate_kernel_args(kernel_args* args)</a>
<a name="ln3764">{</a>
<a name="ln3765">	TRACE((&quot;allocate_kernel_args()\n&quot;));</a>
<a name="ln3766"> </a>
<a name="ln3767">	for (uint32 i = 0; i &lt; args-&gt;num_kernel_args_ranges; i++) {</a>
<a name="ln3768">		void* address = (void*)(addr_t)args-&gt;kernel_args_range[i].start;</a>
<a name="ln3769"> </a>
<a name="ln3770">		create_area(&quot;_kernel args_&quot;, &amp;address, B_EXACT_ADDRESS,</a>
<a name="ln3771">			args-&gt;kernel_args_range[i].size, B_ALREADY_WIRED,</a>
<a name="ln3772">			B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln3773">	}</a>
<a name="ln3774">}</a>
<a name="ln3775"> </a>
<a name="ln3776"> </a>
<a name="ln3777">static void</a>
<a name="ln3778">unreserve_boot_loader_ranges(kernel_args* args)</a>
<a name="ln3779">{</a>
<a name="ln3780">	TRACE((&quot;unreserve_boot_loader_ranges()\n&quot;));</a>
<a name="ln3781"> </a>
<a name="ln3782">	for (uint32 i = 0; i &lt; args-&gt;num_virtual_allocated_ranges; i++) {</a>
<a name="ln3783">		vm_unreserve_address_range(VMAddressSpace::KernelID(),</a>
<a name="ln3784">			(void*)(addr_t)args-&gt;virtual_allocated_range[i].start,</a>
<a name="ln3785">			args-&gt;virtual_allocated_range[i].size);</a>
<a name="ln3786">	}</a>
<a name="ln3787">}</a>
<a name="ln3788"> </a>
<a name="ln3789"> </a>
<a name="ln3790">static void</a>
<a name="ln3791">reserve_boot_loader_ranges(kernel_args* args)</a>
<a name="ln3792">{</a>
<a name="ln3793">	TRACE((&quot;reserve_boot_loader_ranges()\n&quot;));</a>
<a name="ln3794"> </a>
<a name="ln3795">	for (uint32 i = 0; i &lt; args-&gt;num_virtual_allocated_ranges; i++) {</a>
<a name="ln3796">		void* address = (void*)(addr_t)args-&gt;virtual_allocated_range[i].start;</a>
<a name="ln3797"> </a>
<a name="ln3798">		// If the address is no kernel address, we just skip it. The</a>
<a name="ln3799">		// architecture specific code has to deal with it.</a>
<a name="ln3800">		if (!IS_KERNEL_ADDRESS(address)) {</a>
<a name="ln3801">			dprintf(&quot;reserve_boot_loader_ranges(): Skipping range: %p, %&quot;</a>
<a name="ln3802">				B_PRIu64 &quot;\n&quot;, address, args-&gt;virtual_allocated_range[i].size);</a>
<a name="ln3803">			continue;</a>
<a name="ln3804">		}</a>
<a name="ln3805"> </a>
<a name="ln3806">		status_t status = vm_reserve_address_range(VMAddressSpace::KernelID(),</a>
<a name="ln3807">			&amp;address, B_EXACT_ADDRESS, args-&gt;virtual_allocated_range[i].size, 0);</a>
<a name="ln3808">		if (status &lt; B_OK)</a>
<a name="ln3809">			panic(&quot;could not reserve boot loader ranges\n&quot;);</a>
<a name="ln3810">	}</a>
<a name="ln3811">}</a>
<a name="ln3812"> </a>
<a name="ln3813"> </a>
<a name="ln3814">static addr_t</a>
<a name="ln3815">allocate_early_virtual(kernel_args* args, size_t size, addr_t alignment)</a>
<a name="ln3816">{</a>
<a name="ln3817">	size = PAGE_ALIGN(size);</a>
<a name="ln3818"> </a>
<a name="ln3819">	// find a slot in the virtual allocation addr range</a>
<a name="ln3820">	for (uint32 i = 1; i &lt; args-&gt;num_virtual_allocated_ranges; i++) {</a>
<a name="ln3821">		// check to see if the space between this one and the last is big enough</a>
<a name="ln3822">		addr_t rangeStart = args-&gt;virtual_allocated_range[i].start;</a>
<a name="ln3823">		addr_t previousRangeEnd = args-&gt;virtual_allocated_range[i - 1].start</a>
<a name="ln3824">			+ args-&gt;virtual_allocated_range[i - 1].size;</a>
<a name="ln3825"> </a>
<a name="ln3826">		addr_t base = alignment &gt; 0</a>
<a name="ln3827">			? ROUNDUP(previousRangeEnd, alignment) : previousRangeEnd;</a>
<a name="ln3828"> </a>
<a name="ln3829">		if (base &gt;= KERNEL_BASE &amp;&amp; base &lt; rangeStart</a>
<a name="ln3830">				&amp;&amp; rangeStart - base &gt;= size) {</a>
<a name="ln3831">			args-&gt;virtual_allocated_range[i - 1].size</a>
<a name="ln3832">				+= base + size - previousRangeEnd;</a>
<a name="ln3833">			return base;</a>
<a name="ln3834">		}</a>
<a name="ln3835">	}</a>
<a name="ln3836"> </a>
<a name="ln3837">	// we hadn't found one between allocation ranges. this is ok.</a>
<a name="ln3838">	// see if there's a gap after the last one</a>
<a name="ln3839">	int lastEntryIndex = args-&gt;num_virtual_allocated_ranges - 1;</a>
<a name="ln3840">	addr_t lastRangeEnd = args-&gt;virtual_allocated_range[lastEntryIndex].start</a>
<a name="ln3841">		+ args-&gt;virtual_allocated_range[lastEntryIndex].size;</a>
<a name="ln3842">	addr_t base = alignment &gt; 0</a>
<a name="ln3843">		? ROUNDUP(lastRangeEnd, alignment) : lastRangeEnd;</a>
<a name="ln3844">	if (KERNEL_BASE + (KERNEL_SIZE - 1) - base &gt;= size) {</a>
<a name="ln3845">		args-&gt;virtual_allocated_range[lastEntryIndex].size</a>
<a name="ln3846">			+= base + size - lastRangeEnd;</a>
<a name="ln3847">		return base;</a>
<a name="ln3848">	}</a>
<a name="ln3849"> </a>
<a name="ln3850">	// see if there's a gap before the first one</a>
<a name="ln3851">	addr_t rangeStart = args-&gt;virtual_allocated_range[0].start;</a>
<a name="ln3852">	if (rangeStart &gt; KERNEL_BASE &amp;&amp; rangeStart - KERNEL_BASE &gt;= size) {</a>
<a name="ln3853">		base = rangeStart - size;</a>
<a name="ln3854">		if (alignment &gt; 0)</a>
<a name="ln3855">			base = ROUNDDOWN(base, alignment);</a>
<a name="ln3856"> </a>
<a name="ln3857">		if (base &gt;= KERNEL_BASE) {</a>
<a name="ln3858">			args-&gt;virtual_allocated_range[0].start = base;</a>
<a name="ln3859">			args-&gt;virtual_allocated_range[0].size += rangeStart - base;</a>
<a name="ln3860">			return base;</a>
<a name="ln3861">		}</a>
<a name="ln3862">	}</a>
<a name="ln3863"> </a>
<a name="ln3864">	return 0;</a>
<a name="ln3865">}</a>
<a name="ln3866"> </a>
<a name="ln3867"> </a>
<a name="ln3868">static bool</a>
<a name="ln3869">is_page_in_physical_memory_range(kernel_args* args, phys_addr_t address)</a>
<a name="ln3870">{</a>
<a name="ln3871">	// TODO: horrible brute-force method of determining if the page can be</a>
<a name="ln3872">	// allocated</a>
<a name="ln3873">	for (uint32 i = 0; i &lt; args-&gt;num_physical_memory_ranges; i++) {</a>
<a name="ln3874">		if (address &gt;= args-&gt;physical_memory_range[i].start</a>
<a name="ln3875">			&amp;&amp; address &lt; args-&gt;physical_memory_range[i].start</a>
<a name="ln3876">				+ args-&gt;physical_memory_range[i].size)</a>
<a name="ln3877">			return true;</a>
<a name="ln3878">	}</a>
<a name="ln3879">	return false;</a>
<a name="ln3880">}</a>
<a name="ln3881"> </a>
<a name="ln3882"> </a>
<a name="ln3883">page_num_t</a>
<a name="ln3884">vm_allocate_early_physical_page(kernel_args* args)</a>
<a name="ln3885">{</a>
<a name="ln3886">	for (uint32 i = 0; i &lt; args-&gt;num_physical_allocated_ranges; i++) {</a>
<a name="ln3887">		phys_addr_t nextPage;</a>
<a name="ln3888"> </a>
<a name="ln3889">		nextPage = args-&gt;physical_allocated_range[i].start</a>
<a name="ln3890">			+ args-&gt;physical_allocated_range[i].size;</a>
<a name="ln3891">		// see if the page after the next allocated paddr run can be allocated</a>
<a name="ln3892">		if (i + 1 &lt; args-&gt;num_physical_allocated_ranges</a>
<a name="ln3893">			&amp;&amp; args-&gt;physical_allocated_range[i + 1].size != 0) {</a>
<a name="ln3894">			// see if the next page will collide with the next allocated range</a>
<a name="ln3895">			if (nextPage &gt;= args-&gt;physical_allocated_range[i+1].start)</a>
<a name="ln3896">				continue;</a>
<a name="ln3897">		}</a>
<a name="ln3898">		// see if the next physical page fits in the memory block</a>
<a name="ln3899">		if (is_page_in_physical_memory_range(args, nextPage)) {</a>
<a name="ln3900">			// we got one!</a>
<a name="ln3901">			args-&gt;physical_allocated_range[i].size += B_PAGE_SIZE;</a>
<a name="ln3902">			return nextPage / B_PAGE_SIZE;</a>
<a name="ln3903">		}</a>
<a name="ln3904">	}</a>
<a name="ln3905"> </a>
<a name="ln3906">	// Expanding upwards didn't work, try going downwards.</a>
<a name="ln3907">	for (uint32 i = 0; i &lt; args-&gt;num_physical_allocated_ranges; i++) {</a>
<a name="ln3908">		phys_addr_t nextPage;</a>
<a name="ln3909"> </a>
<a name="ln3910">		nextPage = args-&gt;physical_allocated_range[i].start - B_PAGE_SIZE;</a>
<a name="ln3911">		// see if the page after the prev allocated paddr run can be allocated</a>
<a name="ln3912">		if (i &gt; 0 &amp;&amp; args-&gt;physical_allocated_range[i - 1].size != 0) {</a>
<a name="ln3913">			// see if the next page will collide with the next allocated range</a>
<a name="ln3914">			if (nextPage &lt; args-&gt;physical_allocated_range[i-1].start</a>
<a name="ln3915">				+ args-&gt;physical_allocated_range[i-1].size)</a>
<a name="ln3916">				continue;</a>
<a name="ln3917">		}</a>
<a name="ln3918">		// see if the next physical page fits in the memory block</a>
<a name="ln3919">		if (is_page_in_physical_memory_range(args, nextPage)) {</a>
<a name="ln3920">			// we got one!</a>
<a name="ln3921">			args-&gt;physical_allocated_range[i].start -= B_PAGE_SIZE;</a>
<a name="ln3922">			args-&gt;physical_allocated_range[i].size += B_PAGE_SIZE;</a>
<a name="ln3923">			return nextPage / B_PAGE_SIZE;</a>
<a name="ln3924">		}</a>
<a name="ln3925">	}</a>
<a name="ln3926"> </a>
<a name="ln3927">	return 0;</a>
<a name="ln3928">		// could not allocate a block</a>
<a name="ln3929">}</a>
<a name="ln3930"> </a>
<a name="ln3931"> </a>
<a name="ln3932">/*!	This one uses the kernel_args' physical and virtual memory ranges to</a>
<a name="ln3933">	allocate some pages before the VM is completely up.</a>
<a name="ln3934">*/</a>
<a name="ln3935">addr_t</a>
<a name="ln3936">vm_allocate_early(kernel_args* args, size_t virtualSize, size_t physicalSize,</a>
<a name="ln3937">	uint32 attributes, addr_t alignment)</a>
<a name="ln3938">{</a>
<a name="ln3939">	if (physicalSize &gt; virtualSize)</a>
<a name="ln3940">		physicalSize = virtualSize;</a>
<a name="ln3941"> </a>
<a name="ln3942">	// find the vaddr to allocate at</a>
<a name="ln3943">	addr_t virtualBase = allocate_early_virtual(args, virtualSize, alignment);</a>
<a name="ln3944">	//dprintf(&quot;vm_allocate_early: vaddr 0x%lx\n&quot;, virtualBase);</a>
<a name="ln3945">	if (virtualBase == 0) {</a>
<a name="ln3946">		panic(&quot;vm_allocate_early: could not allocate virtual address\n&quot;);</a>
<a name="ln3947">		return 0;</a>
<a name="ln3948">	}</a>
<a name="ln3949"> </a>
<a name="ln3950">	// map the pages</a>
<a name="ln3951">	for (uint32 i = 0; i &lt; PAGE_ALIGN(physicalSize) / B_PAGE_SIZE; i++) {</a>
<a name="ln3952">		page_num_t physicalAddress = vm_allocate_early_physical_page(args);</a>
<a name="ln3953">		if (physicalAddress == 0)</a>
<a name="ln3954">			panic(&quot;error allocating early page!\n&quot;);</a>
<a name="ln3955"> </a>
<a name="ln3956">		//dprintf(&quot;vm_allocate_early: paddr 0x%lx\n&quot;, physicalAddress);</a>
<a name="ln3957"> </a>
<a name="ln3958">		arch_vm_translation_map_early_map(args, virtualBase + i * B_PAGE_SIZE,</a>
<a name="ln3959">			physicalAddress * B_PAGE_SIZE, attributes,</a>
<a name="ln3960">			&amp;vm_allocate_early_physical_page);</a>
<a name="ln3961">	}</a>
<a name="ln3962"> </a>
<a name="ln3963">	return virtualBase;</a>
<a name="ln3964">}</a>
<a name="ln3965"> </a>
<a name="ln3966"> </a>
<a name="ln3967">/*!	The main entrance point to initialize the VM. */</a>
<a name="ln3968">status_t</a>
<a name="ln3969">vm_init(kernel_args* args)</a>
<a name="ln3970">{</a>
<a name="ln3971">	struct preloaded_image* image;</a>
<a name="ln3972">	void* address;</a>
<a name="ln3973">	status_t err = 0;</a>
<a name="ln3974">	uint32 i;</a>
<a name="ln3975"> </a>
<a name="ln3976">	TRACE((&quot;vm_init: entry\n&quot;));</a>
<a name="ln3977">	err = arch_vm_translation_map_init(args, &amp;sPhysicalPageMapper);</a>
<a name="ln3978">	err = arch_vm_init(args);</a>
<a name="ln3979"> </a>
<a name="ln3980">	// initialize some globals</a>
<a name="ln3981">	vm_page_init_num_pages(args);</a>
<a name="ln3982">	sAvailableMemory = vm_page_num_pages() * B_PAGE_SIZE;</a>
<a name="ln3983"> </a>
<a name="ln3984">	slab_init(args);</a>
<a name="ln3985"> </a>
<a name="ln3986">#if USE_DEBUG_HEAP_FOR_MALLOC || USE_GUARDED_HEAP_FOR_MALLOC</a>
<a name="ln3987">	off_t heapSize = INITIAL_HEAP_SIZE;</a>
<a name="ln3988">	// try to accomodate low memory systems</a>
<a name="ln3989">	while (heapSize &gt; sAvailableMemory / 8)</a>
<a name="ln3990">		heapSize /= 2;</a>
<a name="ln3991">	if (heapSize &lt; 1024 * 1024)</a>
<a name="ln3992">		panic(&quot;vm_init: go buy some RAM please.&quot;);</a>
<a name="ln3993"> </a>
<a name="ln3994">	// map in the new heap and initialize it</a>
<a name="ln3995">	addr_t heapBase = vm_allocate_early(args, heapSize, heapSize,</a>
<a name="ln3996">		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA, 0);</a>
<a name="ln3997">	TRACE((&quot;heap at 0x%lx\n&quot;, heapBase));</a>
<a name="ln3998">	heap_init(heapBase, heapSize);</a>
<a name="ln3999">#endif</a>
<a name="ln4000"> </a>
<a name="ln4001">	// initialize the free page list and physical page mapper</a>
<a name="ln4002">	vm_page_init(args);</a>
<a name="ln4003"> </a>
<a name="ln4004">	// initialize the cache allocators</a>
<a name="ln4005">	vm_cache_init(args);</a>
<a name="ln4006"> </a>
<a name="ln4007">	{</a>
<a name="ln4008">		status_t error = VMAreaHash::Init();</a>
<a name="ln4009">		if (error != B_OK)</a>
<a name="ln4010">			panic(&quot;vm_init: error initializing area hash table\n&quot;);</a>
<a name="ln4011">	}</a>
<a name="ln4012"> </a>
<a name="ln4013">	VMAddressSpace::Init();</a>
<a name="ln4014">	reserve_boot_loader_ranges(args);</a>
<a name="ln4015"> </a>
<a name="ln4016">#if USE_DEBUG_HEAP_FOR_MALLOC || USE_GUARDED_HEAP_FOR_MALLOC</a>
<a name="ln4017">	heap_init_post_area();</a>
<a name="ln4018">#endif</a>
<a name="ln4019"> </a>
<a name="ln4020">	// Do any further initialization that the architecture dependant layers may</a>
<a name="ln4021">	// need now</a>
<a name="ln4022">	arch_vm_translation_map_init_post_area(args);</a>
<a name="ln4023">	arch_vm_init_post_area(args);</a>
<a name="ln4024">	vm_page_init_post_area(args);</a>
<a name="ln4025">	slab_init_post_area();</a>
<a name="ln4026"> </a>
<a name="ln4027">	// allocate areas to represent stuff that already exists</a>
<a name="ln4028"> </a>
<a name="ln4029">#if USE_DEBUG_HEAP_FOR_MALLOC || USE_GUARDED_HEAP_FOR_MALLOC</a>
<a name="ln4030">	address = (void*)ROUNDDOWN(heapBase, B_PAGE_SIZE);</a>
<a name="ln4031">	create_area(&quot;kernel heap&quot;, &amp;address, B_EXACT_ADDRESS, heapSize,</a>
<a name="ln4032">		B_ALREADY_WIRED, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln4033">#endif</a>
<a name="ln4034"> </a>
<a name="ln4035">	allocate_kernel_args(args);</a>
<a name="ln4036"> </a>
<a name="ln4037">	create_preloaded_image_areas(args-&gt;kernel_image);</a>
<a name="ln4038"> </a>
<a name="ln4039">	// allocate areas for preloaded images</a>
<a name="ln4040">	for (image = args-&gt;preloaded_images; image != NULL; image = image-&gt;next)</a>
<a name="ln4041">		create_preloaded_image_areas(image);</a>
<a name="ln4042"> </a>
<a name="ln4043">	// allocate kernel stacks</a>
<a name="ln4044">	for (i = 0; i &lt; args-&gt;num_cpus; i++) {</a>
<a name="ln4045">		char name[64];</a>
<a name="ln4046"> </a>
<a name="ln4047">		sprintf(name, &quot;idle thread %&quot; B_PRIu32 &quot; kstack&quot;, i + 1);</a>
<a name="ln4048">		address = (void*)args-&gt;cpu_kstack[i].start;</a>
<a name="ln4049">		create_area(name, &amp;address, B_EXACT_ADDRESS, args-&gt;cpu_kstack[i].size,</a>
<a name="ln4050">			B_ALREADY_WIRED, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln4051">	}</a>
<a name="ln4052"> </a>
<a name="ln4053">	void* lastPage = (void*)ROUNDDOWN(~(addr_t)0, B_PAGE_SIZE);</a>
<a name="ln4054">	vm_block_address_range(&quot;overflow protection&quot;, lastPage, B_PAGE_SIZE);</a>
<a name="ln4055"> </a>
<a name="ln4056">#if PARANOID_KERNEL_MALLOC</a>
<a name="ln4057">	vm_block_address_range(&quot;uninitialized heap memory&quot;,</a>
<a name="ln4058">		(void *)ROUNDDOWN(0xcccccccc, B_PAGE_SIZE), B_PAGE_SIZE * 64);</a>
<a name="ln4059">#endif</a>
<a name="ln4060">#if PARANOID_KERNEL_FREE</a>
<a name="ln4061">	vm_block_address_range(&quot;freed heap memory&quot;,</a>
<a name="ln4062">		(void *)ROUNDDOWN(0xdeadbeef, B_PAGE_SIZE), B_PAGE_SIZE * 64);</a>
<a name="ln4063">#endif</a>
<a name="ln4064"> </a>
<a name="ln4065">	// create the object cache for the page mappings</a>
<a name="ln4066">	gPageMappingsObjectCache = create_object_cache_etc(&quot;page mappings&quot;,</a>
<a name="ln4067">		sizeof(vm_page_mapping), 0, 0, 64, 128, CACHE_LARGE_SLAB, NULL, NULL,</a>
<a name="ln4068">		NULL, NULL);</a>
<a name="ln4069">	if (gPageMappingsObjectCache == NULL)</a>
<a name="ln4070">		panic(&quot;failed to create page mappings object cache&quot;);</a>
<a name="ln4071"> </a>
<a name="ln4072">	object_cache_set_minimum_reserve(gPageMappingsObjectCache, 1024);</a>
<a name="ln4073"> </a>
<a name="ln4074">#if DEBUG_CACHE_LIST</a>
<a name="ln4075">	if (vm_page_num_free_pages() &gt;= 200 * 1024 * 1024 / B_PAGE_SIZE) {</a>
<a name="ln4076">		virtual_address_restrictions virtualRestrictions = {};</a>
<a name="ln4077">		virtualRestrictions.address_specification = B_ANY_KERNEL_ADDRESS;</a>
<a name="ln4078">		physical_address_restrictions physicalRestrictions = {};</a>
<a name="ln4079">		create_area_etc(VMAddressSpace::KernelID(), &quot;cache info table&quot;,</a>
<a name="ln4080">			ROUNDUP(kCacheInfoTableCount * sizeof(cache_info), B_PAGE_SIZE),</a>
<a name="ln4081">			B_FULL_LOCK, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA,</a>
<a name="ln4082">			CREATE_AREA_DONT_WAIT, 0, &amp;virtualRestrictions,</a>
<a name="ln4083">			&amp;physicalRestrictions, (void**)&amp;sCacheInfoTable);</a>
<a name="ln4084">	}</a>
<a name="ln4085">#endif	// DEBUG_CACHE_LIST</a>
<a name="ln4086"> </a>
<a name="ln4087">	// add some debugger commands</a>
<a name="ln4088">	add_debugger_command(&quot;areas&quot;, &amp;dump_area_list, &quot;Dump a list of all areas&quot;);</a>
<a name="ln4089">	add_debugger_command(&quot;area&quot;, &amp;dump_area,</a>
<a name="ln4090">		&quot;Dump info about a particular area&quot;);</a>
<a name="ln4091">	add_debugger_command(&quot;cache&quot;, &amp;dump_cache, &quot;Dump VMCache&quot;);</a>
<a name="ln4092">	add_debugger_command(&quot;cache_tree&quot;, &amp;dump_cache_tree, &quot;Dump VMCache tree&quot;);</a>
<a name="ln4093">#if DEBUG_CACHE_LIST</a>
<a name="ln4094">	if (sCacheInfoTable != NULL) {</a>
<a name="ln4095">		add_debugger_command_etc(&quot;caches&quot;, &amp;dump_caches,</a>
<a name="ln4096">			&quot;List all VMCache trees&quot;,</a>
<a name="ln4097">			&quot;[ \&quot;-c\&quot; ]\n&quot;</a>
<a name="ln4098">			&quot;All cache trees are listed sorted in decreasing order by number &quot;</a>
<a name="ln4099">				&quot;of\n&quot;</a>
<a name="ln4100">			&quot;used pages or, if \&quot;-c\&quot; is specified, by size of committed &quot;</a>
<a name="ln4101">				&quot;memory.\n&quot;,</a>
<a name="ln4102">			0);</a>
<a name="ln4103">	}</a>
<a name="ln4104">#endif</a>
<a name="ln4105">	add_debugger_command(&quot;avail&quot;, &amp;dump_available_memory,</a>
<a name="ln4106">		&quot;Dump available memory&quot;);</a>
<a name="ln4107">	add_debugger_command(&quot;dl&quot;, &amp;display_mem, &quot;dump memory long words (64-bit)&quot;);</a>
<a name="ln4108">	add_debugger_command(&quot;dw&quot;, &amp;display_mem, &quot;dump memory words (32-bit)&quot;);</a>
<a name="ln4109">	add_debugger_command(&quot;ds&quot;, &amp;display_mem, &quot;dump memory shorts (16-bit)&quot;);</a>
<a name="ln4110">	add_debugger_command(&quot;db&quot;, &amp;display_mem, &quot;dump memory bytes (8-bit)&quot;);</a>
<a name="ln4111">	add_debugger_command(&quot;string&quot;, &amp;display_mem, &quot;dump strings&quot;);</a>
<a name="ln4112"> </a>
<a name="ln4113">	add_debugger_command_etc(&quot;mapping&quot;, &amp;dump_mapping_info,</a>
<a name="ln4114">		&quot;Print address mapping information&quot;,</a>
<a name="ln4115">		&quot;[ \&quot;-r\&quot; | \&quot;-p\&quot; ] &lt;address&gt; [ &lt;thread ID&gt; ]\n&quot;</a>
<a name="ln4116">		&quot;Prints low-level page mapping information for a given address. If\n&quot;</a>
<a name="ln4117">		&quot;neither \&quot;-r\&quot; nor \&quot;-p\&quot; are specified, &lt;address&gt; is a virtual\n&quot;</a>
<a name="ln4118">		&quot;address that is looked up in the translation map of the current\n&quot;</a>
<a name="ln4119">		&quot;team, respectively the team specified by thread ID &lt;thread ID&gt;. If\n&quot;</a>
<a name="ln4120">		&quot;\&quot;-r\&quot; is specified, &lt;address&gt; is a physical address that is\n&quot;</a>
<a name="ln4121">		&quot;searched in the translation map of all teams, respectively the team\n&quot;</a>
<a name="ln4122">		&quot;specified by thread ID &lt;thread ID&gt;. If \&quot;-p\&quot; is specified,\n&quot;</a>
<a name="ln4123">		&quot;&lt;address&gt; is the address of a vm_page structure. The behavior is\n&quot;</a>
<a name="ln4124">		&quot;equivalent to specifying \&quot;-r\&quot; with the physical address of that\n&quot;</a>
<a name="ln4125">		&quot;page.\n&quot;,</a>
<a name="ln4126">		0);</a>
<a name="ln4127"> </a>
<a name="ln4128">	TRACE((&quot;vm_init: exit\n&quot;));</a>
<a name="ln4129"> </a>
<a name="ln4130">	vm_cache_init_post_heap();</a>
<a name="ln4131"> </a>
<a name="ln4132">	return err;</a>
<a name="ln4133">}</a>
<a name="ln4134"> </a>
<a name="ln4135"> </a>
<a name="ln4136">status_t</a>
<a name="ln4137">vm_init_post_sem(kernel_args* args)</a>
<a name="ln4138">{</a>
<a name="ln4139">	// This frees all unused boot loader resources and makes its space available</a>
<a name="ln4140">	// again</a>
<a name="ln4141">	arch_vm_init_end(args);</a>
<a name="ln4142">	unreserve_boot_loader_ranges(args);</a>
<a name="ln4143"> </a>
<a name="ln4144">	// fill in all of the semaphores that were not allocated before</a>
<a name="ln4145">	// since we're still single threaded and only the kernel address space</a>
<a name="ln4146">	// exists, it isn't that hard to find all of the ones we need to create</a>
<a name="ln4147"> </a>
<a name="ln4148">	arch_vm_translation_map_init_post_sem(args);</a>
<a name="ln4149"> </a>
<a name="ln4150">	slab_init_post_sem();</a>
<a name="ln4151"> </a>
<a name="ln4152">#if USE_DEBUG_HEAP_FOR_MALLOC || USE_GUARDED_HEAP_FOR_MALLOC</a>
<a name="ln4153">	heap_init_post_sem();</a>
<a name="ln4154">#endif</a>
<a name="ln4155"> </a>
<a name="ln4156">	return B_OK;</a>
<a name="ln4157">}</a>
<a name="ln4158"> </a>
<a name="ln4159"> </a>
<a name="ln4160">status_t</a>
<a name="ln4161">vm_init_post_thread(kernel_args* args)</a>
<a name="ln4162">{</a>
<a name="ln4163">	vm_page_init_post_thread(args);</a>
<a name="ln4164">	slab_init_post_thread();</a>
<a name="ln4165">	return heap_init_post_thread();</a>
<a name="ln4166">}</a>
<a name="ln4167"> </a>
<a name="ln4168"> </a>
<a name="ln4169">status_t</a>
<a name="ln4170">vm_init_post_modules(kernel_args* args)</a>
<a name="ln4171">{</a>
<a name="ln4172">	return arch_vm_init_post_modules(args);</a>
<a name="ln4173">}</a>
<a name="ln4174"> </a>
<a name="ln4175"> </a>
<a name="ln4176">void</a>
<a name="ln4177">permit_page_faults(void)</a>
<a name="ln4178">{</a>
<a name="ln4179">	Thread* thread = thread_get_current_thread();</a>
<a name="ln4180">	if (thread != NULL)</a>
<a name="ln4181">		atomic_add(&amp;thread-&gt;page_faults_allowed, 1);</a>
<a name="ln4182">}</a>
<a name="ln4183"> </a>
<a name="ln4184"> </a>
<a name="ln4185">void</a>
<a name="ln4186">forbid_page_faults(void)</a>
<a name="ln4187">{</a>
<a name="ln4188">	Thread* thread = thread_get_current_thread();</a>
<a name="ln4189">	if (thread != NULL)</a>
<a name="ln4190">		atomic_add(&amp;thread-&gt;page_faults_allowed, -1);</a>
<a name="ln4191">}</a>
<a name="ln4192"> </a>
<a name="ln4193"> </a>
<a name="ln4194">status_t</a>
<a name="ln4195">vm_page_fault(addr_t address, addr_t faultAddress, bool isWrite, bool isExecute,</a>
<a name="ln4196">	bool isUser, addr_t* newIP)</a>
<a name="ln4197">{</a>
<a name="ln4198">	FTRACE((&quot;vm_page_fault: page fault at 0x%lx, ip 0x%lx\n&quot;, address,</a>
<a name="ln4199">		faultAddress));</a>
<a name="ln4200"> </a>
<a name="ln4201">	TPF(PageFaultStart(address, isWrite, isUser, faultAddress));</a>
<a name="ln4202"> </a>
<a name="ln4203">	addr_t pageAddress = ROUNDDOWN(address, B_PAGE_SIZE);</a>
<a name="ln4204">	VMAddressSpace* addressSpace = NULL;</a>
<a name="ln4205"> </a>
<a name="ln4206">	status_t status = B_OK;</a>
<a name="ln4207">	*newIP = 0;</a>
<a name="ln4208">	atomic_add((int32*)&amp;sPageFaults, 1);</a>
<a name="ln4209"> </a>
<a name="ln4210">	if (IS_KERNEL_ADDRESS(pageAddress)) {</a>
<a name="ln4211">		addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln4212">	} else if (IS_USER_ADDRESS(pageAddress)) {</a>
<a name="ln4213">		addressSpace = VMAddressSpace::GetCurrent();</a>
<a name="ln4214">		if (addressSpace == NULL) {</a>
<a name="ln4215">			if (!isUser) {</a>
<a name="ln4216">				dprintf(&quot;vm_page_fault: kernel thread accessing invalid user &quot;</a>
<a name="ln4217">					&quot;memory!\n&quot;);</a>
<a name="ln4218">				status = B_BAD_ADDRESS;</a>
<a name="ln4219">				TPF(PageFaultError(-1,</a>
<a name="ln4220">					VMPageFaultTracing</a>
<a name="ln4221">						::PAGE_FAULT_ERROR_KERNEL_BAD_USER_MEMORY));</a>
<a name="ln4222">			} else {</a>
<a name="ln4223">				// XXX weird state.</a>
<a name="ln4224">				panic(&quot;vm_page_fault: non kernel thread accessing user memory &quot;</a>
<a name="ln4225">					&quot;that doesn't exist!\n&quot;);</a>
<a name="ln4226">				status = B_BAD_ADDRESS;</a>
<a name="ln4227">			}</a>
<a name="ln4228">		}</a>
<a name="ln4229">	} else {</a>
<a name="ln4230">		// the hit was probably in the 64k DMZ between kernel and user space</a>
<a name="ln4231">		// this keeps a user space thread from passing a buffer that crosses</a>
<a name="ln4232">		// into kernel space</a>
<a name="ln4233">		status = B_BAD_ADDRESS;</a>
<a name="ln4234">		TPF(PageFaultError(-1,</a>
<a name="ln4235">			VMPageFaultTracing::PAGE_FAULT_ERROR_NO_ADDRESS_SPACE));</a>
<a name="ln4236">	}</a>
<a name="ln4237"> </a>
<a name="ln4238">	if (status == B_OK) {</a>
<a name="ln4239">		status = vm_soft_fault(addressSpace, pageAddress, isWrite, isExecute,</a>
<a name="ln4240">			isUser, NULL);</a>
<a name="ln4241">	}</a>
<a name="ln4242"> </a>
<a name="ln4243">	if (status &lt; B_OK) {</a>
<a name="ln4244">		dprintf(&quot;vm_page_fault: vm_soft_fault returned error '%s' on fault at &quot;</a>
<a name="ln4245">			&quot;0x%lx, ip 0x%lx, write %d, user %d, thread 0x%&quot; B_PRIx32 &quot;\n&quot;,</a>
<a name="ln4246">			strerror(status), address, faultAddress, isWrite, isUser,</a>
<a name="ln4247">			thread_get_current_thread_id());</a>
<a name="ln4248">		if (!isUser) {</a>
<a name="ln4249">			Thread* thread = thread_get_current_thread();</a>
<a name="ln4250">			if (thread != NULL &amp;&amp; thread-&gt;fault_handler != 0) {</a>
<a name="ln4251">				// this will cause the arch dependant page fault handler to</a>
<a name="ln4252">				// modify the IP on the interrupt frame or whatever to return</a>
<a name="ln4253">				// to this address</a>
<a name="ln4254">				*newIP = reinterpret_cast&lt;uintptr_t&gt;(thread-&gt;fault_handler);</a>
<a name="ln4255">			} else {</a>
<a name="ln4256">				// unhandled page fault in the kernel</a>
<a name="ln4257">				panic(&quot;vm_page_fault: unhandled page fault in kernel space at &quot;</a>
<a name="ln4258">					&quot;0x%lx, ip 0x%lx\n&quot;, address, faultAddress);</a>
<a name="ln4259">			}</a>
<a name="ln4260">		} else {</a>
<a name="ln4261">			Thread* thread = thread_get_current_thread();</a>
<a name="ln4262"> </a>
<a name="ln4263">#ifdef TRACE_FAULTS</a>
<a name="ln4264">			VMArea* area = NULL;</a>
<a name="ln4265">			if (addressSpace != NULL) {</a>
<a name="ln4266">				addressSpace-&gt;ReadLock();</a>
<a name="ln4267">				area = addressSpace-&gt;LookupArea(faultAddress);</a>
<a name="ln4268">			}</a>
<a name="ln4269"> </a>
<a name="ln4270">			dprintf(&quot;vm_page_fault: thread \&quot;%s\&quot; (%&quot; B_PRId32 &quot;) in team &quot;</a>
<a name="ln4271">				&quot;\&quot;%s\&quot; (%&quot; B_PRId32 &quot;) tried to %s address %#lx, ip %#lx &quot;</a>
<a name="ln4272">				&quot;(\&quot;%s\&quot; +%#lx)\n&quot;, thread-&gt;name, thread-&gt;id,</a>
<a name="ln4273">				thread-&gt;team-&gt;Name(), thread-&gt;team-&gt;id,</a>
<a name="ln4274">				isWrite ? &quot;write&quot; : (isExecute ? &quot;execute&quot; : &quot;read&quot;), address,</a>
<a name="ln4275">				faultAddress, area ? area-&gt;name : &quot;???&quot;, faultAddress - (area ?</a>
<a name="ln4276">					area-&gt;Base() : 0x0));</a>
<a name="ln4277"> </a>
<a name="ln4278">			if (addressSpace != NULL)</a>
<a name="ln4279">				addressSpace-&gt;ReadUnlock();</a>
<a name="ln4280">#endif</a>
<a name="ln4281"> </a>
<a name="ln4282">			// If the thread has a signal handler for SIGSEGV, we simply</a>
<a name="ln4283">			// send it the signal. Otherwise we notify the user debugger</a>
<a name="ln4284">			// first.</a>
<a name="ln4285">			struct sigaction action;</a>
<a name="ln4286">			if ((sigaction(SIGSEGV, NULL, &amp;action) == 0</a>
<a name="ln4287">					&amp;&amp; action.sa_handler != SIG_DFL</a>
<a name="ln4288">					&amp;&amp; action.sa_handler != SIG_IGN)</a>
<a name="ln4289">				|| user_debug_exception_occurred(B_SEGMENT_VIOLATION,</a>
<a name="ln4290">					SIGSEGV)) {</a>
<a name="ln4291">				Signal signal(SIGSEGV,</a>
<a name="ln4292">					status == B_PERMISSION_DENIED</a>
<a name="ln4293">						? SEGV_ACCERR : SEGV_MAPERR,</a>
<a name="ln4294">					EFAULT, thread-&gt;team-&gt;id);</a>
<a name="ln4295">				signal.SetAddress((void*)address);</a>
<a name="ln4296">				send_signal_to_thread(thread, signal, 0);</a>
<a name="ln4297">			}</a>
<a name="ln4298">		}</a>
<a name="ln4299">	}</a>
<a name="ln4300"> </a>
<a name="ln4301">	if (addressSpace != NULL)</a>
<a name="ln4302">		addressSpace-&gt;Put();</a>
<a name="ln4303"> </a>
<a name="ln4304">	return B_HANDLED_INTERRUPT;</a>
<a name="ln4305">}</a>
<a name="ln4306"> </a>
<a name="ln4307"> </a>
<a name="ln4308">struct PageFaultContext {</a>
<a name="ln4309">	AddressSpaceReadLocker	addressSpaceLocker;</a>
<a name="ln4310">	VMCacheChainLocker		cacheChainLocker;</a>
<a name="ln4311"> </a>
<a name="ln4312">	VMTranslationMap*		map;</a>
<a name="ln4313">	VMCache*				topCache;</a>
<a name="ln4314">	off_t					cacheOffset;</a>
<a name="ln4315">	vm_page_reservation		reservation;</a>
<a name="ln4316">	bool					isWrite;</a>
<a name="ln4317"> </a>
<a name="ln4318">	// return values</a>
<a name="ln4319">	vm_page*				page;</a>
<a name="ln4320">	bool					restart;</a>
<a name="ln4321">	bool					pageAllocated;</a>
<a name="ln4322"> </a>
<a name="ln4323"> </a>
<a name="ln4324">	PageFaultContext(VMAddressSpace* addressSpace, bool isWrite)</a>
<a name="ln4325">		:</a>
<a name="ln4326">		addressSpaceLocker(addressSpace, true),</a>
<a name="ln4327">		map(addressSpace-&gt;TranslationMap()),</a>
<a name="ln4328">		isWrite(isWrite)</a>
<a name="ln4329">	{</a>
<a name="ln4330">	}</a>
<a name="ln4331"> </a>
<a name="ln4332">	~PageFaultContext()</a>
<a name="ln4333">	{</a>
<a name="ln4334">		UnlockAll();</a>
<a name="ln4335">		vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln4336">	}</a>
<a name="ln4337"> </a>
<a name="ln4338">	void Prepare(VMCache* topCache, off_t cacheOffset)</a>
<a name="ln4339">	{</a>
<a name="ln4340">		this-&gt;topCache = topCache;</a>
<a name="ln4341">		this-&gt;cacheOffset = cacheOffset;</a>
<a name="ln4342">		page = NULL;</a>
<a name="ln4343">		restart = false;</a>
<a name="ln4344">		pageAllocated = false;</a>
<a name="ln4345"> </a>
<a name="ln4346">		cacheChainLocker.SetTo(topCache);</a>
<a name="ln4347">	}</a>
<a name="ln4348"> </a>
<a name="ln4349">	void UnlockAll(VMCache* exceptCache = NULL)</a>
<a name="ln4350">	{</a>
<a name="ln4351">		topCache = NULL;</a>
<a name="ln4352">		addressSpaceLocker.Unlock();</a>
<a name="ln4353">		cacheChainLocker.Unlock(exceptCache);</a>
<a name="ln4354">	}</a>
<a name="ln4355">};</a>
<a name="ln4356"> </a>
<a name="ln4357"> </a>
<a name="ln4358">/*!	Gets the page that should be mapped into the area.</a>
<a name="ln4359">	Returns an error code other than \c B_OK, if the page couldn't be found or</a>
<a name="ln4360">	paged in. The locking state of the address space and the caches is undefined</a>
<a name="ln4361">	in that case.</a>
<a name="ln4362">	Returns \c B_OK with \c context.restart set to \c true, if the functions</a>
<a name="ln4363">	had to unlock the address space and all caches and is supposed to be called</a>
<a name="ln4364">	again.</a>
<a name="ln4365">	Returns \c B_OK with \c context.restart set to \c false, if the page was</a>
<a name="ln4366">	found. It is returned in \c context.page. The address space will still be</a>
<a name="ln4367">	locked as well as all caches starting from the top cache to at least the</a>
<a name="ln4368">	cache the page lives in.</a>
<a name="ln4369">*/</a>
<a name="ln4370">static status_t</a>
<a name="ln4371">fault_get_page(PageFaultContext&amp; context)</a>
<a name="ln4372">{</a>
<a name="ln4373">	VMCache* cache = context.topCache;</a>
<a name="ln4374">	VMCache* lastCache = NULL;</a>
<a name="ln4375">	vm_page* page = NULL;</a>
<a name="ln4376"> </a>
<a name="ln4377">	while (cache != NULL) {</a>
<a name="ln4378">		// We already hold the lock of the cache at this point.</a>
<a name="ln4379"> </a>
<a name="ln4380">		lastCache = cache;</a>
<a name="ln4381"> </a>
<a name="ln4382">		page = cache-&gt;LookupPage(context.cacheOffset);</a>
<a name="ln4383">		if (page != NULL &amp;&amp; page-&gt;busy) {</a>
<a name="ln4384">			// page must be busy -- wait for it to become unbusy</a>
<a name="ln4385">			context.UnlockAll(cache);</a>
<a name="ln4386">			cache-&gt;ReleaseRefLocked();</a>
<a name="ln4387">			cache-&gt;WaitForPageEvents(page, PAGE_EVENT_NOT_BUSY, false);</a>
<a name="ln4388"> </a>
<a name="ln4389">			// restart the whole process</a>
<a name="ln4390">			context.restart = true;</a>
<a name="ln4391">			return B_OK;</a>
<a name="ln4392">		}</a>
<a name="ln4393"> </a>
<a name="ln4394">		if (page != NULL)</a>
<a name="ln4395">			break;</a>
<a name="ln4396"> </a>
<a name="ln4397">		// The current cache does not contain the page we're looking for.</a>
<a name="ln4398"> </a>
<a name="ln4399">		// see if the backing store has it</a>
<a name="ln4400">		if (cache-&gt;HasPage(context.cacheOffset)) {</a>
<a name="ln4401">			// insert a fresh page and mark it busy -- we're going to read it in</a>
<a name="ln4402">			page = vm_page_allocate_page(&amp;context.reservation,</a>
<a name="ln4403">				PAGE_STATE_ACTIVE | VM_PAGE_ALLOC_BUSY);</a>
<a name="ln4404">			cache-&gt;InsertPage(page, context.cacheOffset);</a>
<a name="ln4405"> </a>
<a name="ln4406">			// We need to unlock all caches and the address space while reading</a>
<a name="ln4407">			// the page in. Keep a reference to the cache around.</a>
<a name="ln4408">			cache-&gt;AcquireRefLocked();</a>
<a name="ln4409">			context.UnlockAll();</a>
<a name="ln4410"> </a>
<a name="ln4411">			// read the page in</a>
<a name="ln4412">			generic_io_vec vec;</a>
<a name="ln4413">			vec.base = (phys_addr_t)page-&gt;physical_page_number * B_PAGE_SIZE;</a>
<a name="ln4414">			generic_size_t bytesRead = vec.length = B_PAGE_SIZE;</a>
<a name="ln4415"> </a>
<a name="ln4416">			status_t status = cache-&gt;Read(context.cacheOffset, &amp;vec, 1,</a>
<a name="ln4417">				B_PHYSICAL_IO_REQUEST, &amp;bytesRead);</a>
<a name="ln4418"> </a>
<a name="ln4419">			cache-&gt;Lock();</a>
<a name="ln4420"> </a>
<a name="ln4421">			if (status &lt; B_OK) {</a>
<a name="ln4422">				// on error remove and free the page</a>
<a name="ln4423">				dprintf(&quot;reading page from cache %p returned: %s!\n&quot;,</a>
<a name="ln4424">					cache, strerror(status));</a>
<a name="ln4425"> </a>
<a name="ln4426">				cache-&gt;NotifyPageEvents(page, PAGE_EVENT_NOT_BUSY);</a>
<a name="ln4427">				cache-&gt;RemovePage(page);</a>
<a name="ln4428">				vm_page_set_state(page, PAGE_STATE_FREE);</a>
<a name="ln4429"> </a>
<a name="ln4430">				cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln4431">				return status;</a>
<a name="ln4432">			}</a>
<a name="ln4433"> </a>
<a name="ln4434">			// mark the page unbusy again</a>
<a name="ln4435">			cache-&gt;MarkPageUnbusy(page);</a>
<a name="ln4436"> </a>
<a name="ln4437">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln4438"> </a>
<a name="ln4439">			// Since we needed to unlock everything temporarily, the area</a>
<a name="ln4440">			// situation might have changed. So we need to restart the whole</a>
<a name="ln4441">			// process.</a>
<a name="ln4442">			cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln4443">			context.restart = true;</a>
<a name="ln4444">			return B_OK;</a>
<a name="ln4445">		}</a>
<a name="ln4446"> </a>
<a name="ln4447">		cache = context.cacheChainLocker.LockSourceCache();</a>
<a name="ln4448">	}</a>
<a name="ln4449"> </a>
<a name="ln4450">	if (page == NULL) {</a>
<a name="ln4451">		// There was no adequate page, determine the cache for a clean one.</a>
<a name="ln4452">		// Read-only pages come in the deepest cache, only the top most cache</a>
<a name="ln4453">		// may have direct write access.</a>
<a name="ln4454">		cache = context.isWrite ? context.topCache : lastCache;</a>
<a name="ln4455"> </a>
<a name="ln4456">		// allocate a clean page</a>
<a name="ln4457">		page = vm_page_allocate_page(&amp;context.reservation,</a>
<a name="ln4458">			PAGE_STATE_ACTIVE | VM_PAGE_ALLOC_CLEAR);</a>
<a name="ln4459">		FTRACE((&quot;vm_soft_fault: just allocated page 0x%&quot; B_PRIxPHYSADDR &quot;\n&quot;,</a>
<a name="ln4460">			page-&gt;physical_page_number));</a>
<a name="ln4461"> </a>
<a name="ln4462">		// insert the new page into our cache</a>
<a name="ln4463">		cache-&gt;InsertPage(page, context.cacheOffset);</a>
<a name="ln4464">		context.pageAllocated = true;</a>
<a name="ln4465">	} else if (page-&gt;Cache() != context.topCache &amp;&amp; context.isWrite) {</a>
<a name="ln4466">		// We have a page that has the data we want, but in the wrong cache</a>
<a name="ln4467">		// object so we need to copy it and stick it into the top cache.</a>
<a name="ln4468">		vm_page* sourcePage = page;</a>
<a name="ln4469"> </a>
<a name="ln4470">		// TODO: If memory is low, it might be a good idea to steal the page</a>
<a name="ln4471">		// from our source cache -- if possible, that is.</a>
<a name="ln4472">		FTRACE((&quot;get new page, copy it, and put it into the topmost cache\n&quot;));</a>
<a name="ln4473">		page = vm_page_allocate_page(&amp;context.reservation, PAGE_STATE_ACTIVE);</a>
<a name="ln4474"> </a>
<a name="ln4475">		// To not needlessly kill concurrency we unlock all caches but the top</a>
<a name="ln4476">		// one while copying the page. Lacking another mechanism to ensure that</a>
<a name="ln4477">		// the source page doesn't disappear, we mark it busy.</a>
<a name="ln4478">		sourcePage-&gt;busy = true;</a>
<a name="ln4479">		context.cacheChainLocker.UnlockKeepRefs(true);</a>
<a name="ln4480"> </a>
<a name="ln4481">		// copy the page</a>
<a name="ln4482">		vm_memcpy_physical_page(page-&gt;physical_page_number * B_PAGE_SIZE,</a>
<a name="ln4483">			sourcePage-&gt;physical_page_number * B_PAGE_SIZE);</a>
<a name="ln4484"> </a>
<a name="ln4485">		context.cacheChainLocker.RelockCaches(true);</a>
<a name="ln4486">		sourcePage-&gt;Cache()-&gt;MarkPageUnbusy(sourcePage);</a>
<a name="ln4487"> </a>
<a name="ln4488">		// insert the new page into our cache</a>
<a name="ln4489">		context.topCache-&gt;InsertPage(page, context.cacheOffset);</a>
<a name="ln4490">		context.pageAllocated = true;</a>
<a name="ln4491">	} else</a>
<a name="ln4492">		DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln4493"> </a>
<a name="ln4494">	context.page = page;</a>
<a name="ln4495">	return B_OK;</a>
<a name="ln4496">}</a>
<a name="ln4497"> </a>
<a name="ln4498"> </a>
<a name="ln4499">/*!	Makes sure the address in the given address space is mapped.</a>
<a name="ln4500"> </a>
<a name="ln4501">	\param addressSpace The address space.</a>
<a name="ln4502">	\param originalAddress The address. Doesn't need to be page aligned.</a>
<a name="ln4503">	\param isWrite If \c true the address shall be write-accessible.</a>
<a name="ln4504">	\param isUser If \c true the access is requested by a userland team.</a>
<a name="ln4505">	\param wirePage On success, if non \c NULL, the wired count of the page</a>
<a name="ln4506">		mapped at the given address is incremented and the page is returned</a>
<a name="ln4507">		via this parameter.</a>
<a name="ln4508">	\return \c B_OK on success, another error code otherwise.</a>
<a name="ln4509">*/</a>
<a name="ln4510">static status_t</a>
<a name="ln4511">vm_soft_fault(VMAddressSpace* addressSpace, addr_t originalAddress,</a>
<a name="ln4512">	bool isWrite, bool isExecute, bool isUser, vm_page** wirePage)</a>
<a name="ln4513">{</a>
<a name="ln4514">	FTRACE((&quot;vm_soft_fault: thid 0x%&quot; B_PRIx32 &quot; address 0x%&quot; B_PRIxADDR &quot;, &quot;</a>
<a name="ln4515">		&quot;isWrite %d, isUser %d\n&quot;, thread_get_current_thread_id(),</a>
<a name="ln4516">		originalAddress, isWrite, isUser));</a>
<a name="ln4517"> </a>
<a name="ln4518">	PageFaultContext context(addressSpace, isWrite);</a>
<a name="ln4519"> </a>
<a name="ln4520">	addr_t address = ROUNDDOWN(originalAddress, B_PAGE_SIZE);</a>
<a name="ln4521">	status_t status = B_OK;</a>
<a name="ln4522"> </a>
<a name="ln4523">	addressSpace-&gt;IncrementFaultCount();</a>
<a name="ln4524"> </a>
<a name="ln4525">	// We may need up to 2 pages plus pages needed for mapping them -- reserving</a>
<a name="ln4526">	// the pages upfront makes sure we don't have any cache locked, so that the</a>
<a name="ln4527">	// page daemon/thief can do their job without problems.</a>
<a name="ln4528">	size_t reservePages = 2 + context.map-&gt;MaxPagesNeededToMap(originalAddress,</a>
<a name="ln4529">		originalAddress);</a>
<a name="ln4530">	context.addressSpaceLocker.Unlock();</a>
<a name="ln4531">	vm_page_reserve_pages(&amp;context.reservation, reservePages,</a>
<a name="ln4532">		addressSpace == VMAddressSpace::Kernel()</a>
<a name="ln4533">			? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER);</a>
<a name="ln4534"> </a>
<a name="ln4535">	while (true) {</a>
<a name="ln4536">		context.addressSpaceLocker.Lock();</a>
<a name="ln4537"> </a>
<a name="ln4538">		// get the area the fault was in</a>
<a name="ln4539">		VMArea* area = addressSpace-&gt;LookupArea(address);</a>
<a name="ln4540">		if (area == NULL) {</a>
<a name="ln4541">			dprintf(&quot;vm_soft_fault: va 0x%lx not covered by area in address &quot;</a>
<a name="ln4542">				&quot;space\n&quot;, originalAddress);</a>
<a name="ln4543">			TPF(PageFaultError(-1,</a>
<a name="ln4544">				VMPageFaultTracing::PAGE_FAULT_ERROR_NO_AREA));</a>
<a name="ln4545">			status = B_BAD_ADDRESS;</a>
<a name="ln4546">			break;</a>
<a name="ln4547">		}</a>
<a name="ln4548"> </a>
<a name="ln4549">		// check permissions</a>
<a name="ln4550">		uint32 protection = get_area_page_protection(area, address);</a>
<a name="ln4551">		if (isUser &amp;&amp; (protection &amp; B_USER_PROTECTION) == 0) {</a>
<a name="ln4552">			dprintf(&quot;user access on kernel area 0x%&quot; B_PRIx32 &quot; at %p\n&quot;,</a>
<a name="ln4553">				area-&gt;id, (void*)originalAddress);</a>
<a name="ln4554">			TPF(PageFaultError(area-&gt;id,</a>
<a name="ln4555">				VMPageFaultTracing::PAGE_FAULT_ERROR_KERNEL_ONLY));</a>
<a name="ln4556">			status = B_PERMISSION_DENIED;</a>
<a name="ln4557">			break;</a>
<a name="ln4558">		}</a>
<a name="ln4559">		if (isWrite &amp;&amp; (protection</a>
<a name="ln4560">				&amp; (B_WRITE_AREA | (isUser ? 0 : B_KERNEL_WRITE_AREA))) == 0) {</a>
<a name="ln4561">			dprintf(&quot;write access attempted on write-protected area 0x%&quot;</a>
<a name="ln4562">				B_PRIx32 &quot; at %p\n&quot;, area-&gt;id, (void*)originalAddress);</a>
<a name="ln4563">			TPF(PageFaultError(area-&gt;id,</a>
<a name="ln4564">				VMPageFaultTracing::PAGE_FAULT_ERROR_WRITE_PROTECTED));</a>
<a name="ln4565">			status = B_PERMISSION_DENIED;</a>
<a name="ln4566">			break;</a>
<a name="ln4567">		} else if (isExecute &amp;&amp; (protection</a>
<a name="ln4568">				&amp; (B_EXECUTE_AREA</a>
<a name="ln4569">					| (isUser ? 0 : B_KERNEL_EXECUTE_AREA))) == 0) {</a>
<a name="ln4570">			dprintf(&quot;instruction fetch attempted on execute-protected area 0x%&quot;</a>
<a name="ln4571">				B_PRIx32 &quot; at %p\n&quot;, area-&gt;id, (void*)originalAddress);</a>
<a name="ln4572">			TPF(PageFaultError(area-&gt;id,</a>
<a name="ln4573">				VMPageFaultTracing::PAGE_FAULT_ERROR_EXECUTE_PROTECTED));</a>
<a name="ln4574">			status = B_PERMISSION_DENIED;</a>
<a name="ln4575">			break;</a>
<a name="ln4576">		} else if (!isWrite &amp;&amp; !isExecute &amp;&amp; (protection</a>
<a name="ln4577">				&amp; (B_READ_AREA | (isUser ? 0 : B_KERNEL_READ_AREA))) == 0) {</a>
<a name="ln4578">			dprintf(&quot;read access attempted on read-protected area 0x%&quot; B_PRIx32</a>
<a name="ln4579">				&quot; at %p\n&quot;, area-&gt;id, (void*)originalAddress);</a>
<a name="ln4580">			TPF(PageFaultError(area-&gt;id,</a>
<a name="ln4581">				VMPageFaultTracing::PAGE_FAULT_ERROR_READ_PROTECTED));</a>
<a name="ln4582">			status = B_PERMISSION_DENIED;</a>
<a name="ln4583">			break;</a>
<a name="ln4584">		}</a>
<a name="ln4585"> </a>
<a name="ln4586">		// We have the area, it was a valid access, so let's try to resolve the</a>
<a name="ln4587">		// page fault now.</a>
<a name="ln4588">		// At first, the top most cache from the area is investigated.</a>
<a name="ln4589"> </a>
<a name="ln4590">		context.Prepare(vm_area_get_locked_cache(area),</a>
<a name="ln4591">			address - area-&gt;Base() + area-&gt;cache_offset);</a>
<a name="ln4592"> </a>
<a name="ln4593">		// See if this cache has a fault handler -- this will do all the work</a>
<a name="ln4594">		// for us.</a>
<a name="ln4595">		{</a>
<a name="ln4596">			// Note, since the page fault is resolved with interrupts enabled,</a>
<a name="ln4597">			// the fault handler could be called more than once for the same</a>
<a name="ln4598">			// reason -- the store must take this into account.</a>
<a name="ln4599">			status = context.topCache-&gt;Fault(addressSpace, context.cacheOffset);</a>
<a name="ln4600">			if (status != B_BAD_HANDLER)</a>
<a name="ln4601">				break;</a>
<a name="ln4602">		}</a>
<a name="ln4603"> </a>
<a name="ln4604">		// The top most cache has no fault handler, so let's see if the cache or</a>
<a name="ln4605">		// its sources already have the page we're searching for (we're going</a>
<a name="ln4606">		// from top to bottom).</a>
<a name="ln4607">		status = fault_get_page(context);</a>
<a name="ln4608">		if (status != B_OK) {</a>
<a name="ln4609">			TPF(PageFaultError(area-&gt;id, status));</a>
<a name="ln4610">			break;</a>
<a name="ln4611">		}</a>
<a name="ln4612"> </a>
<a name="ln4613">		if (context.restart)</a>
<a name="ln4614">			continue;</a>
<a name="ln4615"> </a>
<a name="ln4616">		// All went fine, all there is left to do is to map the page into the</a>
<a name="ln4617">		// address space.</a>
<a name="ln4618">		TPF(PageFaultDone(area-&gt;id, context.topCache, context.page-&gt;Cache(),</a>
<a name="ln4619">			context.page));</a>
<a name="ln4620"> </a>
<a name="ln4621">		// If the page doesn't reside in the area's cache, we need to make sure</a>
<a name="ln4622">		// it's mapped in read-only, so that we cannot overwrite someone else's</a>
<a name="ln4623">		// data (copy-on-write)</a>
<a name="ln4624">		uint32 newProtection = protection;</a>
<a name="ln4625">		if (context.page-&gt;Cache() != context.topCache &amp;&amp; !isWrite)</a>
<a name="ln4626">			newProtection &amp;= ~(B_WRITE_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln4627"> </a>
<a name="ln4628">		bool unmapPage = false;</a>
<a name="ln4629">		bool mapPage = true;</a>
<a name="ln4630"> </a>
<a name="ln4631">		// check whether there's already a page mapped at the address</a>
<a name="ln4632">		context.map-&gt;Lock();</a>
<a name="ln4633"> </a>
<a name="ln4634">		phys_addr_t physicalAddress;</a>
<a name="ln4635">		uint32 flags;</a>
<a name="ln4636">		vm_page* mappedPage = NULL;</a>
<a name="ln4637">		if (context.map-&gt;Query(address, &amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln4638">			&amp;&amp; (flags &amp; PAGE_PRESENT) != 0</a>
<a name="ln4639">			&amp;&amp; (mappedPage = vm_lookup_page(physicalAddress / B_PAGE_SIZE))</a>
<a name="ln4640">				!= NULL) {</a>
<a name="ln4641">			// Yep there's already a page. If it's ours, we can simply adjust</a>
<a name="ln4642">			// its protection. Otherwise we have to unmap it.</a>
<a name="ln4643">			if (mappedPage == context.page) {</a>
<a name="ln4644">				context.map-&gt;ProtectPage(area, address, newProtection);</a>
<a name="ln4645">					// Note: We assume that ProtectPage() is atomic (i.e.</a>
<a name="ln4646">					// the page isn't temporarily unmapped), otherwise we'd have</a>
<a name="ln4647">					// to make sure it isn't wired.</a>
<a name="ln4648">				mapPage = false;</a>
<a name="ln4649">			} else</a>
<a name="ln4650">				unmapPage = true;</a>
<a name="ln4651">		}</a>
<a name="ln4652"> </a>
<a name="ln4653">		context.map-&gt;Unlock();</a>
<a name="ln4654"> </a>
<a name="ln4655">		if (unmapPage) {</a>
<a name="ln4656">			// If the page is wired, we can't unmap it. Wait until it is unwired</a>
<a name="ln4657">			// again and restart. Note that the page cannot be wired for</a>
<a name="ln4658">			// writing, since it it isn't in the topmost cache. So we can safely</a>
<a name="ln4659">			// ignore ranges wired for writing (our own and other concurrent</a>
<a name="ln4660">			// wiring attempts in progress) and in fact have to do that to avoid</a>
<a name="ln4661">			// a deadlock.</a>
<a name="ln4662">			VMAreaUnwiredWaiter waiter;</a>
<a name="ln4663">			if (area-&gt;AddWaiterIfWired(&amp;waiter, address, B_PAGE_SIZE,</a>
<a name="ln4664">					VMArea::IGNORE_WRITE_WIRED_RANGES)) {</a>
<a name="ln4665">				// unlock everything and wait</a>
<a name="ln4666">				if (context.pageAllocated) {</a>
<a name="ln4667">					// ... but since we allocated a page and inserted it into</a>
<a name="ln4668">					// the top cache, remove and free it first. Otherwise we'd</a>
<a name="ln4669">					// have a page from a lower cache mapped while an upper</a>
<a name="ln4670">					// cache has a page that would shadow it.</a>
<a name="ln4671">					context.topCache-&gt;RemovePage(context.page);</a>
<a name="ln4672">					vm_page_free_etc(context.topCache, context.page,</a>
<a name="ln4673">						&amp;context.reservation);</a>
<a name="ln4674">				} else</a>
<a name="ln4675">					DEBUG_PAGE_ACCESS_END(context.page);</a>
<a name="ln4676"> </a>
<a name="ln4677">				context.UnlockAll();</a>
<a name="ln4678">				waiter.waitEntry.Wait();</a>
<a name="ln4679">				continue;</a>
<a name="ln4680">			}</a>
<a name="ln4681"> </a>
<a name="ln4682">			// Note: The mapped page is a page of a lower cache. We are</a>
<a name="ln4683">			// guaranteed to have that cached locked, our new page is a copy of</a>
<a name="ln4684">			// that page, and the page is not busy. The logic for that guarantee</a>
<a name="ln4685">			// is as follows: Since the page is mapped, it must live in the top</a>
<a name="ln4686">			// cache (ruled out above) or any of its lower caches, and there is</a>
<a name="ln4687">			// (was before the new page was inserted) no other page in any</a>
<a name="ln4688">			// cache between the top cache and the page's cache (otherwise that</a>
<a name="ln4689">			// would be mapped instead). That in turn means that our algorithm</a>
<a name="ln4690">			// must have found it and therefore it cannot be busy either.</a>
<a name="ln4691">			DEBUG_PAGE_ACCESS_START(mappedPage);</a>
<a name="ln4692">			unmap_page(area, address);</a>
<a name="ln4693">			DEBUG_PAGE_ACCESS_END(mappedPage);</a>
<a name="ln4694">		}</a>
<a name="ln4695"> </a>
<a name="ln4696">		if (mapPage) {</a>
<a name="ln4697">			if (map_page(area, context.page, address, newProtection,</a>
<a name="ln4698">					&amp;context.reservation) != B_OK) {</a>
<a name="ln4699">				// Mapping can only fail, when the page mapping object couldn't</a>
<a name="ln4700">				// be allocated. Save for the missing mapping everything is</a>
<a name="ln4701">				// fine, though. If this was a regular page fault, we'll simply</a>
<a name="ln4702">				// leave and probably fault again. To make sure we'll have more</a>
<a name="ln4703">				// luck then, we ensure that the minimum object reserve is</a>
<a name="ln4704">				// available.</a>
<a name="ln4705">				DEBUG_PAGE_ACCESS_END(context.page);</a>
<a name="ln4706"> </a>
<a name="ln4707">				context.UnlockAll();</a>
<a name="ln4708"> </a>
<a name="ln4709">				if (object_cache_reserve(gPageMappingsObjectCache, 1, 0)</a>
<a name="ln4710">						!= B_OK) {</a>
<a name="ln4711">					// Apparently the situation is serious. Let's get ourselves</a>
<a name="ln4712">					// killed.</a>
<a name="ln4713">					status = B_NO_MEMORY;</a>
<a name="ln4714">				} else if (wirePage != NULL) {</a>
<a name="ln4715">					// The caller expects us to wire the page. Since</a>
<a name="ln4716">					// object_cache_reserve() succeeded, we should now be able</a>
<a name="ln4717">					// to allocate a mapping structure. Restart.</a>
<a name="ln4718">					continue;</a>
<a name="ln4719">				}</a>
<a name="ln4720"> </a>
<a name="ln4721">				break;</a>
<a name="ln4722">			}</a>
<a name="ln4723">		} else if (context.page-&gt;State() == PAGE_STATE_INACTIVE)</a>
<a name="ln4724">			vm_page_set_state(context.page, PAGE_STATE_ACTIVE);</a>
<a name="ln4725"> </a>
<a name="ln4726">		// also wire the page, if requested</a>
<a name="ln4727">		if (wirePage != NULL &amp;&amp; status == B_OK) {</a>
<a name="ln4728">			increment_page_wired_count(context.page);</a>
<a name="ln4729">			*wirePage = context.page;</a>
<a name="ln4730">		}</a>
<a name="ln4731"> </a>
<a name="ln4732">		DEBUG_PAGE_ACCESS_END(context.page);</a>
<a name="ln4733"> </a>
<a name="ln4734">		break;</a>
<a name="ln4735">	}</a>
<a name="ln4736"> </a>
<a name="ln4737">	return status;</a>
<a name="ln4738">}</a>
<a name="ln4739"> </a>
<a name="ln4740"> </a>
<a name="ln4741">status_t</a>
<a name="ln4742">vm_get_physical_page(phys_addr_t paddr, addr_t* _vaddr, void** _handle)</a>
<a name="ln4743">{</a>
<a name="ln4744">	return sPhysicalPageMapper-&gt;GetPage(paddr, _vaddr, _handle);</a>
<a name="ln4745">}</a>
<a name="ln4746"> </a>
<a name="ln4747">status_t</a>
<a name="ln4748">vm_put_physical_page(addr_t vaddr, void* handle)</a>
<a name="ln4749">{</a>
<a name="ln4750">	return sPhysicalPageMapper-&gt;PutPage(vaddr, handle);</a>
<a name="ln4751">}</a>
<a name="ln4752"> </a>
<a name="ln4753"> </a>
<a name="ln4754">status_t</a>
<a name="ln4755">vm_get_physical_page_current_cpu(phys_addr_t paddr, addr_t* _vaddr,</a>
<a name="ln4756">	void** _handle)</a>
<a name="ln4757">{</a>
<a name="ln4758">	return sPhysicalPageMapper-&gt;GetPageCurrentCPU(paddr, _vaddr, _handle);</a>
<a name="ln4759">}</a>
<a name="ln4760"> </a>
<a name="ln4761">status_t</a>
<a name="ln4762">vm_put_physical_page_current_cpu(addr_t vaddr, void* handle)</a>
<a name="ln4763">{</a>
<a name="ln4764">	return sPhysicalPageMapper-&gt;PutPageCurrentCPU(vaddr, handle);</a>
<a name="ln4765">}</a>
<a name="ln4766"> </a>
<a name="ln4767"> </a>
<a name="ln4768">status_t</a>
<a name="ln4769">vm_get_physical_page_debug(phys_addr_t paddr, addr_t* _vaddr, void** _handle)</a>
<a name="ln4770">{</a>
<a name="ln4771">	return sPhysicalPageMapper-&gt;GetPageDebug(paddr, _vaddr, _handle);</a>
<a name="ln4772">}</a>
<a name="ln4773"> </a>
<a name="ln4774">status_t</a>
<a name="ln4775">vm_put_physical_page_debug(addr_t vaddr, void* handle)</a>
<a name="ln4776">{</a>
<a name="ln4777">	return sPhysicalPageMapper-&gt;PutPageDebug(vaddr, handle);</a>
<a name="ln4778">}</a>
<a name="ln4779"> </a>
<a name="ln4780"> </a>
<a name="ln4781">void</a>
<a name="ln4782">vm_get_info(system_info* info)</a>
<a name="ln4783">{</a>
<a name="ln4784">	swap_get_info(info);</a>
<a name="ln4785"> </a>
<a name="ln4786">	MutexLocker locker(sAvailableMemoryLock);</a>
<a name="ln4787">	info-&gt;needed_memory = sNeededMemory;</a>
<a name="ln4788">	info-&gt;free_memory = sAvailableMemory;</a>
<a name="ln4789">}</a>
<a name="ln4790"> </a>
<a name="ln4791"> </a>
<a name="ln4792">uint32</a>
<a name="ln4793">vm_num_page_faults(void)</a>
<a name="ln4794">{</a>
<a name="ln4795">	return sPageFaults;</a>
<a name="ln4796">}</a>
<a name="ln4797"> </a>
<a name="ln4798"> </a>
<a name="ln4799">off_t</a>
<a name="ln4800">vm_available_memory(void)</a>
<a name="ln4801">{</a>
<a name="ln4802">	MutexLocker locker(sAvailableMemoryLock);</a>
<a name="ln4803">	return sAvailableMemory;</a>
<a name="ln4804">}</a>
<a name="ln4805"> </a>
<a name="ln4806"> </a>
<a name="ln4807">off_t</a>
<a name="ln4808">vm_available_not_needed_memory(void)</a>
<a name="ln4809">{</a>
<a name="ln4810">	MutexLocker locker(sAvailableMemoryLock);</a>
<a name="ln4811">	return sAvailableMemory - sNeededMemory;</a>
<a name="ln4812">}</a>
<a name="ln4813"> </a>
<a name="ln4814"> </a>
<a name="ln4815">/*!	Like vm_available_not_needed_memory(), but only for use in the kernel</a>
<a name="ln4816">	debugger.</a>
<a name="ln4817">*/</a>
<a name="ln4818">off_t</a>
<a name="ln4819">vm_available_not_needed_memory_debug(void)</a>
<a name="ln4820">{</a>
<a name="ln4821">	return sAvailableMemory - sNeededMemory;</a>
<a name="ln4822">}</a>
<a name="ln4823"> </a>
<a name="ln4824"> </a>
<a name="ln4825">size_t</a>
<a name="ln4826">vm_kernel_address_space_left(void)</a>
<a name="ln4827">{</a>
<a name="ln4828">	return VMAddressSpace::Kernel()-&gt;FreeSpace();</a>
<a name="ln4829">}</a>
<a name="ln4830"> </a>
<a name="ln4831"> </a>
<a name="ln4832">void</a>
<a name="ln4833">vm_unreserve_memory(size_t amount)</a>
<a name="ln4834">{</a>
<a name="ln4835">	mutex_lock(&amp;sAvailableMemoryLock);</a>
<a name="ln4836"> </a>
<a name="ln4837">	sAvailableMemory += amount;</a>
<a name="ln4838"> </a>
<a name="ln4839">	mutex_unlock(&amp;sAvailableMemoryLock);</a>
<a name="ln4840">}</a>
<a name="ln4841"> </a>
<a name="ln4842"> </a>
<a name="ln4843">status_t</a>
<a name="ln4844">vm_try_reserve_memory(size_t amount, int priority, bigtime_t timeout)</a>
<a name="ln4845">{</a>
<a name="ln4846">	size_t reserve = kMemoryReserveForPriority[priority];</a>
<a name="ln4847"> </a>
<a name="ln4848">	MutexLocker locker(sAvailableMemoryLock);</a>
<a name="ln4849"> </a>
<a name="ln4850">	//dprintf(&quot;try to reserve %lu bytes, %Lu left\n&quot;, amount, sAvailableMemory);</a>
<a name="ln4851"> </a>
<a name="ln4852">	if (sAvailableMemory &gt;= (off_t)(amount + reserve)) {</a>
<a name="ln4853">		sAvailableMemory -= amount;</a>
<a name="ln4854">		return B_OK;</a>
<a name="ln4855">	}</a>
<a name="ln4856"> </a>
<a name="ln4857">	if (timeout &lt;= 0)</a>
<a name="ln4858">		return B_NO_MEMORY;</a>
<a name="ln4859"> </a>
<a name="ln4860">	// turn timeout into an absolute timeout</a>
<a name="ln4861">	timeout += system_time();</a>
<a name="ln4862"> </a>
<a name="ln4863">	// loop until we've got the memory or the timeout occurs</a>
<a name="ln4864">	do {</a>
<a name="ln4865">		sNeededMemory += amount;</a>
<a name="ln4866"> </a>
<a name="ln4867">		// call the low resource manager</a>
<a name="ln4868">		locker.Unlock();</a>
<a name="ln4869">		low_resource(B_KERNEL_RESOURCE_MEMORY, sNeededMemory - sAvailableMemory,</a>
<a name="ln4870">			B_ABSOLUTE_TIMEOUT, timeout);</a>
<a name="ln4871">		locker.Lock();</a>
<a name="ln4872"> </a>
<a name="ln4873">		sNeededMemory -= amount;</a>
<a name="ln4874"> </a>
<a name="ln4875">		if (sAvailableMemory &gt;= (off_t)(amount + reserve)) {</a>
<a name="ln4876">			sAvailableMemory -= amount;</a>
<a name="ln4877">			return B_OK;</a>
<a name="ln4878">		}</a>
<a name="ln4879">	} while (timeout &gt; system_time());</a>
<a name="ln4880"> </a>
<a name="ln4881">	return B_NO_MEMORY;</a>
<a name="ln4882">}</a>
<a name="ln4883"> </a>
<a name="ln4884"> </a>
<a name="ln4885">status_t</a>
<a name="ln4886">vm_set_area_memory_type(area_id id, phys_addr_t physicalBase, uint32 type)</a>
<a name="ln4887">{</a>
<a name="ln4888">	// NOTE: The caller is responsible for synchronizing calls to this function!</a>
<a name="ln4889"> </a>
<a name="ln4890">	AddressSpaceReadLocker locker;</a>
<a name="ln4891">	VMArea* area;</a>
<a name="ln4892">	status_t status = locker.SetFromArea(id, area);</a>
<a name="ln4893">	if (status != B_OK)</a>
<a name="ln4894">		return status;</a>
<a name="ln4895"> </a>
<a name="ln4896">	// nothing to do, if the type doesn't change</a>
<a name="ln4897">	uint32 oldType = area-&gt;MemoryType();</a>
<a name="ln4898">	if (type == oldType)</a>
<a name="ln4899">		return B_OK;</a>
<a name="ln4900"> </a>
<a name="ln4901">	// set the memory type of the area and the mapped pages</a>
<a name="ln4902">	VMTranslationMap* map = area-&gt;address_space-&gt;TranslationMap();</a>
<a name="ln4903">	map-&gt;Lock();</a>
<a name="ln4904">	area-&gt;SetMemoryType(type);</a>
<a name="ln4905">	map-&gt;ProtectArea(area, area-&gt;protection);</a>
<a name="ln4906">	map-&gt;Unlock();</a>
<a name="ln4907"> </a>
<a name="ln4908">	// set the physical memory type</a>
<a name="ln4909">	status_t error = arch_vm_set_memory_type(area, physicalBase, type);</a>
<a name="ln4910">	if (error != B_OK) {</a>
<a name="ln4911">		// reset the memory type of the area and the mapped pages</a>
<a name="ln4912">		map-&gt;Lock();</a>
<a name="ln4913">		area-&gt;SetMemoryType(oldType);</a>
<a name="ln4914">		map-&gt;ProtectArea(area, area-&gt;protection);</a>
<a name="ln4915">		map-&gt;Unlock();</a>
<a name="ln4916">		return error;</a>
<a name="ln4917">	}</a>
<a name="ln4918"> </a>
<a name="ln4919">	return B_OK;</a>
<a name="ln4920"> </a>
<a name="ln4921">}</a>
<a name="ln4922"> </a>
<a name="ln4923"> </a>
<a name="ln4924">/*!	This function enforces some protection properties:</a>
<a name="ln4925">	 - kernel areas must be W^X (after kernel startup)</a>
<a name="ln4926">	 - if B_WRITE_AREA is set, B_KERNEL_WRITE_AREA is set as well</a>
<a name="ln4927">	 - if only B_READ_AREA has been set, B_KERNEL_READ_AREA is also set</a>
<a name="ln4928">	 - if no protection is specified, it defaults to B_KERNEL_READ_AREA</a>
<a name="ln4929">	   and B_KERNEL_WRITE_AREA.</a>
<a name="ln4930">*/</a>
<a name="ln4931">static void</a>
<a name="ln4932">fix_protection(uint32* protection)</a>
<a name="ln4933">{</a>
<a name="ln4934">	if ((*protection &amp; B_KERNEL_EXECUTE_AREA) != 0</a>
<a name="ln4935">		&amp;&amp; ((*protection &amp; B_KERNEL_WRITE_AREA) != 0</a>
<a name="ln4936">			|| (*protection &amp; B_WRITE_AREA) != 0)</a>
<a name="ln4937">		&amp;&amp; !gKernelStartup)</a>
<a name="ln4938">		panic(&quot;kernel areas cannot be both writable and executable!&quot;);</a>
<a name="ln4939"> </a>
<a name="ln4940">	if ((*protection &amp; B_KERNEL_PROTECTION) == 0) {</a>
<a name="ln4941">		if ((*protection &amp; B_USER_PROTECTION) == 0</a>
<a name="ln4942">			|| (*protection &amp; B_WRITE_AREA) != 0)</a>
<a name="ln4943">			*protection |= B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA;</a>
<a name="ln4944">		else</a>
<a name="ln4945">			*protection |= B_KERNEL_READ_AREA;</a>
<a name="ln4946">	}</a>
<a name="ln4947">}</a>
<a name="ln4948"> </a>
<a name="ln4949"> </a>
<a name="ln4950">static void</a>
<a name="ln4951">fill_area_info(struct VMArea* area, area_info* info, size_t size)</a>
<a name="ln4952">{</a>
<a name="ln4953">	strlcpy(info-&gt;name, area-&gt;name, B_OS_NAME_LENGTH);</a>
<a name="ln4954">	info-&gt;area = area-&gt;id;</a>
<a name="ln4955">	info-&gt;address = (void*)area-&gt;Base();</a>
<a name="ln4956">	info-&gt;size = area-&gt;Size();</a>
<a name="ln4957">	info-&gt;protection = area-&gt;protection;</a>
<a name="ln4958">	info-&gt;lock = B_FULL_LOCK;</a>
<a name="ln4959">	info-&gt;team = area-&gt;address_space-&gt;ID();</a>
<a name="ln4960">	info-&gt;copy_count = 0;</a>
<a name="ln4961">	info-&gt;in_count = 0;</a>
<a name="ln4962">	info-&gt;out_count = 0;</a>
<a name="ln4963">		// TODO: retrieve real values here!</a>
<a name="ln4964"> </a>
<a name="ln4965">	VMCache* cache = vm_area_get_locked_cache(area);</a>
<a name="ln4966"> </a>
<a name="ln4967">	// Note, this is a simplification; the cache could be larger than this area</a>
<a name="ln4968">	info-&gt;ram_size = cache-&gt;page_count * B_PAGE_SIZE;</a>
<a name="ln4969"> </a>
<a name="ln4970">	vm_area_put_locked_cache(cache);</a>
<a name="ln4971">}</a>
<a name="ln4972"> </a>
<a name="ln4973"> </a>
<a name="ln4974">static status_t</a>
<a name="ln4975">vm_resize_area(area_id areaID, size_t newSize, bool kernel)</a>
<a name="ln4976">{</a>
<a name="ln4977">	// is newSize a multiple of B_PAGE_SIZE?</a>
<a name="ln4978">	if (newSize &amp; (B_PAGE_SIZE - 1))</a>
<a name="ln4979">		return B_BAD_VALUE;</a>
<a name="ln4980"> </a>
<a name="ln4981">	// lock all affected address spaces and the cache</a>
<a name="ln4982">	VMArea* area;</a>
<a name="ln4983">	VMCache* cache;</a>
<a name="ln4984"> </a>
<a name="ln4985">	MultiAddressSpaceLocker locker;</a>
<a name="ln4986">	AreaCacheLocker cacheLocker;</a>
<a name="ln4987"> </a>
<a name="ln4988">	status_t status;</a>
<a name="ln4989">	size_t oldSize;</a>
<a name="ln4990">	bool anyKernelArea;</a>
<a name="ln4991">	bool restart;</a>
<a name="ln4992"> </a>
<a name="ln4993">	do {</a>
<a name="ln4994">		anyKernelArea = false;</a>
<a name="ln4995">		restart = false;</a>
<a name="ln4996"> </a>
<a name="ln4997">		locker.Unset();</a>
<a name="ln4998">		status = locker.AddAreaCacheAndLock(areaID, true, true, area, &amp;cache);</a>
<a name="ln4999">		if (status != B_OK)</a>
<a name="ln5000">			return status;</a>
<a name="ln5001">		cacheLocker.SetTo(cache, true);	// already locked</a>
<a name="ln5002"> </a>
<a name="ln5003">		// enforce restrictions</a>
<a name="ln5004">		if (!kernel &amp;&amp; area-&gt;address_space == VMAddressSpace::Kernel()) {</a>
<a name="ln5005">			dprintf(&quot;vm_resize_area: team %&quot; B_PRId32 &quot; tried to &quot;</a>
<a name="ln5006">				&quot;resize kernel area %&quot; B_PRId32 &quot; (%s)\n&quot;,</a>
<a name="ln5007">				team_get_current_team_id(), areaID, area-&gt;name);</a>
<a name="ln5008">			return B_NOT_ALLOWED;</a>
<a name="ln5009">		}</a>
<a name="ln5010">		// TODO: Enforce all restrictions (team, etc.)!</a>
<a name="ln5011"> </a>
<a name="ln5012">		oldSize = area-&gt;Size();</a>
<a name="ln5013">		if (newSize == oldSize)</a>
<a name="ln5014">			return B_OK;</a>
<a name="ln5015"> </a>
<a name="ln5016">		if (cache-&gt;type != CACHE_TYPE_RAM)</a>
<a name="ln5017">			return B_NOT_ALLOWED;</a>
<a name="ln5018"> </a>
<a name="ln5019">		if (oldSize &lt; newSize) {</a>
<a name="ln5020">			// We need to check if all areas of this cache can be resized.</a>
<a name="ln5021">			for (VMArea* current = cache-&gt;areas; current != NULL;</a>
<a name="ln5022">					current = current-&gt;cache_next) {</a>
<a name="ln5023">				if (!current-&gt;address_space-&gt;CanResizeArea(current, newSize))</a>
<a name="ln5024">					return B_ERROR;</a>
<a name="ln5025">				anyKernelArea</a>
<a name="ln5026">					|= current-&gt;address_space == VMAddressSpace::Kernel();</a>
<a name="ln5027">			}</a>
<a name="ln5028">		} else {</a>
<a name="ln5029">			// We're shrinking the areas, so we must make sure the affected</a>
<a name="ln5030">			// ranges are not wired.</a>
<a name="ln5031">			for (VMArea* current = cache-&gt;areas; current != NULL;</a>
<a name="ln5032">					current = current-&gt;cache_next) {</a>
<a name="ln5033">				anyKernelArea</a>
<a name="ln5034">					|= current-&gt;address_space == VMAddressSpace::Kernel();</a>
<a name="ln5035"> </a>
<a name="ln5036">				if (wait_if_area_range_is_wired(current,</a>
<a name="ln5037">						current-&gt;Base() + newSize, oldSize - newSize, &amp;locker,</a>
<a name="ln5038">						&amp;cacheLocker)) {</a>
<a name="ln5039">					restart = true;</a>
<a name="ln5040">					break;</a>
<a name="ln5041">				}</a>
<a name="ln5042">			}</a>
<a name="ln5043">		}</a>
<a name="ln5044">	} while (restart);</a>
<a name="ln5045"> </a>
<a name="ln5046">	// Okay, looks good so far, so let's do it</a>
<a name="ln5047"> </a>
<a name="ln5048">	int priority = kernel &amp;&amp; anyKernelArea</a>
<a name="ln5049">		? VM_PRIORITY_SYSTEM : VM_PRIORITY_USER;</a>
<a name="ln5050">	uint32 allocationFlags = kernel &amp;&amp; anyKernelArea</a>
<a name="ln5051">		? HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE : 0;</a>
<a name="ln5052"> </a>
<a name="ln5053">	if (oldSize &lt; newSize) {</a>
<a name="ln5054">		// Growing the cache can fail, so we do it first.</a>
<a name="ln5055">		status = cache-&gt;Resize(cache-&gt;virtual_base + newSize, priority);</a>
<a name="ln5056">		if (status != B_OK)</a>
<a name="ln5057">			return status;</a>
<a name="ln5058">	}</a>
<a name="ln5059"> </a>
<a name="ln5060">	for (VMArea* current = cache-&gt;areas; current != NULL;</a>
<a name="ln5061">			current = current-&gt;cache_next) {</a>
<a name="ln5062">		status = current-&gt;address_space-&gt;ResizeArea(current, newSize,</a>
<a name="ln5063">			allocationFlags);</a>
<a name="ln5064">		if (status != B_OK)</a>
<a name="ln5065">			break;</a>
<a name="ln5066"> </a>
<a name="ln5067">		// We also need to unmap all pages beyond the new size, if the area has</a>
<a name="ln5068">		// shrunk</a>
<a name="ln5069">		if (newSize &lt; oldSize) {</a>
<a name="ln5070">			VMCacheChainLocker cacheChainLocker(cache);</a>
<a name="ln5071">			cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5072"> </a>
<a name="ln5073">			unmap_pages(current, current-&gt;Base() + newSize,</a>
<a name="ln5074">				oldSize - newSize);</a>
<a name="ln5075"> </a>
<a name="ln5076">			cacheChainLocker.Unlock(cache);</a>
<a name="ln5077">		}</a>
<a name="ln5078">	}</a>
<a name="ln5079"> </a>
<a name="ln5080">	if (status == B_OK) {</a>
<a name="ln5081">		// Shrink or grow individual page protections if in use.</a>
<a name="ln5082">		if (area-&gt;page_protections != NULL) {</a>
<a name="ln5083">			uint32 bytes = (newSize / B_PAGE_SIZE + 1) / 2;</a>
<a name="ln5084">			uint8* newProtections</a>
<a name="ln5085">				= (uint8*)realloc(area-&gt;page_protections, bytes);</a>
<a name="ln5086">			if (newProtections == NULL)</a>
<a name="ln5087">				status = B_NO_MEMORY;</a>
<a name="ln5088">			else {</a>
<a name="ln5089">				area-&gt;page_protections = newProtections;</a>
<a name="ln5090"> </a>
<a name="ln5091">				if (oldSize &lt; newSize) {</a>
<a name="ln5092">					// init the additional page protections to that of the area</a>
<a name="ln5093">					uint32 offset = (oldSize / B_PAGE_SIZE + 1) / 2;</a>
<a name="ln5094">					uint32 areaProtection = area-&gt;protection</a>
<a name="ln5095">						&amp; (B_READ_AREA | B_WRITE_AREA | B_EXECUTE_AREA);</a>
<a name="ln5096">					memset(area-&gt;page_protections + offset,</a>
<a name="ln5097">						areaProtection | (areaProtection &lt;&lt; 4), bytes - offset);</a>
<a name="ln5098">					if ((oldSize / B_PAGE_SIZE) % 2 != 0) {</a>
<a name="ln5099">						uint8&amp; entry = area-&gt;page_protections[offset - 1];</a>
<a name="ln5100">						entry = (entry &amp; 0x0f) | (areaProtection &lt;&lt; 4);</a>
<a name="ln5101">					}</a>
<a name="ln5102">				}</a>
<a name="ln5103">			}</a>
<a name="ln5104">		}</a>
<a name="ln5105">	}</a>
<a name="ln5106"> </a>
<a name="ln5107">	// shrinking the cache can't fail, so we do it now</a>
<a name="ln5108">	if (status == B_OK &amp;&amp; newSize &lt; oldSize)</a>
<a name="ln5109">		status = cache-&gt;Resize(cache-&gt;virtual_base + newSize, priority);</a>
<a name="ln5110"> </a>
<a name="ln5111">	if (status != B_OK) {</a>
<a name="ln5112">		// Something failed -- resize the areas back to their original size.</a>
<a name="ln5113">		// This can fail, too, in which case we're seriously screwed.</a>
<a name="ln5114">		for (VMArea* current = cache-&gt;areas; current != NULL;</a>
<a name="ln5115">				current = current-&gt;cache_next) {</a>
<a name="ln5116">			if (current-&gt;address_space-&gt;ResizeArea(current, oldSize,</a>
<a name="ln5117">					allocationFlags) != B_OK) {</a>
<a name="ln5118">				panic(&quot;vm_resize_area(): Failed and not being able to restore &quot;</a>
<a name="ln5119">					&quot;original state.&quot;);</a>
<a name="ln5120">			}</a>
<a name="ln5121">		}</a>
<a name="ln5122"> </a>
<a name="ln5123">		cache-&gt;Resize(cache-&gt;virtual_base + oldSize, priority);</a>
<a name="ln5124">	}</a>
<a name="ln5125"> </a>
<a name="ln5126">	// TODO: we must honour the lock restrictions of this area</a>
<a name="ln5127">	return status;</a>
<a name="ln5128">}</a>
<a name="ln5129"> </a>
<a name="ln5130"> </a>
<a name="ln5131">status_t</a>
<a name="ln5132">vm_memset_physical(phys_addr_t address, int value, phys_size_t length)</a>
<a name="ln5133">{</a>
<a name="ln5134">	return sPhysicalPageMapper-&gt;MemsetPhysical(address, value, length);</a>
<a name="ln5135">}</a>
<a name="ln5136"> </a>
<a name="ln5137"> </a>
<a name="ln5138">status_t</a>
<a name="ln5139">vm_memcpy_from_physical(void* to, phys_addr_t from, size_t length, bool user)</a>
<a name="ln5140">{</a>
<a name="ln5141">	return sPhysicalPageMapper-&gt;MemcpyFromPhysical(to, from, length, user);</a>
<a name="ln5142">}</a>
<a name="ln5143"> </a>
<a name="ln5144"> </a>
<a name="ln5145">status_t</a>
<a name="ln5146">vm_memcpy_to_physical(phys_addr_t to, const void* _from, size_t length,</a>
<a name="ln5147">	bool user)</a>
<a name="ln5148">{</a>
<a name="ln5149">	return sPhysicalPageMapper-&gt;MemcpyToPhysical(to, _from, length, user);</a>
<a name="ln5150">}</a>
<a name="ln5151"> </a>
<a name="ln5152"> </a>
<a name="ln5153">void</a>
<a name="ln5154">vm_memcpy_physical_page(phys_addr_t to, phys_addr_t from)</a>
<a name="ln5155">{</a>
<a name="ln5156">	return sPhysicalPageMapper-&gt;MemcpyPhysicalPage(to, from);</a>
<a name="ln5157">}</a>
<a name="ln5158"> </a>
<a name="ln5159"> </a>
<a name="ln5160">/*!	Copies a range of memory directly from/to a page that might not be mapped</a>
<a name="ln5161">	at the moment.</a>
<a name="ln5162"> </a>
<a name="ln5163">	For \a unsafeMemory the current mapping (if any is ignored). The function</a>
<a name="ln5164">	walks through the respective area's cache chain to find the physical page</a>
<a name="ln5165">	and copies from/to it directly.</a>
<a name="ln5166">	The memory range starting at \a unsafeMemory with a length of \a size bytes</a>
<a name="ln5167">	must not cross a page boundary.</a>
<a name="ln5168"> </a>
<a name="ln5169">	\param teamID The team ID identifying the address space \a unsafeMemory is</a>
<a name="ln5170">		to be interpreted in. Ignored, if \a unsafeMemory is a kernel address</a>
<a name="ln5171">		(the kernel address space is assumed in this case). If \c B_CURRENT_TEAM</a>
<a name="ln5172">		is passed, the address space of the thread returned by</a>
<a name="ln5173">		debug_get_debugged_thread() is used.</a>
<a name="ln5174">	\param unsafeMemory The start of the unsafe memory range to be copied</a>
<a name="ln5175">		from/to.</a>
<a name="ln5176">	\param buffer A safely accessible kernel buffer to be copied from/to.</a>
<a name="ln5177">	\param size The number of bytes to be copied.</a>
<a name="ln5178">	\param copyToUnsafe If \c true, memory is copied from \a buffer to</a>
<a name="ln5179">		\a unsafeMemory, the other way around otherwise.</a>
<a name="ln5180">*/</a>
<a name="ln5181">status_t</a>
<a name="ln5182">vm_debug_copy_page_memory(team_id teamID, void* unsafeMemory, void* buffer,</a>
<a name="ln5183">	size_t size, bool copyToUnsafe)</a>
<a name="ln5184">{</a>
<a name="ln5185">	if (size &gt; B_PAGE_SIZE || ROUNDDOWN((addr_t)unsafeMemory, B_PAGE_SIZE)</a>
<a name="ln5186">			!= ROUNDDOWN((addr_t)unsafeMemory + size - 1, B_PAGE_SIZE)) {</a>
<a name="ln5187">		return B_BAD_VALUE;</a>
<a name="ln5188">	}</a>
<a name="ln5189"> </a>
<a name="ln5190">	// get the address space for the debugged thread</a>
<a name="ln5191">	VMAddressSpace* addressSpace;</a>
<a name="ln5192">	if (IS_KERNEL_ADDRESS(unsafeMemory)) {</a>
<a name="ln5193">		addressSpace = VMAddressSpace::Kernel();</a>
<a name="ln5194">	} else if (teamID == B_CURRENT_TEAM) {</a>
<a name="ln5195">		Thread* thread = debug_get_debugged_thread();</a>
<a name="ln5196">		if (thread == NULL || thread-&gt;team == NULL)</a>
<a name="ln5197">			return B_BAD_ADDRESS;</a>
<a name="ln5198"> </a>
<a name="ln5199">		addressSpace = thread-&gt;team-&gt;address_space;</a>
<a name="ln5200">	} else</a>
<a name="ln5201">		addressSpace = VMAddressSpace::DebugGet(teamID);</a>
<a name="ln5202"> </a>
<a name="ln5203">	if (addressSpace == NULL)</a>
<a name="ln5204">		return B_BAD_ADDRESS;</a>
<a name="ln5205"> </a>
<a name="ln5206">	// get the area</a>
<a name="ln5207">	VMArea* area = addressSpace-&gt;LookupArea((addr_t)unsafeMemory);</a>
<a name="ln5208">	if (area == NULL)</a>
<a name="ln5209">		return B_BAD_ADDRESS;</a>
<a name="ln5210"> </a>
<a name="ln5211">	// search the page</a>
<a name="ln5212">	off_t cacheOffset = (addr_t)unsafeMemory - area-&gt;Base()</a>
<a name="ln5213">		+ area-&gt;cache_offset;</a>
<a name="ln5214">	VMCache* cache = area-&gt;cache;</a>
<a name="ln5215">	vm_page* page = NULL;</a>
<a name="ln5216">	while (cache != NULL) {</a>
<a name="ln5217">		page = cache-&gt;DebugLookupPage(cacheOffset);</a>
<a name="ln5218">		if (page != NULL)</a>
<a name="ln5219">			break;</a>
<a name="ln5220"> </a>
<a name="ln5221">		// Page not found in this cache -- if it is paged out, we must not try</a>
<a name="ln5222">		// to get it from lower caches.</a>
<a name="ln5223">		if (cache-&gt;DebugHasPage(cacheOffset))</a>
<a name="ln5224">			break;</a>
<a name="ln5225"> </a>
<a name="ln5226">		cache = cache-&gt;source;</a>
<a name="ln5227">	}</a>
<a name="ln5228"> </a>
<a name="ln5229">	if (page == NULL)</a>
<a name="ln5230">		return B_UNSUPPORTED;</a>
<a name="ln5231"> </a>
<a name="ln5232">	// copy from/to physical memory</a>
<a name="ln5233">	phys_addr_t physicalAddress = page-&gt;physical_page_number * B_PAGE_SIZE</a>
<a name="ln5234">		+ (addr_t)unsafeMemory % B_PAGE_SIZE;</a>
<a name="ln5235"> </a>
<a name="ln5236">	if (copyToUnsafe) {</a>
<a name="ln5237">		if (page-&gt;Cache() != area-&gt;cache)</a>
<a name="ln5238">			return B_UNSUPPORTED;</a>
<a name="ln5239"> </a>
<a name="ln5240">		return vm_memcpy_to_physical(physicalAddress, buffer, size, false);</a>
<a name="ln5241">	}</a>
<a name="ln5242"> </a>
<a name="ln5243">	return vm_memcpy_from_physical(buffer, physicalAddress, size, false);</a>
<a name="ln5244">}</a>
<a name="ln5245"> </a>
<a name="ln5246"> </a>
<a name="ln5247">//	#pragma mark - kernel public API</a>
<a name="ln5248"> </a>
<a name="ln5249"> </a>
<a name="ln5250">status_t</a>
<a name="ln5251">user_memcpy(void* to, const void* from, size_t size)</a>
<a name="ln5252">{</a>
<a name="ln5253">	// don't allow address overflows</a>
<a name="ln5254">	if ((addr_t)from + size &lt; (addr_t)from || (addr_t)to + size &lt; (addr_t)to)</a>
<a name="ln5255">		return B_BAD_ADDRESS;</a>
<a name="ln5256"> </a>
<a name="ln5257">	if (arch_cpu_user_memcpy(to, from, size) &lt; B_OK)</a>
<a name="ln5258">		return B_BAD_ADDRESS;</a>
<a name="ln5259"> </a>
<a name="ln5260">	return B_OK;</a>
<a name="ln5261">}</a>
<a name="ln5262"> </a>
<a name="ln5263"> </a>
<a name="ln5264">/*!	\brief Copies at most (\a size - 1) characters from the string in \a from to</a>
<a name="ln5265">	the string in \a to, NULL-terminating the result.</a>
<a name="ln5266"> </a>
<a name="ln5267">	\param to Pointer to the destination C-string.</a>
<a name="ln5268">	\param from Pointer to the source C-string.</a>
<a name="ln5269">	\param size Size in bytes of the string buffer pointed to by \a to.</a>
<a name="ln5270"> </a>
<a name="ln5271">	\return strlen(\a from).</a>
<a name="ln5272">*/</a>
<a name="ln5273">ssize_t</a>
<a name="ln5274">user_strlcpy(char* to, const char* from, size_t size)</a>
<a name="ln5275">{</a>
<a name="ln5276">	if (to == NULL &amp;&amp; size != 0)</a>
<a name="ln5277">		return B_BAD_VALUE;</a>
<a name="ln5278">	if (from == NULL)</a>
<a name="ln5279">		return B_BAD_ADDRESS;</a>
<a name="ln5280"> </a>
<a name="ln5281">	// limit size to avoid address overflows</a>
<a name="ln5282">	size_t maxSize = std::min((addr_t)size,</a>
<a name="ln5283">		~(addr_t)0 - std::max((addr_t)from, (addr_t)to) + 1);</a>
<a name="ln5284">		// NOTE: Since arch_cpu_user_strlcpy() determines the length of \a from,</a>
<a name="ln5285">		// the source address might still overflow.</a>
<a name="ln5286"> </a>
<a name="ln5287">	ssize_t result = arch_cpu_user_strlcpy(to, from, maxSize);</a>
<a name="ln5288"> </a>
<a name="ln5289">	// If we hit the address overflow boundary, fail.</a>
<a name="ln5290">	if (result &lt; 0 || (result &gt;= 0 &amp;&amp; (size_t)result &gt;= maxSize</a>
<a name="ln5291">			&amp;&amp; maxSize &lt; size)) {</a>
<a name="ln5292">		return B_BAD_ADDRESS;</a>
<a name="ln5293">	}</a>
<a name="ln5294"> </a>
<a name="ln5295">	return result;</a>
<a name="ln5296">}</a>
<a name="ln5297"> </a>
<a name="ln5298"> </a>
<a name="ln5299">status_t</a>
<a name="ln5300">user_memset(void* s, char c, size_t count)</a>
<a name="ln5301">{</a>
<a name="ln5302">	// don't allow address overflows</a>
<a name="ln5303">	if ((addr_t)s + count &lt; (addr_t)s)</a>
<a name="ln5304">		return B_BAD_ADDRESS;</a>
<a name="ln5305">	if (arch_cpu_user_memset(s, c, count) &lt; B_OK)</a>
<a name="ln5306">		return B_BAD_ADDRESS;</a>
<a name="ln5307"> </a>
<a name="ln5308">	return B_OK;</a>
<a name="ln5309">}</a>
<a name="ln5310"> </a>
<a name="ln5311"> </a>
<a name="ln5312">/*!	Wires a single page at the given address.</a>
<a name="ln5313"> </a>
<a name="ln5314">	\param team The team whose address space the address belongs to. Supports</a>
<a name="ln5315">		also \c B_CURRENT_TEAM. If the given address is a kernel address, the</a>
<a name="ln5316">		parameter is ignored.</a>
<a name="ln5317">	\param address address The virtual address to wire down. Does not need to</a>
<a name="ln5318">		be page aligned.</a>
<a name="ln5319">	\param writable If \c true the page shall be writable.</a>
<a name="ln5320">	\param info On success the info is filled in, among other things</a>
<a name="ln5321">		containing the physical address the given virtual one translates to.</a>
<a name="ln5322">	\return \c B_OK, when the page could be wired, another error code otherwise.</a>
<a name="ln5323">*/</a>
<a name="ln5324">status_t</a>
<a name="ln5325">vm_wire_page(team_id team, addr_t address, bool writable,</a>
<a name="ln5326">	VMPageWiringInfo* info)</a>
<a name="ln5327">{</a>
<a name="ln5328">	addr_t pageAddress = ROUNDDOWN((addr_t)address, B_PAGE_SIZE);</a>
<a name="ln5329">	info-&gt;range.SetTo(pageAddress, B_PAGE_SIZE, writable, false);</a>
<a name="ln5330"> </a>
<a name="ln5331">	// compute the page protection that is required</a>
<a name="ln5332">	bool isUser = IS_USER_ADDRESS(address);</a>
<a name="ln5333">	uint32 requiredProtection = PAGE_PRESENT</a>
<a name="ln5334">		| B_KERNEL_READ_AREA | (isUser ? B_READ_AREA : 0);</a>
<a name="ln5335">	if (writable)</a>
<a name="ln5336">		requiredProtection |= B_KERNEL_WRITE_AREA | (isUser ? B_WRITE_AREA : 0);</a>
<a name="ln5337"> </a>
<a name="ln5338">	// get and read lock the address space</a>
<a name="ln5339">	VMAddressSpace* addressSpace = NULL;</a>
<a name="ln5340">	if (isUser) {</a>
<a name="ln5341">		if (team == B_CURRENT_TEAM)</a>
<a name="ln5342">			addressSpace = VMAddressSpace::GetCurrent();</a>
<a name="ln5343">		else</a>
<a name="ln5344">			addressSpace = VMAddressSpace::Get(team);</a>
<a name="ln5345">	} else</a>
<a name="ln5346">		addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln5347">	if (addressSpace == NULL)</a>
<a name="ln5348">		return B_ERROR;</a>
<a name="ln5349"> </a>
<a name="ln5350">	AddressSpaceReadLocker addressSpaceLocker(addressSpace, true);</a>
<a name="ln5351"> </a>
<a name="ln5352">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln5353">	status_t error = B_OK;</a>
<a name="ln5354"> </a>
<a name="ln5355">	// get the area</a>
<a name="ln5356">	VMArea* area = addressSpace-&gt;LookupArea(pageAddress);</a>
<a name="ln5357">	if (area == NULL) {</a>
<a name="ln5358">		addressSpace-&gt;Put();</a>
<a name="ln5359">		return B_BAD_ADDRESS;</a>
<a name="ln5360">	}</a>
<a name="ln5361"> </a>
<a name="ln5362">	// Lock the area's top cache. This is a requirement for VMArea::Wire().</a>
<a name="ln5363">	VMCacheChainLocker cacheChainLocker(vm_area_get_locked_cache(area));</a>
<a name="ln5364"> </a>
<a name="ln5365">	// mark the area range wired</a>
<a name="ln5366">	area-&gt;Wire(&amp;info-&gt;range);</a>
<a name="ln5367"> </a>
<a name="ln5368">	// Lock the area's cache chain and the translation map. Needed to look</a>
<a name="ln5369">	// up the page and play with its wired count.</a>
<a name="ln5370">	cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5371">	map-&gt;Lock();</a>
<a name="ln5372"> </a>
<a name="ln5373">	phys_addr_t physicalAddress;</a>
<a name="ln5374">	uint32 flags;</a>
<a name="ln5375">	vm_page* page;</a>
<a name="ln5376">	if (map-&gt;Query(pageAddress, &amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln5377">		&amp;&amp; (flags &amp; requiredProtection) == requiredProtection</a>
<a name="ln5378">		&amp;&amp; (page = vm_lookup_page(physicalAddress / B_PAGE_SIZE))</a>
<a name="ln5379">			!= NULL) {</a>
<a name="ln5380">		// Already mapped with the correct permissions -- just increment</a>
<a name="ln5381">		// the page's wired count.</a>
<a name="ln5382">		increment_page_wired_count(page);</a>
<a name="ln5383"> </a>
<a name="ln5384">		map-&gt;Unlock();</a>
<a name="ln5385">		cacheChainLocker.Unlock();</a>
<a name="ln5386">		addressSpaceLocker.Unlock();</a>
<a name="ln5387">	} else {</a>
<a name="ln5388">		// Let vm_soft_fault() map the page for us, if possible. We need</a>
<a name="ln5389">		// to fully unlock to avoid deadlocks. Since we have already</a>
<a name="ln5390">		// wired the area itself, nothing disturbing will happen with it</a>
<a name="ln5391">		// in the meantime.</a>
<a name="ln5392">		map-&gt;Unlock();</a>
<a name="ln5393">		cacheChainLocker.Unlock();</a>
<a name="ln5394">		addressSpaceLocker.Unlock();</a>
<a name="ln5395"> </a>
<a name="ln5396">		error = vm_soft_fault(addressSpace, pageAddress, writable, false,</a>
<a name="ln5397">			isUser, &amp;page);</a>
<a name="ln5398"> </a>
<a name="ln5399">		if (error != B_OK) {</a>
<a name="ln5400">			// The page could not be mapped -- clean up.</a>
<a name="ln5401">			VMCache* cache = vm_area_get_locked_cache(area);</a>
<a name="ln5402">			area-&gt;Unwire(&amp;info-&gt;range);</a>
<a name="ln5403">			cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln5404">			addressSpace-&gt;Put();</a>
<a name="ln5405">			return error;</a>
<a name="ln5406">		}</a>
<a name="ln5407">	}</a>
<a name="ln5408"> </a>
<a name="ln5409">	info-&gt;physicalAddress</a>
<a name="ln5410">		= (phys_addr_t)page-&gt;physical_page_number * B_PAGE_SIZE</a>
<a name="ln5411">			+ address % B_PAGE_SIZE;</a>
<a name="ln5412">	info-&gt;page = page;</a>
<a name="ln5413"> </a>
<a name="ln5414">	return B_OK;</a>
<a name="ln5415">}</a>
<a name="ln5416"> </a>
<a name="ln5417"> </a>
<a name="ln5418">/*!	Unwires a single page previously wired via vm_wire_page().</a>
<a name="ln5419"> </a>
<a name="ln5420">	\param info The same object passed to vm_wire_page() before.</a>
<a name="ln5421">*/</a>
<a name="ln5422">void</a>
<a name="ln5423">vm_unwire_page(VMPageWiringInfo* info)</a>
<a name="ln5424">{</a>
<a name="ln5425">	// lock the address space</a>
<a name="ln5426">	VMArea* area = info-&gt;range.area;</a>
<a name="ln5427">	AddressSpaceReadLocker addressSpaceLocker(area-&gt;address_space, false);</a>
<a name="ln5428">		// takes over our reference</a>
<a name="ln5429"> </a>
<a name="ln5430">	// lock the top cache</a>
<a name="ln5431">	VMCache* cache = vm_area_get_locked_cache(area);</a>
<a name="ln5432">	VMCacheChainLocker cacheChainLocker(cache);</a>
<a name="ln5433"> </a>
<a name="ln5434">	if (info-&gt;page-&gt;Cache() != cache) {</a>
<a name="ln5435">		// The page is not in the top cache, so we lock the whole cache chain</a>
<a name="ln5436">		// before touching the page's wired count.</a>
<a name="ln5437">		cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5438">	}</a>
<a name="ln5439"> </a>
<a name="ln5440">	decrement_page_wired_count(info-&gt;page);</a>
<a name="ln5441"> </a>
<a name="ln5442">	// remove the wired range from the range</a>
<a name="ln5443">	area-&gt;Unwire(&amp;info-&gt;range);</a>
<a name="ln5444"> </a>
<a name="ln5445">	cacheChainLocker.Unlock();</a>
<a name="ln5446">}</a>
<a name="ln5447"> </a>
<a name="ln5448"> </a>
<a name="ln5449">/*!	Wires down the given address range in the specified team's address space.</a>
<a name="ln5450"> </a>
<a name="ln5451">	If successful the function</a>
<a name="ln5452">	- acquires a reference to the specified team's address space,</a>
<a name="ln5453">	- adds respective wired ranges to all areas that intersect with the given</a>
<a name="ln5454">	  address range,</a>
<a name="ln5455">	- makes sure all pages in the given address range are mapped with the</a>
<a name="ln5456">	  requested access permissions and increments their wired count.</a>
<a name="ln5457"> </a>
<a name="ln5458">	It fails, when \a team doesn't specify a valid address space, when any part</a>
<a name="ln5459">	of the specified address range is not covered by areas, when the concerned</a>
<a name="ln5460">	areas don't allow mapping with the requested permissions, or when mapping</a>
<a name="ln5461">	failed for another reason.</a>
<a name="ln5462"> </a>
<a name="ln5463">	When successful the call must be balanced by a unlock_memory_etc() call with</a>
<a name="ln5464">	the exact same parameters.</a>
<a name="ln5465"> </a>
<a name="ln5466">	\param team Identifies the address (via team ID). \c B_CURRENT_TEAM is</a>
<a name="ln5467">		supported.</a>
<a name="ln5468">	\param address The start of the address range to be wired.</a>
<a name="ln5469">	\param numBytes The size of the address range to be wired.</a>
<a name="ln5470">	\param flags Flags. Currently only \c B_READ_DEVICE is defined, which</a>
<a name="ln5471">		requests that the range must be wired writable (&quot;read from device</a>
<a name="ln5472">		into memory&quot;).</a>
<a name="ln5473">	\return \c B_OK on success, another error code otherwise.</a>
<a name="ln5474">*/</a>
<a name="ln5475">status_t</a>
<a name="ln5476">lock_memory_etc(team_id team, void* address, size_t numBytes, uint32 flags)</a>
<a name="ln5477">{</a>
<a name="ln5478">	addr_t lockBaseAddress = ROUNDDOWN((addr_t)address, B_PAGE_SIZE);</a>
<a name="ln5479">	addr_t lockEndAddress = ROUNDUP((addr_t)address + numBytes, B_PAGE_SIZE);</a>
<a name="ln5480"> </a>
<a name="ln5481">	// compute the page protection that is required</a>
<a name="ln5482">	bool isUser = IS_USER_ADDRESS(address);</a>
<a name="ln5483">	bool writable = (flags &amp; B_READ_DEVICE) == 0;</a>
<a name="ln5484">	uint32 requiredProtection = PAGE_PRESENT</a>
<a name="ln5485">		| B_KERNEL_READ_AREA | (isUser ? B_READ_AREA : 0);</a>
<a name="ln5486">	if (writable)</a>
<a name="ln5487">		requiredProtection |= B_KERNEL_WRITE_AREA | (isUser ? B_WRITE_AREA : 0);</a>
<a name="ln5488"> </a>
<a name="ln5489">	uint32 mallocFlags = isUser</a>
<a name="ln5490">		? 0 : HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE;</a>
<a name="ln5491"> </a>
<a name="ln5492">	// get and read lock the address space</a>
<a name="ln5493">	VMAddressSpace* addressSpace = NULL;</a>
<a name="ln5494">	if (isUser) {</a>
<a name="ln5495">		if (team == B_CURRENT_TEAM)</a>
<a name="ln5496">			addressSpace = VMAddressSpace::GetCurrent();</a>
<a name="ln5497">		else</a>
<a name="ln5498">			addressSpace = VMAddressSpace::Get(team);</a>
<a name="ln5499">	} else</a>
<a name="ln5500">		addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln5501">	if (addressSpace == NULL)</a>
<a name="ln5502">		return B_ERROR;</a>
<a name="ln5503"> </a>
<a name="ln5504">	AddressSpaceReadLocker addressSpaceLocker(addressSpace, true);</a>
<a name="ln5505">		// We get a new address space reference here. The one we got above will</a>
<a name="ln5506">		// be freed by unlock_memory_etc().</a>
<a name="ln5507"> </a>
<a name="ln5508">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln5509">	status_t error = B_OK;</a>
<a name="ln5510"> </a>
<a name="ln5511">	// iterate through all concerned areas</a>
<a name="ln5512">	addr_t nextAddress = lockBaseAddress;</a>
<a name="ln5513">	while (nextAddress != lockEndAddress) {</a>
<a name="ln5514">		// get the next area</a>
<a name="ln5515">		VMArea* area = addressSpace-&gt;LookupArea(nextAddress);</a>
<a name="ln5516">		if (area == NULL) {</a>
<a name="ln5517">			error = B_BAD_ADDRESS;</a>
<a name="ln5518">			break;</a>
<a name="ln5519">		}</a>
<a name="ln5520"> </a>
<a name="ln5521">		addr_t areaStart = nextAddress;</a>
<a name="ln5522">		addr_t areaEnd = std::min(lockEndAddress, area-&gt;Base() + area-&gt;Size());</a>
<a name="ln5523"> </a>
<a name="ln5524">		// allocate the wired range (do that before locking the cache to avoid</a>
<a name="ln5525">		// deadlocks)</a>
<a name="ln5526">		VMAreaWiredRange* range = new(malloc_flags(mallocFlags))</a>
<a name="ln5527">			VMAreaWiredRange(areaStart, areaEnd - areaStart, writable, true);</a>
<a name="ln5528">		if (range == NULL) {</a>
<a name="ln5529">			error = B_NO_MEMORY;</a>
<a name="ln5530">			break;</a>
<a name="ln5531">		}</a>
<a name="ln5532"> </a>
<a name="ln5533">		// Lock the area's top cache. This is a requirement for VMArea::Wire().</a>
<a name="ln5534">		VMCacheChainLocker cacheChainLocker(vm_area_get_locked_cache(area));</a>
<a name="ln5535"> </a>
<a name="ln5536">		// mark the area range wired</a>
<a name="ln5537">		area-&gt;Wire(range);</a>
<a name="ln5538"> </a>
<a name="ln5539">		// Depending on the area cache type and the wiring, we may not need to</a>
<a name="ln5540">		// look at the individual pages.</a>
<a name="ln5541">		if (area-&gt;cache_type == CACHE_TYPE_NULL</a>
<a name="ln5542">			|| area-&gt;cache_type == CACHE_TYPE_DEVICE</a>
<a name="ln5543">			|| area-&gt;wiring == B_FULL_LOCK</a>
<a name="ln5544">			|| area-&gt;wiring == B_CONTIGUOUS) {</a>
<a name="ln5545">			nextAddress = areaEnd;</a>
<a name="ln5546">			continue;</a>
<a name="ln5547">		}</a>
<a name="ln5548"> </a>
<a name="ln5549">		// Lock the area's cache chain and the translation map. Needed to look</a>
<a name="ln5550">		// up pages and play with their wired count.</a>
<a name="ln5551">		cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5552">		map-&gt;Lock();</a>
<a name="ln5553"> </a>
<a name="ln5554">		// iterate through the pages and wire them</a>
<a name="ln5555">		for (; nextAddress != areaEnd; nextAddress += B_PAGE_SIZE) {</a>
<a name="ln5556">			phys_addr_t physicalAddress;</a>
<a name="ln5557">			uint32 flags;</a>
<a name="ln5558"> </a>
<a name="ln5559">			vm_page* page;</a>
<a name="ln5560">			if (map-&gt;Query(nextAddress, &amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln5561">				&amp;&amp; (flags &amp; requiredProtection) == requiredProtection</a>
<a name="ln5562">				&amp;&amp; (page = vm_lookup_page(physicalAddress / B_PAGE_SIZE))</a>
<a name="ln5563">					!= NULL) {</a>
<a name="ln5564">				// Already mapped with the correct permissions -- just increment</a>
<a name="ln5565">				// the page's wired count.</a>
<a name="ln5566">				increment_page_wired_count(page);</a>
<a name="ln5567">			} else {</a>
<a name="ln5568">				// Let vm_soft_fault() map the page for us, if possible. We need</a>
<a name="ln5569">				// to fully unlock to avoid deadlocks. Since we have already</a>
<a name="ln5570">				// wired the area itself, nothing disturbing will happen with it</a>
<a name="ln5571">				// in the meantime.</a>
<a name="ln5572">				map-&gt;Unlock();</a>
<a name="ln5573">				cacheChainLocker.Unlock();</a>
<a name="ln5574">				addressSpaceLocker.Unlock();</a>
<a name="ln5575"> </a>
<a name="ln5576">				error = vm_soft_fault(addressSpace, nextAddress, writable,</a>
<a name="ln5577">					false, isUser, &amp;page);</a>
<a name="ln5578"> </a>
<a name="ln5579">				addressSpaceLocker.Lock();</a>
<a name="ln5580">				cacheChainLocker.SetTo(vm_area_get_locked_cache(area));</a>
<a name="ln5581">				cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5582">				map-&gt;Lock();</a>
<a name="ln5583">			}</a>
<a name="ln5584"> </a>
<a name="ln5585">			if (error != B_OK)</a>
<a name="ln5586">				break;</a>
<a name="ln5587">		}</a>
<a name="ln5588"> </a>
<a name="ln5589">		map-&gt;Unlock();</a>
<a name="ln5590"> </a>
<a name="ln5591">		if (error == B_OK) {</a>
<a name="ln5592">			cacheChainLocker.Unlock();</a>
<a name="ln5593">		} else {</a>
<a name="ln5594">			// An error occurred, so abort right here. If the current address</a>
<a name="ln5595">			// is the first in this area, unwire the area, since we won't get</a>
<a name="ln5596">			// to it when reverting what we've done so far.</a>
<a name="ln5597">			if (nextAddress == areaStart) {</a>
<a name="ln5598">				area-&gt;Unwire(range);</a>
<a name="ln5599">				cacheChainLocker.Unlock();</a>
<a name="ln5600">				range-&gt;~VMAreaWiredRange();</a>
<a name="ln5601">				free_etc(range, mallocFlags);</a>
<a name="ln5602">			} else</a>
<a name="ln5603">				cacheChainLocker.Unlock();</a>
<a name="ln5604"> </a>
<a name="ln5605">			break;</a>
<a name="ln5606">		}</a>
<a name="ln5607">	}</a>
<a name="ln5608"> </a>
<a name="ln5609">	if (error != B_OK) {</a>
<a name="ln5610">		// An error occurred, so unwire all that we've already wired. Note that</a>
<a name="ln5611">		// even if not a single page was wired, unlock_memory_etc() is called</a>
<a name="ln5612">		// to put the address space reference.</a>
<a name="ln5613">		addressSpaceLocker.Unlock();</a>
<a name="ln5614">		unlock_memory_etc(team, (void*)lockBaseAddress,</a>
<a name="ln5615">			nextAddress - lockBaseAddress, flags);</a>
<a name="ln5616">	}</a>
<a name="ln5617"> </a>
<a name="ln5618">	return error;</a>
<a name="ln5619">}</a>
<a name="ln5620"> </a>
<a name="ln5621"> </a>
<a name="ln5622">status_t</a>
<a name="ln5623">lock_memory(void* address, size_t numBytes, uint32 flags)</a>
<a name="ln5624">{</a>
<a name="ln5625">	return lock_memory_etc(B_CURRENT_TEAM, address, numBytes, flags);</a>
<a name="ln5626">}</a>
<a name="ln5627"> </a>
<a name="ln5628"> </a>
<a name="ln5629">/*!	Unwires an address range previously wired with lock_memory_etc().</a>
<a name="ln5630"> </a>
<a name="ln5631">	Note that a call to this function must balance a previous lock_memory_etc()</a>
<a name="ln5632">	call with exactly the same parameters.</a>
<a name="ln5633">*/</a>
<a name="ln5634">status_t</a>
<a name="ln5635">unlock_memory_etc(team_id team, void* address, size_t numBytes, uint32 flags)</a>
<a name="ln5636">{</a>
<a name="ln5637">	addr_t lockBaseAddress = ROUNDDOWN((addr_t)address, B_PAGE_SIZE);</a>
<a name="ln5638">	addr_t lockEndAddress = ROUNDUP((addr_t)address + numBytes, B_PAGE_SIZE);</a>
<a name="ln5639"> </a>
<a name="ln5640">	// compute the page protection that is required</a>
<a name="ln5641">	bool isUser = IS_USER_ADDRESS(address);</a>
<a name="ln5642">	bool writable = (flags &amp; B_READ_DEVICE) == 0;</a>
<a name="ln5643">	uint32 requiredProtection = PAGE_PRESENT</a>
<a name="ln5644">		| B_KERNEL_READ_AREA | (isUser ? B_READ_AREA : 0);</a>
<a name="ln5645">	if (writable)</a>
<a name="ln5646">		requiredProtection |= B_KERNEL_WRITE_AREA | (isUser ? B_WRITE_AREA : 0);</a>
<a name="ln5647"> </a>
<a name="ln5648">	uint32 mallocFlags = isUser</a>
<a name="ln5649">		? 0 : HEAP_DONT_WAIT_FOR_MEMORY | HEAP_DONT_LOCK_KERNEL_SPACE;</a>
<a name="ln5650"> </a>
<a name="ln5651">	// get and read lock the address space</a>
<a name="ln5652">	VMAddressSpace* addressSpace = NULL;</a>
<a name="ln5653">	if (isUser) {</a>
<a name="ln5654">		if (team == B_CURRENT_TEAM)</a>
<a name="ln5655">			addressSpace = VMAddressSpace::GetCurrent();</a>
<a name="ln5656">		else</a>
<a name="ln5657">			addressSpace = VMAddressSpace::Get(team);</a>
<a name="ln5658">	} else</a>
<a name="ln5659">		addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln5660">	if (addressSpace == NULL)</a>
<a name="ln5661">		return B_ERROR;</a>
<a name="ln5662"> </a>
<a name="ln5663">	AddressSpaceReadLocker addressSpaceLocker(addressSpace, false);</a>
<a name="ln5664">		// Take over the address space reference. We don't unlock until we're</a>
<a name="ln5665">		// done.</a>
<a name="ln5666"> </a>
<a name="ln5667">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln5668">	status_t error = B_OK;</a>
<a name="ln5669"> </a>
<a name="ln5670">	// iterate through all concerned areas</a>
<a name="ln5671">	addr_t nextAddress = lockBaseAddress;</a>
<a name="ln5672">	while (nextAddress != lockEndAddress) {</a>
<a name="ln5673">		// get the next area</a>
<a name="ln5674">		VMArea* area = addressSpace-&gt;LookupArea(nextAddress);</a>
<a name="ln5675">		if (area == NULL) {</a>
<a name="ln5676">			error = B_BAD_ADDRESS;</a>
<a name="ln5677">			break;</a>
<a name="ln5678">		}</a>
<a name="ln5679"> </a>
<a name="ln5680">		addr_t areaStart = nextAddress;</a>
<a name="ln5681">		addr_t areaEnd = std::min(lockEndAddress, area-&gt;Base() + area-&gt;Size());</a>
<a name="ln5682"> </a>
<a name="ln5683">		// Lock the area's top cache. This is a requirement for</a>
<a name="ln5684">		// VMArea::Unwire().</a>
<a name="ln5685">		VMCacheChainLocker cacheChainLocker(vm_area_get_locked_cache(area));</a>
<a name="ln5686"> </a>
<a name="ln5687">		// Depending on the area cache type and the wiring, we may not need to</a>
<a name="ln5688">		// look at the individual pages.</a>
<a name="ln5689">		if (area-&gt;cache_type == CACHE_TYPE_NULL</a>
<a name="ln5690">			|| area-&gt;cache_type == CACHE_TYPE_DEVICE</a>
<a name="ln5691">			|| area-&gt;wiring == B_FULL_LOCK</a>
<a name="ln5692">			|| area-&gt;wiring == B_CONTIGUOUS) {</a>
<a name="ln5693">			// unwire the range (to avoid deadlocks we delete the range after</a>
<a name="ln5694">			// unlocking the cache)</a>
<a name="ln5695">			nextAddress = areaEnd;</a>
<a name="ln5696">			VMAreaWiredRange* range = area-&gt;Unwire(areaStart,</a>
<a name="ln5697">				areaEnd - areaStart, writable);</a>
<a name="ln5698">			cacheChainLocker.Unlock();</a>
<a name="ln5699">			if (range != NULL) {</a>
<a name="ln5700">				range-&gt;~VMAreaWiredRange();</a>
<a name="ln5701">				free_etc(range, mallocFlags);</a>
<a name="ln5702">			}</a>
<a name="ln5703">			continue;</a>
<a name="ln5704">		}</a>
<a name="ln5705"> </a>
<a name="ln5706">		// Lock the area's cache chain and the translation map. Needed to look</a>
<a name="ln5707">		// up pages and play with their wired count.</a>
<a name="ln5708">		cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln5709">		map-&gt;Lock();</a>
<a name="ln5710"> </a>
<a name="ln5711">		// iterate through the pages and unwire them</a>
<a name="ln5712">		for (; nextAddress != areaEnd; nextAddress += B_PAGE_SIZE) {</a>
<a name="ln5713">			phys_addr_t physicalAddress;</a>
<a name="ln5714">			uint32 flags;</a>
<a name="ln5715"> </a>
<a name="ln5716">			vm_page* page;</a>
<a name="ln5717">			if (map-&gt;Query(nextAddress, &amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln5718">				&amp;&amp; (flags &amp; PAGE_PRESENT) != 0</a>
<a name="ln5719">				&amp;&amp; (page = vm_lookup_page(physicalAddress / B_PAGE_SIZE))</a>
<a name="ln5720">					!= NULL) {</a>
<a name="ln5721">				// Already mapped with the correct permissions -- just increment</a>
<a name="ln5722">				// the page's wired count.</a>
<a name="ln5723">				decrement_page_wired_count(page);</a>
<a name="ln5724">			} else {</a>
<a name="ln5725">				panic(&quot;unlock_memory_etc(): Failed to unwire page: address &quot;</a>
<a name="ln5726">					&quot;space %p, address: %#&quot; B_PRIxADDR, addressSpace,</a>
<a name="ln5727">					nextAddress);</a>
<a name="ln5728">				error = B_BAD_VALUE;</a>
<a name="ln5729">				break;</a>
<a name="ln5730">			}</a>
<a name="ln5731">		}</a>
<a name="ln5732"> </a>
<a name="ln5733">		map-&gt;Unlock();</a>
<a name="ln5734"> </a>
<a name="ln5735">		// All pages are unwired. Remove the area's wired range as well (to</a>
<a name="ln5736">		// avoid deadlocks we delete the range after unlocking the cache).</a>
<a name="ln5737">		VMAreaWiredRange* range = area-&gt;Unwire(areaStart,</a>
<a name="ln5738">			areaEnd - areaStart, writable);</a>
<a name="ln5739"> </a>
<a name="ln5740">		cacheChainLocker.Unlock();</a>
<a name="ln5741"> </a>
<a name="ln5742">		if (range != NULL) {</a>
<a name="ln5743">			range-&gt;~VMAreaWiredRange();</a>
<a name="ln5744">			free_etc(range, mallocFlags);</a>
<a name="ln5745">		}</a>
<a name="ln5746"> </a>
<a name="ln5747">		if (error != B_OK)</a>
<a name="ln5748">			break;</a>
<a name="ln5749">	}</a>
<a name="ln5750"> </a>
<a name="ln5751">	// get rid of the address space reference lock_memory_etc() acquired</a>
<a name="ln5752">	addressSpace-&gt;Put();</a>
<a name="ln5753"> </a>
<a name="ln5754">	return error;</a>
<a name="ln5755">}</a>
<a name="ln5756"> </a>
<a name="ln5757"> </a>
<a name="ln5758">status_t</a>
<a name="ln5759">unlock_memory(void* address, size_t numBytes, uint32 flags)</a>
<a name="ln5760">{</a>
<a name="ln5761">	return unlock_memory_etc(B_CURRENT_TEAM, address, numBytes, flags);</a>
<a name="ln5762">}</a>
<a name="ln5763"> </a>
<a name="ln5764"> </a>
<a name="ln5765">/*!	Similar to get_memory_map(), but also allows to specify the address space</a>
<a name="ln5766">	for the memory in question and has a saner semantics.</a>
<a name="ln5767">	Returns \c B_OK when the complete range could be translated or</a>
<a name="ln5768">	\c B_BUFFER_OVERFLOW, if the provided array wasn't big enough. In either</a>
<a name="ln5769">	case the actual number of entries is written to \c *_numEntries. Any other</a>
<a name="ln5770">	error case indicates complete failure; \c *_numEntries will be set to \c 0</a>
<a name="ln5771">	in this case.</a>
<a name="ln5772">*/</a>
<a name="ln5773">status_t</a>
<a name="ln5774">get_memory_map_etc(team_id team, const void* address, size_t numBytes,</a>
<a name="ln5775">	physical_entry* table, uint32* _numEntries)</a>
<a name="ln5776">{</a>
<a name="ln5777">	uint32 numEntries = *_numEntries;</a>
<a name="ln5778">	*_numEntries = 0;</a>
<a name="ln5779"> </a>
<a name="ln5780">	VMAddressSpace* addressSpace;</a>
<a name="ln5781">	addr_t virtualAddress = (addr_t)address;</a>
<a name="ln5782">	addr_t pageOffset = virtualAddress &amp; (B_PAGE_SIZE - 1);</a>
<a name="ln5783">	phys_addr_t physicalAddress;</a>
<a name="ln5784">	status_t status = B_OK;</a>
<a name="ln5785">	int32 index = -1;</a>
<a name="ln5786">	addr_t offset = 0;</a>
<a name="ln5787">	bool interrupts = are_interrupts_enabled();</a>
<a name="ln5788"> </a>
<a name="ln5789">	TRACE((&quot;get_memory_map_etc(%&quot; B_PRId32 &quot;, %p, %lu bytes, %&quot; B_PRIu32 &quot; &quot;</a>
<a name="ln5790">		&quot;entries)\n&quot;, team, address, numBytes, numEntries));</a>
<a name="ln5791"> </a>
<a name="ln5792">	if (numEntries == 0 || numBytes == 0)</a>
<a name="ln5793">		return B_BAD_VALUE;</a>
<a name="ln5794"> </a>
<a name="ln5795">	// in which address space is the address to be found?</a>
<a name="ln5796">	if (IS_USER_ADDRESS(virtualAddress)) {</a>
<a name="ln5797">		if (team == B_CURRENT_TEAM)</a>
<a name="ln5798">			addressSpace = VMAddressSpace::GetCurrent();</a>
<a name="ln5799">		else</a>
<a name="ln5800">			addressSpace = VMAddressSpace::Get(team);</a>
<a name="ln5801">	} else</a>
<a name="ln5802">		addressSpace = VMAddressSpace::GetKernel();</a>
<a name="ln5803"> </a>
<a name="ln5804">	if (addressSpace == NULL)</a>
<a name="ln5805">		return B_ERROR;</a>
<a name="ln5806"> </a>
<a name="ln5807">	VMTranslationMap* map = addressSpace-&gt;TranslationMap();</a>
<a name="ln5808"> </a>
<a name="ln5809">	if (interrupts)</a>
<a name="ln5810">		map-&gt;Lock();</a>
<a name="ln5811"> </a>
<a name="ln5812">	while (offset &lt; numBytes) {</a>
<a name="ln5813">		addr_t bytes = min_c(numBytes - offset, B_PAGE_SIZE);</a>
<a name="ln5814">		uint32 flags;</a>
<a name="ln5815"> </a>
<a name="ln5816">		if (interrupts) {</a>
<a name="ln5817">			status = map-&gt;Query((addr_t)address + offset, &amp;physicalAddress,</a>
<a name="ln5818">				&amp;flags);</a>
<a name="ln5819">		} else {</a>
<a name="ln5820">			status = map-&gt;QueryInterrupt((addr_t)address + offset,</a>
<a name="ln5821">				&amp;physicalAddress, &amp;flags);</a>
<a name="ln5822">		}</a>
<a name="ln5823">		if (status &lt; B_OK)</a>
<a name="ln5824">			break;</a>
<a name="ln5825">		if ((flags &amp; PAGE_PRESENT) == 0) {</a>
<a name="ln5826">			panic(&quot;get_memory_map() called on unmapped memory!&quot;);</a>
<a name="ln5827">			return B_BAD_ADDRESS;</a>
<a name="ln5828">		}</a>
<a name="ln5829"> </a>
<a name="ln5830">		if (index &lt; 0 &amp;&amp; pageOffset &gt; 0) {</a>
<a name="ln5831">			physicalAddress += pageOffset;</a>
<a name="ln5832">			if (bytes &gt; B_PAGE_SIZE - pageOffset)</a>
<a name="ln5833">				bytes = B_PAGE_SIZE - pageOffset;</a>
<a name="ln5834">		}</a>
<a name="ln5835"> </a>
<a name="ln5836">		// need to switch to the next physical_entry?</a>
<a name="ln5837">		if (index &lt; 0 || table[index].address</a>
<a name="ln5838">				!= physicalAddress - table[index].size) {</a>
<a name="ln5839">			if ((uint32)++index + 1 &gt; numEntries) {</a>
<a name="ln5840">				// table to small</a>
<a name="ln5841">				break;</a>
<a name="ln5842">			}</a>
<a name="ln5843">			table[index].address = physicalAddress;</a>
<a name="ln5844">			table[index].size = bytes;</a>
<a name="ln5845">		} else {</a>
<a name="ln5846">			// page does fit in current entry</a>
<a name="ln5847">			table[index].size += bytes;</a>
<a name="ln5848">		}</a>
<a name="ln5849"> </a>
<a name="ln5850">		offset += bytes;</a>
<a name="ln5851">	}</a>
<a name="ln5852"> </a>
<a name="ln5853">	if (interrupts)</a>
<a name="ln5854">		map-&gt;Unlock();</a>
<a name="ln5855"> </a>
<a name="ln5856">	if (status != B_OK)</a>
<a name="ln5857">		return status;</a>
<a name="ln5858"> </a>
<a name="ln5859">	if ((uint32)index + 1 &gt; numEntries) {</a>
<a name="ln5860">		*_numEntries = index;</a>
<a name="ln5861">		return B_BUFFER_OVERFLOW;</a>
<a name="ln5862">	}</a>
<a name="ln5863"> </a>
<a name="ln5864">	*_numEntries = index + 1;</a>
<a name="ln5865">	return B_OK;</a>
<a name="ln5866">}</a>
<a name="ln5867"> </a>
<a name="ln5868"> </a>
<a name="ln5869">/*!	According to the BeBook, this function should always succeed.</a>
<a name="ln5870">	This is no longer the case.</a>
<a name="ln5871">*/</a>
<a name="ln5872">extern &quot;C&quot; int32</a>
<a name="ln5873">__get_memory_map_haiku(const void* address, size_t numBytes,</a>
<a name="ln5874">	physical_entry* table, int32 numEntries)</a>
<a name="ln5875">{</a>
<a name="ln5876">	uint32 entriesRead = numEntries;</a>
<a name="ln5877">	status_t error = get_memory_map_etc(B_CURRENT_TEAM, address, numBytes,</a>
<a name="ln5878">		table, &amp;entriesRead);</a>
<a name="ln5879">	if (error != B_OK)</a>
<a name="ln5880">		return error;</a>
<a name="ln5881"> </a>
<a name="ln5882">	// close the entry list</a>
<a name="ln5883"> </a>
<a name="ln5884">	// if it's only one entry, we will silently accept the missing ending</a>
<a name="ln5885">	if (numEntries == 1)</a>
<a name="ln5886">		return B_OK;</a>
<a name="ln5887"> </a>
<a name="ln5888">	if (entriesRead + 1 &gt; (uint32)numEntries)</a>
<a name="ln5889">		return B_BUFFER_OVERFLOW;</a>
<a name="ln5890"> </a>
<a name="ln5891">	table[entriesRead].address = 0;</a>
<a name="ln5892">	table[entriesRead].size = 0;</a>
<a name="ln5893"> </a>
<a name="ln5894">	return B_OK;</a>
<a name="ln5895">}</a>
<a name="ln5896"> </a>
<a name="ln5897"> </a>
<a name="ln5898">area_id</a>
<a name="ln5899">area_for(void* address)</a>
<a name="ln5900">{</a>
<a name="ln5901">	return vm_area_for((addr_t)address, true);</a>
<a name="ln5902">}</a>
<a name="ln5903"> </a>
<a name="ln5904"> </a>
<a name="ln5905">area_id</a>
<a name="ln5906">find_area(const char* name)</a>
<a name="ln5907">{</a>
<a name="ln5908">	return VMAreaHash::Find(name);</a>
<a name="ln5909">}</a>
<a name="ln5910"> </a>
<a name="ln5911"> </a>
<a name="ln5912">status_t</a>
<a name="ln5913">_get_area_info(area_id id, area_info* info, size_t size)</a>
<a name="ln5914">{</a>
<a name="ln5915">	if (size != sizeof(area_info) || info == NULL)</a>
<a name="ln5916">		return B_BAD_VALUE;</a>
<a name="ln5917"> </a>
<a name="ln5918">	AddressSpaceReadLocker locker;</a>
<a name="ln5919">	VMArea* area;</a>
<a name="ln5920">	status_t status = locker.SetFromArea(id, area);</a>
<a name="ln5921">	if (status != B_OK)</a>
<a name="ln5922">		return status;</a>
<a name="ln5923"> </a>
<a name="ln5924">	fill_area_info(area, info, size);</a>
<a name="ln5925">	return B_OK;</a>
<a name="ln5926">}</a>
<a name="ln5927"> </a>
<a name="ln5928"> </a>
<a name="ln5929">status_t</a>
<a name="ln5930">_get_next_area_info(team_id team, ssize_t* cookie, area_info* info, size_t size)</a>
<a name="ln5931">{</a>
<a name="ln5932">	addr_t nextBase = *(addr_t*)cookie;</a>
<a name="ln5933"> </a>
<a name="ln5934">	// we're already through the list</a>
<a name="ln5935">	if (nextBase == (addr_t)-1)</a>
<a name="ln5936">		return B_ENTRY_NOT_FOUND;</a>
<a name="ln5937"> </a>
<a name="ln5938">	if (team == B_CURRENT_TEAM)</a>
<a name="ln5939">		team = team_get_current_team_id();</a>
<a name="ln5940"> </a>
<a name="ln5941">	AddressSpaceReadLocker locker(team);</a>
<a name="ln5942">	if (!locker.IsLocked())</a>
<a name="ln5943">		return B_BAD_TEAM_ID;</a>
<a name="ln5944"> </a>
<a name="ln5945">	VMArea* area;</a>
<a name="ln5946">	for (VMAddressSpace::AreaIterator it</a>
<a name="ln5947">				= locker.AddressSpace()-&gt;GetAreaIterator();</a>
<a name="ln5948">			(area = it.Next()) != NULL;) {</a>
<a name="ln5949">		if (area-&gt;Base() &gt; nextBase)</a>
<a name="ln5950">			break;</a>
<a name="ln5951">	}</a>
<a name="ln5952"> </a>
<a name="ln5953">	if (area == NULL) {</a>
<a name="ln5954">		nextBase = (addr_t)-1;</a>
<a name="ln5955">		return B_ENTRY_NOT_FOUND;</a>
<a name="ln5956">	}</a>
<a name="ln5957"> </a>
<a name="ln5958">	fill_area_info(area, info, size);</a>
<a name="ln5959">	*cookie = (ssize_t)(area-&gt;Base());</a>
<a name="ln5960"> </a>
<a name="ln5961">	return B_OK;</a>
<a name="ln5962">}</a>
<a name="ln5963"> </a>
<a name="ln5964"> </a>
<a name="ln5965">status_t</a>
<a name="ln5966">set_area_protection(area_id area, uint32 newProtection)</a>
<a name="ln5967">{</a>
<a name="ln5968">	return vm_set_area_protection(VMAddressSpace::KernelID(), area,</a>
<a name="ln5969">		newProtection, true);</a>
<a name="ln5970">}</a>
<a name="ln5971"> </a>
<a name="ln5972"> </a>
<a name="ln5973">status_t</a>
<a name="ln5974">resize_area(area_id areaID, size_t newSize)</a>
<a name="ln5975">{</a>
<a name="ln5976">	return vm_resize_area(areaID, newSize, true);</a>
<a name="ln5977">}</a>
<a name="ln5978"> </a>
<a name="ln5979"> </a>
<a name="ln5980">/*!	Transfers the specified area to a new team. The caller must be the owner</a>
<a name="ln5981">	of the area.</a>
<a name="ln5982">*/</a>
<a name="ln5983">area_id</a>
<a name="ln5984">transfer_area(area_id id, void** _address, uint32 addressSpec, team_id target,</a>
<a name="ln5985">	bool kernel)</a>
<a name="ln5986">{</a>
<a name="ln5987">	area_info info;</a>
<a name="ln5988">	status_t status = get_area_info(id, &amp;info);</a>
<a name="ln5989">	if (status != B_OK)</a>
<a name="ln5990">		return status;</a>
<a name="ln5991"> </a>
<a name="ln5992">	if (info.team != thread_get_current_thread()-&gt;team-&gt;id)</a>
<a name="ln5993">		return B_PERMISSION_DENIED;</a>
<a name="ln5994"> </a>
<a name="ln5995">	area_id clonedArea = vm_clone_area(target, info.name, _address,</a>
<a name="ln5996">		addressSpec, info.protection, REGION_NO_PRIVATE_MAP, id, kernel);</a>
<a name="ln5997">	if (clonedArea &lt; 0)</a>
<a name="ln5998">		return clonedArea;</a>
<a name="ln5999"> </a>
<a name="ln6000">	status = vm_delete_area(info.team, id, kernel);</a>
<a name="ln6001">	if (status != B_OK) {</a>
<a name="ln6002">		vm_delete_area(target, clonedArea, kernel);</a>
<a name="ln6003">		return status;</a>
<a name="ln6004">	}</a>
<a name="ln6005"> </a>
<a name="ln6006">	// TODO: The clonedArea is B_SHARED_AREA, which is not really desired.</a>
<a name="ln6007"> </a>
<a name="ln6008">	return clonedArea;</a>
<a name="ln6009">}</a>
<a name="ln6010"> </a>
<a name="ln6011"> </a>
<a name="ln6012">extern &quot;C&quot; area_id</a>
<a name="ln6013">__map_physical_memory_haiku(const char* name, phys_addr_t physicalAddress,</a>
<a name="ln6014">	size_t numBytes, uint32 addressSpec, uint32 protection,</a>
<a name="ln6015">	void** _virtualAddress)</a>
<a name="ln6016">{</a>
<a name="ln6017">	if (!arch_vm_supports_protection(protection))</a>
<a name="ln6018">		return B_NOT_SUPPORTED;</a>
<a name="ln6019"> </a>
<a name="ln6020">	fix_protection(&amp;protection);</a>
<a name="ln6021"> </a>
<a name="ln6022">	return vm_map_physical_memory(VMAddressSpace::KernelID(), name,</a>
<a name="ln6023">		_virtualAddress, addressSpec, numBytes, protection, physicalAddress,</a>
<a name="ln6024">		false);</a>
<a name="ln6025">}</a>
<a name="ln6026"> </a>
<a name="ln6027"> </a>
<a name="ln6028">area_id</a>
<a name="ln6029">clone_area(const char* name, void** _address, uint32 addressSpec,</a>
<a name="ln6030">	uint32 protection, area_id source)</a>
<a name="ln6031">{</a>
<a name="ln6032">	if ((protection &amp; B_KERNEL_PROTECTION) == 0)</a>
<a name="ln6033">		protection |= B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA;</a>
<a name="ln6034"> </a>
<a name="ln6035">	return vm_clone_area(VMAddressSpace::KernelID(), name, _address,</a>
<a name="ln6036">		addressSpec, protection, REGION_NO_PRIVATE_MAP, source, true);</a>
<a name="ln6037">}</a>
<a name="ln6038"> </a>
<a name="ln6039"> </a>
<a name="ln6040">area_id</a>
<a name="ln6041">create_area_etc(team_id team, const char* name, size_t size, uint32 lock,</a>
<a name="ln6042">	uint32 protection, uint32 flags, uint32 guardSize,</a>
<a name="ln6043">	const virtual_address_restrictions* virtualAddressRestrictions,</a>
<a name="ln6044">	const physical_address_restrictions* physicalAddressRestrictions,</a>
<a name="ln6045">	void** _address)</a>
<a name="ln6046">{</a>
<a name="ln6047">	fix_protection(&amp;protection);</a>
<a name="ln6048"> </a>
<a name="ln6049">	return vm_create_anonymous_area(team, name, size, lock, protection, flags,</a>
<a name="ln6050">		guardSize, virtualAddressRestrictions, physicalAddressRestrictions,</a>
<a name="ln6051">		true, _address);</a>
<a name="ln6052">}</a>
<a name="ln6053"> </a>
<a name="ln6054"> </a>
<a name="ln6055">extern &quot;C&quot; area_id</a>
<a name="ln6056">__create_area_haiku(const char* name, void** _address, uint32 addressSpec,</a>
<a name="ln6057">	size_t size, uint32 lock, uint32 protection)</a>
<a name="ln6058">{</a>
<a name="ln6059">	fix_protection(&amp;protection);</a>
<a name="ln6060"> </a>
<a name="ln6061">	virtual_address_restrictions virtualRestrictions = {};</a>
<a name="ln6062">	virtualRestrictions.address = *_address;</a>
<a name="ln6063">	virtualRestrictions.address_specification = addressSpec;</a>
<a name="ln6064">	physical_address_restrictions physicalRestrictions = {};</a>
<a name="ln6065">	return vm_create_anonymous_area(VMAddressSpace::KernelID(), name, size,</a>
<a name="ln6066">		lock, protection, 0, 0, &amp;virtualRestrictions, &amp;physicalRestrictions,</a>
<a name="ln6067">		true, _address);</a>
<a name="ln6068">}</a>
<a name="ln6069"> </a>
<a name="ln6070"> </a>
<a name="ln6071">status_t</a>
<a name="ln6072">delete_area(area_id area)</a>
<a name="ln6073">{</a>
<a name="ln6074">	return vm_delete_area(VMAddressSpace::KernelID(), area, true);</a>
<a name="ln6075">}</a>
<a name="ln6076"> </a>
<a name="ln6077"> </a>
<a name="ln6078">//	#pragma mark - Userland syscalls</a>
<a name="ln6079"> </a>
<a name="ln6080"> </a>
<a name="ln6081">status_t</a>
<a name="ln6082">_user_reserve_address_range(addr_t* userAddress, uint32 addressSpec,</a>
<a name="ln6083">	addr_t size)</a>
<a name="ln6084">{</a>
<a name="ln6085">	// filter out some unavailable values (for userland)</a>
<a name="ln6086">	switch (addressSpec) {</a>
<a name="ln6087">		case B_ANY_KERNEL_ADDRESS:</a>
<a name="ln6088">		case B_ANY_KERNEL_BLOCK_ADDRESS:</a>
<a name="ln6089">			return B_BAD_VALUE;</a>
<a name="ln6090">	}</a>
<a name="ln6091"> </a>
<a name="ln6092">	addr_t address;</a>
<a name="ln6093"> </a>
<a name="ln6094">	if (!IS_USER_ADDRESS(userAddress)</a>
<a name="ln6095">		|| user_memcpy(&amp;address, userAddress, sizeof(address)) != B_OK)</a>
<a name="ln6096">		return B_BAD_ADDRESS;</a>
<a name="ln6097"> </a>
<a name="ln6098">	status_t status = vm_reserve_address_range(</a>
<a name="ln6099">		VMAddressSpace::CurrentID(), (void**)&amp;address, addressSpec, size,</a>
<a name="ln6100">		RESERVED_AVOID_BASE);</a>
<a name="ln6101">	if (status != B_OK)</a>
<a name="ln6102">		return status;</a>
<a name="ln6103"> </a>
<a name="ln6104">	if (user_memcpy(userAddress, &amp;address, sizeof(address)) != B_OK) {</a>
<a name="ln6105">		vm_unreserve_address_range(VMAddressSpace::CurrentID(),</a>
<a name="ln6106">			(void*)address, size);</a>
<a name="ln6107">		return B_BAD_ADDRESS;</a>
<a name="ln6108">	}</a>
<a name="ln6109"> </a>
<a name="ln6110">	return B_OK;</a>
<a name="ln6111">}</a>
<a name="ln6112"> </a>
<a name="ln6113"> </a>
<a name="ln6114">status_t</a>
<a name="ln6115">_user_unreserve_address_range(addr_t address, addr_t size)</a>
<a name="ln6116">{</a>
<a name="ln6117">	return vm_unreserve_address_range(VMAddressSpace::CurrentID(),</a>
<a name="ln6118">		(void*)address, size);</a>
<a name="ln6119">}</a>
<a name="ln6120"> </a>
<a name="ln6121"> </a>
<a name="ln6122">area_id</a>
<a name="ln6123">_user_area_for(void* address)</a>
<a name="ln6124">{</a>
<a name="ln6125">	return vm_area_for((addr_t)address, false);</a>
<a name="ln6126">}</a>
<a name="ln6127"> </a>
<a name="ln6128"> </a>
<a name="ln6129">area_id</a>
<a name="ln6130">_user_find_area(const char* userName)</a>
<a name="ln6131">{</a>
<a name="ln6132">	char name[B_OS_NAME_LENGTH];</a>
<a name="ln6133"> </a>
<a name="ln6134">	if (!IS_USER_ADDRESS(userName)</a>
<a name="ln6135">		|| user_strlcpy(name, userName, B_OS_NAME_LENGTH) &lt; B_OK)</a>
<a name="ln6136">		return B_BAD_ADDRESS;</a>
<a name="ln6137"> </a>
<a name="ln6138">	return find_area(name);</a>
<a name="ln6139">}</a>
<a name="ln6140"> </a>
<a name="ln6141"> </a>
<a name="ln6142">status_t</a>
<a name="ln6143">_user_get_area_info(area_id area, area_info* userInfo)</a>
<a name="ln6144">{</a>
<a name="ln6145">	if (!IS_USER_ADDRESS(userInfo))</a>
<a name="ln6146">		return B_BAD_ADDRESS;</a>
<a name="ln6147"> </a>
<a name="ln6148">	area_info info;</a>
<a name="ln6149">	status_t status = get_area_info(area, &amp;info);</a>
<a name="ln6150">	if (status &lt; B_OK)</a>
<a name="ln6151">		return status;</a>
<a name="ln6152"> </a>
<a name="ln6153">	// TODO: do we want to prevent userland from seeing kernel protections?</a>
<a name="ln6154">	//info.protection &amp;= B_USER_PROTECTION;</a>
<a name="ln6155"> </a>
<a name="ln6156">	if (user_memcpy(userInfo, &amp;info, sizeof(area_info)) &lt; B_OK)</a>
<a name="ln6157">		return B_BAD_ADDRESS;</a>
<a name="ln6158"> </a>
<a name="ln6159">	return status;</a>
<a name="ln6160">}</a>
<a name="ln6161"> </a>
<a name="ln6162"> </a>
<a name="ln6163">status_t</a>
<a name="ln6164">_user_get_next_area_info(team_id team, ssize_t* userCookie, area_info* userInfo)</a>
<a name="ln6165">{</a>
<a name="ln6166">	ssize_t cookie;</a>
<a name="ln6167"> </a>
<a name="ln6168">	if (!IS_USER_ADDRESS(userCookie)</a>
<a name="ln6169">		|| !IS_USER_ADDRESS(userInfo)</a>
<a name="ln6170">		|| user_memcpy(&amp;cookie, userCookie, sizeof(ssize_t)) &lt; B_OK)</a>
<a name="ln6171">		return B_BAD_ADDRESS;</a>
<a name="ln6172"> </a>
<a name="ln6173">	area_info info;</a>
<a name="ln6174">	status_t status = _get_next_area_info(team, &amp;cookie, &amp;info,</a>
<a name="ln6175">		sizeof(area_info));</a>
<a name="ln6176">	if (status != B_OK)</a>
<a name="ln6177">		return status;</a>
<a name="ln6178"> </a>
<a name="ln6179">	//info.protection &amp;= B_USER_PROTECTION;</a>
<a name="ln6180"> </a>
<a name="ln6181">	if (user_memcpy(userCookie, &amp;cookie, sizeof(ssize_t)) &lt; B_OK</a>
<a name="ln6182">		|| user_memcpy(userInfo, &amp;info, sizeof(area_info)) &lt; B_OK)</a>
<a name="ln6183">		return B_BAD_ADDRESS;</a>
<a name="ln6184"> </a>
<a name="ln6185">	return status;</a>
<a name="ln6186">}</a>
<a name="ln6187"> </a>
<a name="ln6188"> </a>
<a name="ln6189">status_t</a>
<a name="ln6190">_user_set_area_protection(area_id area, uint32 newProtection)</a>
<a name="ln6191">{</a>
<a name="ln6192">	if ((newProtection &amp; ~B_USER_PROTECTION) != 0)</a>
<a name="ln6193">		return B_BAD_VALUE;</a>
<a name="ln6194"> </a>
<a name="ln6195">	return vm_set_area_protection(VMAddressSpace::CurrentID(), area,</a>
<a name="ln6196">		newProtection, false);</a>
<a name="ln6197">}</a>
<a name="ln6198"> </a>
<a name="ln6199"> </a>
<a name="ln6200">status_t</a>
<a name="ln6201">_user_resize_area(area_id area, size_t newSize)</a>
<a name="ln6202">{</a>
<a name="ln6203">	// TODO: Since we restrict deleting of areas to those owned by the team,</a>
<a name="ln6204">	// we should also do that for resizing (check other functions, too).</a>
<a name="ln6205">	return vm_resize_area(area, newSize, false);</a>
<a name="ln6206">}</a>
<a name="ln6207"> </a>
<a name="ln6208"> </a>
<a name="ln6209">area_id</a>
<a name="ln6210">_user_transfer_area(area_id area, void** userAddress, uint32 addressSpec,</a>
<a name="ln6211">	team_id target)</a>
<a name="ln6212">{</a>
<a name="ln6213">	// filter out some unavailable values (for userland)</a>
<a name="ln6214">	switch (addressSpec) {</a>
<a name="ln6215">		case B_ANY_KERNEL_ADDRESS:</a>
<a name="ln6216">		case B_ANY_KERNEL_BLOCK_ADDRESS:</a>
<a name="ln6217">			return B_BAD_VALUE;</a>
<a name="ln6218">	}</a>
<a name="ln6219"> </a>
<a name="ln6220">	void* address;</a>
<a name="ln6221">	if (!IS_USER_ADDRESS(userAddress)</a>
<a name="ln6222">		|| user_memcpy(&amp;address, userAddress, sizeof(address)) &lt; B_OK)</a>
<a name="ln6223">		return B_BAD_ADDRESS;</a>
<a name="ln6224"> </a>
<a name="ln6225">	area_id newArea = transfer_area(area, &amp;address, addressSpec, target, false);</a>
<a name="ln6226">	if (newArea &lt; B_OK)</a>
<a name="ln6227">		return newArea;</a>
<a name="ln6228"> </a>
<a name="ln6229">	if (user_memcpy(userAddress, &amp;address, sizeof(address)) &lt; B_OK)</a>
<a name="ln6230">		return B_BAD_ADDRESS;</a>
<a name="ln6231"> </a>
<a name="ln6232">	return newArea;</a>
<a name="ln6233">}</a>
<a name="ln6234"> </a>
<a name="ln6235"> </a>
<a name="ln6236">area_id</a>
<a name="ln6237">_user_clone_area(const char* userName, void** userAddress, uint32 addressSpec,</a>
<a name="ln6238">	uint32 protection, area_id sourceArea)</a>
<a name="ln6239">{</a>
<a name="ln6240">	char name[B_OS_NAME_LENGTH];</a>
<a name="ln6241">	void* address;</a>
<a name="ln6242"> </a>
<a name="ln6243">	// filter out some unavailable values (for userland)</a>
<a name="ln6244">	switch (addressSpec) {</a>
<a name="ln6245">		case B_ANY_KERNEL_ADDRESS:</a>
<a name="ln6246">		case B_ANY_KERNEL_BLOCK_ADDRESS:</a>
<a name="ln6247">			return B_BAD_VALUE;</a>
<a name="ln6248">	}</a>
<a name="ln6249">	if ((protection &amp; ~B_USER_AREA_FLAGS) != 0)</a>
<a name="ln6250">		return B_BAD_VALUE;</a>
<a name="ln6251"> </a>
<a name="ln6252">	if (!IS_USER_ADDRESS(userName)</a>
<a name="ln6253">		|| !IS_USER_ADDRESS(userAddress)</a>
<a name="ln6254">		|| user_strlcpy(name, userName, sizeof(name)) &lt; B_OK</a>
<a name="ln6255">		|| user_memcpy(&amp;address, userAddress, sizeof(address)) &lt; B_OK)</a>
<a name="ln6256">		return B_BAD_ADDRESS;</a>
<a name="ln6257"> </a>
<a name="ln6258">	fix_protection(&amp;protection);</a>
<a name="ln6259"> </a>
<a name="ln6260">	area_id clonedArea = vm_clone_area(VMAddressSpace::CurrentID(), name,</a>
<a name="ln6261">		&amp;address, addressSpec, protection, REGION_NO_PRIVATE_MAP, sourceArea,</a>
<a name="ln6262">		false);</a>
<a name="ln6263">	if (clonedArea &lt; B_OK)</a>
<a name="ln6264">		return clonedArea;</a>
<a name="ln6265"> </a>
<a name="ln6266">	if (user_memcpy(userAddress, &amp;address, sizeof(address)) &lt; B_OK) {</a>
<a name="ln6267">		delete_area(clonedArea);</a>
<a name="ln6268">		return B_BAD_ADDRESS;</a>
<a name="ln6269">	}</a>
<a name="ln6270"> </a>
<a name="ln6271">	return clonedArea;</a>
<a name="ln6272">}</a>
<a name="ln6273"> </a>
<a name="ln6274"> </a>
<a name="ln6275">area_id</a>
<a name="ln6276">_user_create_area(const char* userName, void** userAddress, uint32 addressSpec,</a>
<a name="ln6277">	size_t size, uint32 lock, uint32 protection)</a>
<a name="ln6278">{</a>
<a name="ln6279">	char name[B_OS_NAME_LENGTH];</a>
<a name="ln6280">	void* address;</a>
<a name="ln6281"> </a>
<a name="ln6282">	// filter out some unavailable values (for userland)</a>
<a name="ln6283">	switch (addressSpec) {</a>
<a name="ln6284">		case B_ANY_KERNEL_ADDRESS:</a>
<a name="ln6285">		case B_ANY_KERNEL_BLOCK_ADDRESS:</a>
<a name="ln6286">			return B_BAD_VALUE;</a>
<a name="ln6287">	}</a>
<a name="ln6288">	if ((protection &amp; ~B_USER_AREA_FLAGS) != 0)</a>
<a name="ln6289">		return B_BAD_VALUE;</a>
<a name="ln6290"> </a>
<a name="ln6291">	if (!IS_USER_ADDRESS(userName)</a>
<a name="ln6292">		|| !IS_USER_ADDRESS(userAddress)</a>
<a name="ln6293">		|| user_strlcpy(name, userName, sizeof(name)) &lt; B_OK</a>
<a name="ln6294">		|| user_memcpy(&amp;address, userAddress, sizeof(address)) &lt; B_OK)</a>
<a name="ln6295">		return B_BAD_ADDRESS;</a>
<a name="ln6296"> </a>
<a name="ln6297">	if (addressSpec == B_EXACT_ADDRESS</a>
<a name="ln6298">		&amp;&amp; IS_KERNEL_ADDRESS(address))</a>
<a name="ln6299">		return B_BAD_VALUE;</a>
<a name="ln6300"> </a>
<a name="ln6301">	if (addressSpec == B_ANY_ADDRESS)</a>
<a name="ln6302">		addressSpec = B_RANDOMIZED_ANY_ADDRESS;</a>
<a name="ln6303">	if (addressSpec == B_BASE_ADDRESS)</a>
<a name="ln6304">		addressSpec = B_RANDOMIZED_BASE_ADDRESS;</a>
<a name="ln6305"> </a>
<a name="ln6306">	fix_protection(&amp;protection);</a>
<a name="ln6307"> </a>
<a name="ln6308">	virtual_address_restrictions virtualRestrictions = {};</a>
<a name="ln6309">	virtualRestrictions.address = address;</a>
<a name="ln6310">	virtualRestrictions.address_specification = addressSpec;</a>
<a name="ln6311">	physical_address_restrictions physicalRestrictions = {};</a>
<a name="ln6312">	area_id area = vm_create_anonymous_area(VMAddressSpace::CurrentID(), name,</a>
<a name="ln6313">		size, lock, protection, 0, 0, &amp;virtualRestrictions,</a>
<a name="ln6314">		&amp;physicalRestrictions, false, &amp;address);</a>
<a name="ln6315"> </a>
<a name="ln6316">	if (area &gt;= B_OK</a>
<a name="ln6317">		&amp;&amp; user_memcpy(userAddress, &amp;address, sizeof(address)) &lt; B_OK) {</a>
<a name="ln6318">		delete_area(area);</a>
<a name="ln6319">		return B_BAD_ADDRESS;</a>
<a name="ln6320">	}</a>
<a name="ln6321"> </a>
<a name="ln6322">	return area;</a>
<a name="ln6323">}</a>
<a name="ln6324"> </a>
<a name="ln6325"> </a>
<a name="ln6326">status_t</a>
<a name="ln6327">_user_delete_area(area_id area)</a>
<a name="ln6328">{</a>
<a name="ln6329">	// Unlike the BeOS implementation, you can now only delete areas</a>
<a name="ln6330">	// that you have created yourself from userland.</a>
<a name="ln6331">	// The documentation to delete_area() explicitly states that this</a>
<a name="ln6332">	// will be restricted in the future, and so it will.</a>
<a name="ln6333">	return vm_delete_area(VMAddressSpace::CurrentID(), area, false);</a>
<a name="ln6334">}</a>
<a name="ln6335"> </a>
<a name="ln6336"> </a>
<a name="ln6337">// TODO: create a BeOS style call for this!</a>
<a name="ln6338"> </a>
<a name="ln6339">area_id</a>
<a name="ln6340">_user_map_file(const char* userName, void** userAddress, uint32 addressSpec,</a>
<a name="ln6341">	size_t size, uint32 protection, uint32 mapping, bool unmapAddressRange,</a>
<a name="ln6342">	int fd, off_t offset)</a>
<a name="ln6343">{</a>
<a name="ln6344">	char name[B_OS_NAME_LENGTH];</a>
<a name="ln6345">	void* address;</a>
<a name="ln6346">	area_id area;</a>
<a name="ln6347"> </a>
<a name="ln6348">	if ((protection &amp; ~B_USER_AREA_FLAGS) != 0)</a>
<a name="ln6349">		return B_BAD_VALUE;</a>
<a name="ln6350"> </a>
<a name="ln6351">	fix_protection(&amp;protection);</a>
<a name="ln6352"> </a>
<a name="ln6353">	if (!IS_USER_ADDRESS(userName) || !IS_USER_ADDRESS(userAddress)</a>
<a name="ln6354">		|| user_strlcpy(name, userName, B_OS_NAME_LENGTH) &lt; B_OK</a>
<a name="ln6355">		|| user_memcpy(&amp;address, userAddress, sizeof(address)) &lt; B_OK)</a>
<a name="ln6356">		return B_BAD_ADDRESS;</a>
<a name="ln6357"> </a>
<a name="ln6358">	if (addressSpec == B_EXACT_ADDRESS) {</a>
<a name="ln6359">		if ((addr_t)address + size &lt; (addr_t)address</a>
<a name="ln6360">				|| (addr_t)address % B_PAGE_SIZE != 0) {</a>
<a name="ln6361">			return B_BAD_VALUE;</a>
<a name="ln6362">		}</a>
<a name="ln6363">		if (!IS_USER_ADDRESS(address)</a>
<a name="ln6364">				|| !IS_USER_ADDRESS((addr_t)address + size - 1)) {</a>
<a name="ln6365">			return B_BAD_ADDRESS;</a>
<a name="ln6366">		}</a>
<a name="ln6367">	}</a>
<a name="ln6368"> </a>
<a name="ln6369">	area = _vm_map_file(VMAddressSpace::CurrentID(), name, &amp;address,</a>
<a name="ln6370">		addressSpec, size, protection, mapping, unmapAddressRange, fd, offset,</a>
<a name="ln6371">		false);</a>
<a name="ln6372">	if (area &lt; B_OK)</a>
<a name="ln6373">		return area;</a>
<a name="ln6374"> </a>
<a name="ln6375">	if (user_memcpy(userAddress, &amp;address, sizeof(address)) &lt; B_OK)</a>
<a name="ln6376">		return B_BAD_ADDRESS;</a>
<a name="ln6377"> </a>
<a name="ln6378">	return area;</a>
<a name="ln6379">}</a>
<a name="ln6380"> </a>
<a name="ln6381"> </a>
<a name="ln6382">status_t</a>
<a name="ln6383">_user_unmap_memory(void* _address, size_t size)</a>
<a name="ln6384">{</a>
<a name="ln6385">	addr_t address = (addr_t)_address;</a>
<a name="ln6386"> </a>
<a name="ln6387">	// check params</a>
<a name="ln6388">	if (size == 0 || (addr_t)address + size &lt; (addr_t)address</a>
<a name="ln6389">		|| (addr_t)address % B_PAGE_SIZE != 0) {</a>
<a name="ln6390">		return B_BAD_VALUE;</a>
<a name="ln6391">	}</a>
<a name="ln6392"> </a>
<a name="ln6393">	if (!IS_USER_ADDRESS(address) || !IS_USER_ADDRESS((addr_t)address + size))</a>
<a name="ln6394">		return B_BAD_ADDRESS;</a>
<a name="ln6395"> </a>
<a name="ln6396">	// Write lock the address space and ensure the address range is not wired.</a>
<a name="ln6397">	AddressSpaceWriteLocker locker;</a>
<a name="ln6398">	do {</a>
<a name="ln6399">		status_t status = locker.SetTo(team_get_current_team_id());</a>
<a name="ln6400">		if (status != B_OK)</a>
<a name="ln6401">			return status;</a>
<a name="ln6402">	} while (wait_if_address_range_is_wired(locker.AddressSpace(), address,</a>
<a name="ln6403">			size, &amp;locker));</a>
<a name="ln6404"> </a>
<a name="ln6405">	// unmap</a>
<a name="ln6406">	return unmap_address_range(locker.AddressSpace(), address, size, false);</a>
<a name="ln6407">}</a>
<a name="ln6408"> </a>
<a name="ln6409"> </a>
<a name="ln6410">status_t</a>
<a name="ln6411">_user_set_memory_protection(void* _address, size_t size, uint32 protection)</a>
<a name="ln6412">{</a>
<a name="ln6413">	// check address range</a>
<a name="ln6414">	addr_t address = (addr_t)_address;</a>
<a name="ln6415">	size = PAGE_ALIGN(size);</a>
<a name="ln6416"> </a>
<a name="ln6417">	if ((address % B_PAGE_SIZE) != 0)</a>
<a name="ln6418">		return B_BAD_VALUE;</a>
<a name="ln6419">	if ((addr_t)address + size &lt; (addr_t)address || !IS_USER_ADDRESS(address)</a>
<a name="ln6420">		|| !IS_USER_ADDRESS((addr_t)address + size)) {</a>
<a name="ln6421">		// weird error code required by POSIX</a>
<a name="ln6422">		return ENOMEM;</a>
<a name="ln6423">	}</a>
<a name="ln6424"> </a>
<a name="ln6425">	// extend and check protection</a>
<a name="ln6426">	if ((protection &amp; ~B_USER_PROTECTION) != 0)</a>
<a name="ln6427">		return B_BAD_VALUE;</a>
<a name="ln6428"> </a>
<a name="ln6429">	fix_protection(&amp;protection);</a>
<a name="ln6430"> </a>
<a name="ln6431">	// We need to write lock the address space, since we're going to play with</a>
<a name="ln6432">	// the areas. Also make sure that none of the areas is wired and that we're</a>
<a name="ln6433">	// actually allowed to change the protection.</a>
<a name="ln6434">	AddressSpaceWriteLocker locker;</a>
<a name="ln6435"> </a>
<a name="ln6436">	bool restart;</a>
<a name="ln6437">	do {</a>
<a name="ln6438">		restart = false;</a>
<a name="ln6439"> </a>
<a name="ln6440">		status_t status = locker.SetTo(team_get_current_team_id());</a>
<a name="ln6441">		if (status != B_OK)</a>
<a name="ln6442">			return status;</a>
<a name="ln6443"> </a>
<a name="ln6444">		// First round: Check whether the whole range is covered by areas and we</a>
<a name="ln6445">		// are allowed to modify them.</a>
<a name="ln6446">		addr_t currentAddress = address;</a>
<a name="ln6447">		size_t sizeLeft = size;</a>
<a name="ln6448">		while (sizeLeft &gt; 0) {</a>
<a name="ln6449">			VMArea* area = locker.AddressSpace()-&gt;LookupArea(currentAddress);</a>
<a name="ln6450">			if (area == NULL)</a>
<a name="ln6451">				return B_NO_MEMORY;</a>
<a name="ln6452"> </a>
<a name="ln6453">			if (area-&gt;address_space == VMAddressSpace::Kernel())</a>
<a name="ln6454">				return B_NOT_ALLOWED;</a>
<a name="ln6455"> </a>
<a name="ln6456">			// TODO: For (shared) mapped files we should check whether the new</a>
<a name="ln6457">			// protections are compatible with the file permissions. We don't</a>
<a name="ln6458">			// have a way to do that yet, though.</a>
<a name="ln6459"> </a>
<a name="ln6460">			addr_t offset = currentAddress - area-&gt;Base();</a>
<a name="ln6461">			size_t rangeSize = min_c(area-&gt;Size() - offset, sizeLeft);</a>
<a name="ln6462"> </a>
<a name="ln6463">			AreaCacheLocker cacheLocker(area);</a>
<a name="ln6464"> </a>
<a name="ln6465">			if (wait_if_area_range_is_wired(area, currentAddress, rangeSize,</a>
<a name="ln6466">					&amp;locker, &amp;cacheLocker)) {</a>
<a name="ln6467">				restart = true;</a>
<a name="ln6468">				break;</a>
<a name="ln6469">			}</a>
<a name="ln6470"> </a>
<a name="ln6471">			cacheLocker.Unlock();</a>
<a name="ln6472"> </a>
<a name="ln6473">			currentAddress += rangeSize;</a>
<a name="ln6474">			sizeLeft -= rangeSize;</a>
<a name="ln6475">		}</a>
<a name="ln6476">	} while (restart);</a>
<a name="ln6477"> </a>
<a name="ln6478">	// Second round: If the protections differ from that of the area, create a</a>
<a name="ln6479">	// page protection array and re-map mapped pages.</a>
<a name="ln6480">	VMTranslationMap* map = locker.AddressSpace()-&gt;TranslationMap();</a>
<a name="ln6481">	addr_t currentAddress = address;</a>
<a name="ln6482">	size_t sizeLeft = size;</a>
<a name="ln6483">	while (sizeLeft &gt; 0) {</a>
<a name="ln6484">		VMArea* area = locker.AddressSpace()-&gt;LookupArea(currentAddress);</a>
<a name="ln6485">		if (area == NULL)</a>
<a name="ln6486">			return B_NO_MEMORY;</a>
<a name="ln6487"> </a>
<a name="ln6488">		addr_t offset = currentAddress - area-&gt;Base();</a>
<a name="ln6489">		size_t rangeSize = min_c(area-&gt;Size() - offset, sizeLeft);</a>
<a name="ln6490"> </a>
<a name="ln6491">		currentAddress += rangeSize;</a>
<a name="ln6492">		sizeLeft -= rangeSize;</a>
<a name="ln6493"> </a>
<a name="ln6494">		if (area-&gt;page_protections == NULL) {</a>
<a name="ln6495">			if (area-&gt;protection == protection)</a>
<a name="ln6496">				continue;</a>
<a name="ln6497"> </a>
<a name="ln6498">			status_t status = allocate_area_page_protections(area);</a>
<a name="ln6499">			if (status != B_OK)</a>
<a name="ln6500">				return status;</a>
<a name="ln6501">		}</a>
<a name="ln6502"> </a>
<a name="ln6503">		// We need to lock the complete cache chain, since we potentially unmap</a>
<a name="ln6504">		// pages of lower caches.</a>
<a name="ln6505">		VMCache* topCache = vm_area_get_locked_cache(area);</a>
<a name="ln6506">		VMCacheChainLocker cacheChainLocker(topCache);</a>
<a name="ln6507">		cacheChainLocker.LockAllSourceCaches();</a>
<a name="ln6508"> </a>
<a name="ln6509">		for (addr_t pageAddress = area-&gt;Base() + offset;</a>
<a name="ln6510">				pageAddress &lt; currentAddress; pageAddress += B_PAGE_SIZE) {</a>
<a name="ln6511">			map-&gt;Lock();</a>
<a name="ln6512"> </a>
<a name="ln6513">			set_area_page_protection(area, pageAddress, protection);</a>
<a name="ln6514"> </a>
<a name="ln6515">			phys_addr_t physicalAddress;</a>
<a name="ln6516">			uint32 flags;</a>
<a name="ln6517"> </a>
<a name="ln6518">			status_t error = map-&gt;Query(pageAddress, &amp;physicalAddress, &amp;flags);</a>
<a name="ln6519">			if (error != B_OK || (flags &amp; PAGE_PRESENT) == 0) {</a>
<a name="ln6520">				map-&gt;Unlock();</a>
<a name="ln6521">				continue;</a>
<a name="ln6522">			}</a>
<a name="ln6523"> </a>
<a name="ln6524">			vm_page* page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);</a>
<a name="ln6525">			if (page == NULL) {</a>
<a name="ln6526">				panic(&quot;area %p looking up page failed for pa %#&quot; B_PRIxPHYSADDR</a>
<a name="ln6527">					&quot;\n&quot;, area, physicalAddress);</a>
<a name="ln6528">				map-&gt;Unlock();</a>
<a name="ln6529">				return B_ERROR;</a>
<a name="ln6530">			}</a>
<a name="ln6531"> </a>
<a name="ln6532">			// If the page is not in the topmost cache and write access is</a>
<a name="ln6533">			// requested, we have to unmap it. Otherwise we can re-map it with</a>
<a name="ln6534">			// the new protection.</a>
<a name="ln6535">			bool unmapPage = page-&gt;Cache() != topCache</a>
<a name="ln6536">				&amp;&amp; (protection &amp; B_WRITE_AREA) != 0;</a>
<a name="ln6537"> </a>
<a name="ln6538">			if (!unmapPage)</a>
<a name="ln6539">				map-&gt;ProtectPage(area, pageAddress, protection);</a>
<a name="ln6540"> </a>
<a name="ln6541">			map-&gt;Unlock();</a>
<a name="ln6542"> </a>
<a name="ln6543">			if (unmapPage) {</a>
<a name="ln6544">				DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln6545">				unmap_page(area, pageAddress);</a>
<a name="ln6546">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln6547">			}</a>
<a name="ln6548">		}</a>
<a name="ln6549">	}</a>
<a name="ln6550"> </a>
<a name="ln6551">	return B_OK;</a>
<a name="ln6552">}</a>
<a name="ln6553"> </a>
<a name="ln6554"> </a>
<a name="ln6555">status_t</a>
<a name="ln6556">_user_sync_memory(void* _address, size_t size, uint32 flags)</a>
<a name="ln6557">{</a>
<a name="ln6558">	addr_t address = (addr_t)_address;</a>
<a name="ln6559">	size = PAGE_ALIGN(size);</a>
<a name="ln6560"> </a>
<a name="ln6561">	// check params</a>
<a name="ln6562">	if ((address % B_PAGE_SIZE) != 0)</a>
<a name="ln6563">		return B_BAD_VALUE;</a>
<a name="ln6564">	if ((addr_t)address + size &lt; (addr_t)address || !IS_USER_ADDRESS(address)</a>
<a name="ln6565">		|| !IS_USER_ADDRESS((addr_t)address + size)) {</a>
<a name="ln6566">		// weird error code required by POSIX</a>
<a name="ln6567">		return ENOMEM;</a>
<a name="ln6568">	}</a>
<a name="ln6569"> </a>
<a name="ln6570">	bool writeSync = (flags &amp; MS_SYNC) != 0;</a>
<a name="ln6571">	bool writeAsync = (flags &amp; MS_ASYNC) != 0;</a>
<a name="ln6572">	if (writeSync &amp;&amp; writeAsync)</a>
<a name="ln6573">		return B_BAD_VALUE;</a>
<a name="ln6574"> </a>
<a name="ln6575">	if (size == 0 || (!writeSync &amp;&amp; !writeAsync))</a>
<a name="ln6576">		return B_OK;</a>
<a name="ln6577"> </a>
<a name="ln6578">	// iterate through the range and sync all concerned areas</a>
<a name="ln6579">	while (size &gt; 0) {</a>
<a name="ln6580">		// read lock the address space</a>
<a name="ln6581">		AddressSpaceReadLocker locker;</a>
<a name="ln6582">		status_t error = locker.SetTo(team_get_current_team_id());</a>
<a name="ln6583">		if (error != B_OK)</a>
<a name="ln6584">			return error;</a>
<a name="ln6585"> </a>
<a name="ln6586">		// get the first area</a>
<a name="ln6587">		VMArea* area = locker.AddressSpace()-&gt;LookupArea(address);</a>
<a name="ln6588">		if (area == NULL)</a>
<a name="ln6589">			return B_NO_MEMORY;</a>
<a name="ln6590"> </a>
<a name="ln6591">		uint32 offset = address - area-&gt;Base();</a>
<a name="ln6592">		size_t rangeSize = min_c(area-&gt;Size() - offset, size);</a>
<a name="ln6593">		offset += area-&gt;cache_offset;</a>
<a name="ln6594"> </a>
<a name="ln6595">		// lock the cache</a>
<a name="ln6596">		AreaCacheLocker cacheLocker(area);</a>
<a name="ln6597">		if (!cacheLocker)</a>
<a name="ln6598">			return B_BAD_VALUE;</a>
<a name="ln6599">		VMCache* cache = area-&gt;cache;</a>
<a name="ln6600"> </a>
<a name="ln6601">		locker.Unlock();</a>
<a name="ln6602"> </a>
<a name="ln6603">		uint32 firstPage = offset &gt;&gt; PAGE_SHIFT;</a>
<a name="ln6604">		uint32 endPage = firstPage + (rangeSize &gt;&gt; PAGE_SHIFT);</a>
<a name="ln6605"> </a>
<a name="ln6606">		// write the pages</a>
<a name="ln6607">		if (cache-&gt;type == CACHE_TYPE_VNODE) {</a>
<a name="ln6608">			if (writeSync) {</a>
<a name="ln6609">				// synchronous</a>
<a name="ln6610">				error = vm_page_write_modified_page_range(cache, firstPage,</a>
<a name="ln6611">					endPage);</a>
<a name="ln6612">				if (error != B_OK)</a>
<a name="ln6613">					return error;</a>
<a name="ln6614">			} else {</a>
<a name="ln6615">				// asynchronous</a>
<a name="ln6616">				vm_page_schedule_write_page_range(cache, firstPage, endPage);</a>
<a name="ln6617">				// TODO: This is probably not quite what is supposed to happen.</a>
<a name="ln6618">				// Especially when a lot has to be written, it might take ages</a>
<a name="ln6619">				// until it really hits the disk.</a>
<a name="ln6620">			}</a>
<a name="ln6621">		}</a>
<a name="ln6622"> </a>
<a name="ln6623">		address += rangeSize;</a>
<a name="ln6624">		size -= rangeSize;</a>
<a name="ln6625">	}</a>
<a name="ln6626"> </a>
<a name="ln6627">	// NOTE: If I understand it correctly the purpose of MS_INVALIDATE is to</a>
<a name="ln6628">	// synchronize multiple mappings of the same file. In our VM they never get</a>
<a name="ln6629">	// out of sync, though, so we don't have to do anything.</a>
<a name="ln6630"> </a>
<a name="ln6631">	return B_OK;</a>
<a name="ln6632">}</a>
<a name="ln6633"> </a>
<a name="ln6634"> </a>
<a name="ln6635">status_t</a>
<a name="ln6636">_user_memory_advice(void* address, size_t size, uint32 advice)</a>
<a name="ln6637">{</a>
<a name="ln6638">	// TODO: Implement!</a>
<a name="ln6639">	return B_OK;</a>
<a name="ln6640">}</a>
<a name="ln6641"> </a>
<a name="ln6642"> </a>
<a name="ln6643">status_t</a>
<a name="ln6644">_user_get_memory_properties(team_id teamID, const void* address,</a>
<a name="ln6645">	uint32* _protected, uint32* _lock)</a>
<a name="ln6646">{</a>
<a name="ln6647">	if (!IS_USER_ADDRESS(_protected) || !IS_USER_ADDRESS(_lock))</a>
<a name="ln6648">		return B_BAD_ADDRESS;</a>
<a name="ln6649"> </a>
<a name="ln6650">	AddressSpaceReadLocker locker;</a>
<a name="ln6651">	status_t error = locker.SetTo(teamID);</a>
<a name="ln6652">	if (error != B_OK)</a>
<a name="ln6653">		return error;</a>
<a name="ln6654"> </a>
<a name="ln6655">	VMArea* area = locker.AddressSpace()-&gt;LookupArea((addr_t)address);</a>
<a name="ln6656">	if (area == NULL)</a>
<a name="ln6657">		return B_NO_MEMORY;</a>
<a name="ln6658"> </a>
<a name="ln6659"> </a>
<a name="ln6660">	uint32 protection = area-&gt;protection;</a>
<a name="ln6661">	if (area-&gt;page_protections != NULL)</a>
<a name="ln6662">		protection = get_area_page_protection(area, (addr_t)address);</a>
<a name="ln6663"> </a>
<a name="ln6664">	uint32 wiring = area-&gt;wiring;</a>
<a name="ln6665"> </a>
<a name="ln6666">	locker.Unlock();</a>
<a name="ln6667"> </a>
<a name="ln6668">	error = user_memcpy(_protected, &amp;protection, sizeof(protection));</a>
<a name="ln6669">	if (error != B_OK)</a>
<a name="ln6670">		return error;</a>
<a name="ln6671"> </a>
<a name="ln6672">	error = user_memcpy(_lock, &amp;wiring, sizeof(wiring));</a>
<a name="ln6673"> </a>
<a name="ln6674">	return error;</a>
<a name="ln6675">}</a>
<a name="ln6676"> </a>
<a name="ln6677"> </a>
<a name="ln6678">// #pragma mark -- compatibility</a>
<a name="ln6679"> </a>
<a name="ln6680"> </a>
<a name="ln6681">#if defined(__i386__) &amp;&amp; B_HAIKU_PHYSICAL_BITS &gt; 32</a>
<a name="ln6682"> </a>
<a name="ln6683"> </a>
<a name="ln6684">struct physical_entry_beos {</a>
<a name="ln6685">	uint32	address;</a>
<a name="ln6686">	uint32	size;</a>
<a name="ln6687">};</a>
<a name="ln6688"> </a>
<a name="ln6689"> </a>
<a name="ln6690">/*!	The physical_entry structure has changed. We need to translate it to the</a>
<a name="ln6691">	old one.</a>
<a name="ln6692">*/</a>
<a name="ln6693">extern &quot;C&quot; int32</a>
<a name="ln6694">__get_memory_map_beos(const void* _address, size_t numBytes,</a>
<a name="ln6695">	physical_entry_beos* table, int32 numEntries)</a>
<a name="ln6696">{</a>
<a name="ln6697">	if (numEntries &lt;= 0)</a>
<a name="ln6698">		return B_BAD_VALUE;</a>
<a name="ln6699"> </a>
<a name="ln6700">	const uint8* address = (const uint8*)_address;</a>
<a name="ln6701"> </a>
<a name="ln6702">	int32 count = 0;</a>
<a name="ln6703">	while (numBytes &gt; 0 &amp;&amp; count &lt; numEntries) {</a>
<a name="ln6704">		physical_entry entry;</a>
<a name="ln6705">		status_t result = __get_memory_map_haiku(address, numBytes, &amp;entry, 1);</a>
<a name="ln6706">		if (result &lt; 0) {</a>
<a name="ln6707">			if (result != B_BUFFER_OVERFLOW)</a>
<a name="ln6708">				return result;</a>
<a name="ln6709">		}</a>
<a name="ln6710"> </a>
<a name="ln6711">		if (entry.address &gt;= (phys_addr_t)1 &lt;&lt; 32) {</a>
<a name="ln6712">			panic(&quot;get_memory_map(): Address is greater 4 GB!&quot;);</a>
<a name="ln6713">			return B_ERROR;</a>
<a name="ln6714">		}</a>
<a name="ln6715"> </a>
<a name="ln6716">		table[count].address = entry.address;</a>
<a name="ln6717">		table[count++].size = entry.size;</a>
<a name="ln6718"> </a>
<a name="ln6719">		address += entry.size;</a>
<a name="ln6720">		numBytes -= entry.size;</a>
<a name="ln6721">	}</a>
<a name="ln6722"> </a>
<a name="ln6723">	// null-terminate the table, if possible</a>
<a name="ln6724">	if (count &lt; numEntries) {</a>
<a name="ln6725">		table[count].address = 0;</a>
<a name="ln6726">		table[count].size = 0;</a>
<a name="ln6727">	}</a>
<a name="ln6728"> </a>
<a name="ln6729">	return B_OK;</a>
<a name="ln6730">}</a>
<a name="ln6731"> </a>
<a name="ln6732"> </a>
<a name="ln6733">/*!	The type of the \a physicalAddress parameter has changed from void* to</a>
<a name="ln6734">	phys_addr_t.</a>
<a name="ln6735">*/</a>
<a name="ln6736">extern &quot;C&quot; area_id</a>
<a name="ln6737">__map_physical_memory_beos(const char* name, void* physicalAddress,</a>
<a name="ln6738">	size_t numBytes, uint32 addressSpec, uint32 protection,</a>
<a name="ln6739">	void** _virtualAddress)</a>
<a name="ln6740">{</a>
<a name="ln6741">	return __map_physical_memory_haiku(name, (addr_t)physicalAddress, numBytes,</a>
<a name="ln6742">		addressSpec, protection, _virtualAddress);</a>
<a name="ln6743">}</a>
<a name="ln6744"> </a>
<a name="ln6745"> </a>
<a name="ln6746">/*! The caller might not be able to deal with physical addresses &gt;= 4 GB, so</a>
<a name="ln6747">	we meddle with the \a lock parameter to force 32 bit.</a>
<a name="ln6748">*/</a>
<a name="ln6749">extern &quot;C&quot; area_id</a>
<a name="ln6750">__create_area_beos(const char* name, void** _address, uint32 addressSpec,</a>
<a name="ln6751">	size_t size, uint32 lock, uint32 protection)</a>
<a name="ln6752">{</a>
<a name="ln6753">	switch (lock) {</a>
<a name="ln6754">		case B_NO_LOCK:</a>
<a name="ln6755">			break;</a>
<a name="ln6756">		case B_FULL_LOCK:</a>
<a name="ln6757">		case B_LAZY_LOCK:</a>
<a name="ln6758">			lock = B_32_BIT_FULL_LOCK;</a>
<a name="ln6759">			break;</a>
<a name="ln6760">		case B_CONTIGUOUS:</a>
<a name="ln6761">			lock = B_32_BIT_CONTIGUOUS;</a>
<a name="ln6762">			break;</a>
<a name="ln6763">	}</a>
<a name="ln6764"> </a>
<a name="ln6765">	return __create_area_haiku(name, _address, addressSpec, size, lock,</a>
<a name="ln6766">		protection);</a>
<a name="ln6767">}</a>
<a name="ln6768"> </a>
<a name="ln6769"> </a>
<a name="ln6770">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__get_memory_map_beos&quot;, &quot;get_memory_map@&quot;,</a>
<a name="ln6771">	&quot;BASE&quot;);</a>
<a name="ln6772">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__map_physical_memory_beos&quot;,</a>
<a name="ln6773">	&quot;map_physical_memory@&quot;, &quot;BASE&quot;);</a>
<a name="ln6774">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__create_area_beos&quot;, &quot;create_area@&quot;,</a>
<a name="ln6775">	&quot;BASE&quot;);</a>
<a name="ln6776"> </a>
<a name="ln6777">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__get_memory_map_haiku&quot;,</a>
<a name="ln6778">	&quot;get_memory_map@@&quot;, &quot;1_ALPHA3&quot;);</a>
<a name="ln6779">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__map_physical_memory_haiku&quot;,</a>
<a name="ln6780">	&quot;map_physical_memory@@&quot;, &quot;1_ALPHA3&quot;);</a>
<a name="ln6781">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__create_area_haiku&quot;, &quot;create_area@@&quot;,</a>
<a name="ln6782">	&quot;1_ALPHA3&quot;);</a>
<a name="ln6783"> </a>
<a name="ln6784"> </a>
<a name="ln6785">#else</a>
<a name="ln6786"> </a>
<a name="ln6787"> </a>
<a name="ln6788">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__get_memory_map_haiku&quot;,</a>
<a name="ln6789">	&quot;get_memory_map@@&quot;, &quot;BASE&quot;);</a>
<a name="ln6790">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__map_physical_memory_haiku&quot;,</a>
<a name="ln6791">	&quot;map_physical_memory@@&quot;, &quot;BASE&quot;);</a>
<a name="ln6792">DEFINE_LIBROOT_KERNEL_SYMBOL_VERSION(&quot;__create_area_haiku&quot;, &quot;create_area@@&quot;,</a>
<a name="ln6793">	&quot;BASE&quot;);</a>
<a name="ln6794"> </a>
<a name="ln6795"> </a>
<a name="ln6796">#endif	// defined(__i386__) &amp;&amp; B_HAIKU_PHYSICAL_BITS &gt; 32</a>

</code></pre>
<div class="balloon" rel="2449"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v614/" target="_blank">V614</a> Potentially uninitialized pointer 'cache' used. Consider checking the first actual argument of the 'SetTo' function.</p></div>
<div class="balloon" rel="2451"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v614/" target="_blank">V614</a> Potentially uninitialized pointer 'source' used.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
