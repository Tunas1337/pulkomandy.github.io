
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>vm_page.cpp</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">/*</a>
<a name="ln2"> * Copyright 2010-2011, Ingo Weinhold, ingo_weinhold@gmx.de.</a>
<a name="ln3"> * Copyright 2002-2010, Axel DÃ¶rfler, axeld@pinc-software.de.</a>
<a name="ln4"> * Distributed under the terms of the MIT License.</a>
<a name="ln5"> *</a>
<a name="ln6"> * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.</a>
<a name="ln7"> * Distributed under the terms of the NewOS License.</a>
<a name="ln8"> */</a>
<a name="ln9"> </a>
<a name="ln10"> </a>
<a name="ln11">#include &lt;string.h&gt;</a>
<a name="ln12">#include &lt;stdlib.h&gt;</a>
<a name="ln13"> </a>
<a name="ln14">#include &lt;algorithm&gt;</a>
<a name="ln15"> </a>
<a name="ln16">#include &lt;KernelExport.h&gt;</a>
<a name="ln17">#include &lt;OS.h&gt;</a>
<a name="ln18"> </a>
<a name="ln19">#include &lt;AutoDeleter.h&gt;</a>
<a name="ln20"> </a>
<a name="ln21">#include &lt;arch/cpu.h&gt;</a>
<a name="ln22">#include &lt;arch/vm_translation_map.h&gt;</a>
<a name="ln23">#include &lt;block_cache.h&gt;</a>
<a name="ln24">#include &lt;boot/kernel_args.h&gt;</a>
<a name="ln25">#include &lt;condition_variable.h&gt;</a>
<a name="ln26">#include &lt;elf.h&gt;</a>
<a name="ln27">#include &lt;heap.h&gt;</a>
<a name="ln28">#include &lt;kernel.h&gt;</a>
<a name="ln29">#include &lt;low_resource_manager.h&gt;</a>
<a name="ln30">#include &lt;thread.h&gt;</a>
<a name="ln31">#include &lt;tracing.h&gt;</a>
<a name="ln32">#include &lt;util/AutoLock.h&gt;</a>
<a name="ln33">#include &lt;vfs.h&gt;</a>
<a name="ln34">#include &lt;vm/vm.h&gt;</a>
<a name="ln35">#include &lt;vm/vm_priv.h&gt;</a>
<a name="ln36">#include &lt;vm/vm_page.h&gt;</a>
<a name="ln37">#include &lt;vm/VMAddressSpace.h&gt;</a>
<a name="ln38">#include &lt;vm/VMArea.h&gt;</a>
<a name="ln39">#include &lt;vm/VMCache.h&gt;</a>
<a name="ln40"> </a>
<a name="ln41">#include &quot;IORequest.h&quot;</a>
<a name="ln42">#include &quot;PageCacheLocker.h&quot;</a>
<a name="ln43">#include &quot;VMAnonymousCache.h&quot;</a>
<a name="ln44">#include &quot;VMPageQueue.h&quot;</a>
<a name="ln45"> </a>
<a name="ln46"> </a>
<a name="ln47">//#define TRACE_VM_PAGE</a>
<a name="ln48">#ifdef TRACE_VM_PAGE</a>
<a name="ln49">#	define TRACE(x) dprintf x</a>
<a name="ln50">#else</a>
<a name="ln51">#	define TRACE(x) ;</a>
<a name="ln52">#endif</a>
<a name="ln53"> </a>
<a name="ln54">//#define TRACE_VM_DAEMONS</a>
<a name="ln55">#ifdef TRACE_VM_DAEMONS</a>
<a name="ln56">#define TRACE_DAEMON(x...) dprintf(x)</a>
<a name="ln57">#else</a>
<a name="ln58">#define TRACE_DAEMON(x...) do {} while (false)</a>
<a name="ln59">#endif</a>
<a name="ln60"> </a>
<a name="ln61">//#define TRACK_PAGE_USAGE_STATS	1</a>
<a name="ln62"> </a>
<a name="ln63">#define PAGE_ASSERT(page, condition)	\</a>
<a name="ln64">	ASSERT_PRINT((condition), &quot;page: %p&quot;, (page))</a>
<a name="ln65"> </a>
<a name="ln66">#define SCRUB_SIZE 16</a>
<a name="ln67">	// this many pages will be cleared at once in the page scrubber thread</a>
<a name="ln68"> </a>
<a name="ln69">#define MAX_PAGE_WRITER_IO_PRIORITY				B_URGENT_DISPLAY_PRIORITY</a>
<a name="ln70">	// maximum I/O priority of the page writer</a>
<a name="ln71">#define MAX_PAGE_WRITER_IO_PRIORITY_THRESHOLD	10000</a>
<a name="ln72">	// the maximum I/O priority shall be reached when this many pages need to</a>
<a name="ln73">	// be written</a>
<a name="ln74"> </a>
<a name="ln75"> </a>
<a name="ln76">// The page reserve an allocation of the certain priority must not touch.</a>
<a name="ln77">static const size_t kPageReserveForPriority[] = {</a>
<a name="ln78">	VM_PAGE_RESERVE_USER,		// user</a>
<a name="ln79">	VM_PAGE_RESERVE_SYSTEM,		// system</a>
<a name="ln80">	0							// VIP</a>
<a name="ln81">};</a>
<a name="ln82"> </a>
<a name="ln83">// Minimum number of free pages the page daemon will try to achieve.</a>
<a name="ln84">static uint32 sFreePagesTarget;</a>
<a name="ln85">static uint32 sFreeOrCachedPagesTarget;</a>
<a name="ln86">static uint32 sInactivePagesTarget;</a>
<a name="ln87"> </a>
<a name="ln88">// Wait interval between page daemon runs.</a>
<a name="ln89">static const bigtime_t kIdleScanWaitInterval = 1000000LL;	// 1 sec</a>
<a name="ln90">static const bigtime_t kBusyScanWaitInterval = 500000LL;	// 0.5 sec</a>
<a name="ln91"> </a>
<a name="ln92">// Number of idle runs after which we want to have processed the full active</a>
<a name="ln93">// queue.</a>
<a name="ln94">static const uint32 kIdleRunsForFullQueue = 20;</a>
<a name="ln95"> </a>
<a name="ln96">// Maximum limit for the vm_page::usage_count.</a>
<a name="ln97">static const int32 kPageUsageMax = 64;</a>
<a name="ln98">// vm_page::usage_count buff an accessed page receives in a scan.</a>
<a name="ln99">static const int32 kPageUsageAdvance = 3;</a>
<a name="ln100">// vm_page::usage_count debuff an unaccessed page receives in a scan.</a>
<a name="ln101">static const int32 kPageUsageDecline = 1;</a>
<a name="ln102"> </a>
<a name="ln103">int32 gMappedPagesCount;</a>
<a name="ln104"> </a>
<a name="ln105">static VMPageQueue sPageQueues[PAGE_STATE_COUNT];</a>
<a name="ln106"> </a>
<a name="ln107">static VMPageQueue&amp; sFreePageQueue = sPageQueues[PAGE_STATE_FREE];</a>
<a name="ln108">static VMPageQueue&amp; sClearPageQueue = sPageQueues[PAGE_STATE_CLEAR];</a>
<a name="ln109">static VMPageQueue&amp; sModifiedPageQueue = sPageQueues[PAGE_STATE_MODIFIED];</a>
<a name="ln110">static VMPageQueue&amp; sInactivePageQueue = sPageQueues[PAGE_STATE_INACTIVE];</a>
<a name="ln111">static VMPageQueue&amp; sActivePageQueue = sPageQueues[PAGE_STATE_ACTIVE];</a>
<a name="ln112">static VMPageQueue&amp; sCachedPageQueue = sPageQueues[PAGE_STATE_CACHED];</a>
<a name="ln113"> </a>
<a name="ln114">static vm_page *sPages;</a>
<a name="ln115">static page_num_t sPhysicalPageOffset;</a>
<a name="ln116">static page_num_t sNumPages;</a>
<a name="ln117">static page_num_t sNonExistingPages;</a>
<a name="ln118">	// pages in the sPages array that aren't backed by physical memory</a>
<a name="ln119">static uint64 sIgnoredPages;</a>
<a name="ln120">	// pages of physical memory ignored by the boot loader (and thus not</a>
<a name="ln121">	// available here)</a>
<a name="ln122">static int32 sUnreservedFreePages;</a>
<a name="ln123">static int32 sUnsatisfiedPageReservations;</a>
<a name="ln124">static int32 sModifiedTemporaryPages;</a>
<a name="ln125"> </a>
<a name="ln126">static ConditionVariable sFreePageCondition;</a>
<a name="ln127">static mutex sPageDeficitLock = MUTEX_INITIALIZER(&quot;page deficit&quot;);</a>
<a name="ln128"> </a>
<a name="ln129">// This lock must be used whenever the free or clear page queues are changed.</a>
<a name="ln130">// If you need to work on both queues at the same time, you need to hold a write</a>
<a name="ln131">// lock, otherwise, a read lock suffices (each queue still has a spinlock to</a>
<a name="ln132">// guard against concurrent changes).</a>
<a name="ln133">static rw_lock sFreePageQueuesLock</a>
<a name="ln134">	= RW_LOCK_INITIALIZER(&quot;free/clear page queues&quot;);</a>
<a name="ln135"> </a>
<a name="ln136">#ifdef TRACK_PAGE_USAGE_STATS</a>
<a name="ln137">static page_num_t sPageUsageArrays[512];</a>
<a name="ln138">static page_num_t* sPageUsage = sPageUsageArrays;</a>
<a name="ln139">static page_num_t sPageUsagePageCount;</a>
<a name="ln140">static page_num_t* sNextPageUsage = sPageUsageArrays + 256;</a>
<a name="ln141">static page_num_t sNextPageUsagePageCount;</a>
<a name="ln142">#endif</a>
<a name="ln143"> </a>
<a name="ln144"> </a>
<a name="ln145">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln146"> </a>
<a name="ln147">struct caller_info {</a>
<a name="ln148">	addr_t		caller;</a>
<a name="ln149">	size_t		count;</a>
<a name="ln150">};</a>
<a name="ln151"> </a>
<a name="ln152">static const int32 kCallerInfoTableSize = 1024;</a>
<a name="ln153">static caller_info sCallerInfoTable[kCallerInfoTableSize];</a>
<a name="ln154">static int32 sCallerInfoCount = 0;</a>
<a name="ln155"> </a>
<a name="ln156">static caller_info* get_caller_info(addr_t caller);</a>
<a name="ln157"> </a>
<a name="ln158"> </a>
<a name="ln159">RANGE_MARKER_FUNCTION_PROTOTYPES(vm_page)</a>
<a name="ln160"> </a>
<a name="ln161">static const addr_t kVMPageCodeAddressRange[] = {</a>
<a name="ln162">	RANGE_MARKER_FUNCTION_ADDRESS_RANGE(vm_page)</a>
<a name="ln163">};</a>
<a name="ln164"> </a>
<a name="ln165">#endif</a>
<a name="ln166"> </a>
<a name="ln167"> </a>
<a name="ln168">RANGE_MARKER_FUNCTION_BEGIN(vm_page)</a>
<a name="ln169"> </a>
<a name="ln170"> </a>
<a name="ln171">struct page_stats {</a>
<a name="ln172">	int32	totalFreePages;</a>
<a name="ln173">	int32	unsatisfiedReservations;</a>
<a name="ln174">	int32	cachedPages;</a>
<a name="ln175">};</a>
<a name="ln176"> </a>
<a name="ln177"> </a>
<a name="ln178">struct PageReservationWaiter</a>
<a name="ln179">		: public DoublyLinkedListLinkImpl&lt;PageReservationWaiter&gt; {</a>
<a name="ln180">	Thread*	thread;</a>
<a name="ln181">	uint32	dontTouch;		// reserve not to touch</a>
<a name="ln182">	uint32	missing;		// pages missing for the reservation</a>
<a name="ln183">	int32	threadPriority;</a>
<a name="ln184"> </a>
<a name="ln185">	bool operator&lt;(const PageReservationWaiter&amp; other) const</a>
<a name="ln186">	{</a>
<a name="ln187">		// Implies an order by descending VM priority (ascending dontTouch)</a>
<a name="ln188">		// and (secondarily) descending thread priority.</a>
<a name="ln189">		if (dontTouch != other.dontTouch)</a>
<a name="ln190">			return dontTouch &lt; other.dontTouch;</a>
<a name="ln191">		return threadPriority &gt; other.threadPriority;</a>
<a name="ln192">	}</a>
<a name="ln193">};</a>
<a name="ln194"> </a>
<a name="ln195">typedef DoublyLinkedList&lt;PageReservationWaiter&gt; PageReservationWaiterList;</a>
<a name="ln196">static PageReservationWaiterList sPageReservationWaiters;</a>
<a name="ln197"> </a>
<a name="ln198"> </a>
<a name="ln199">struct DaemonCondition {</a>
<a name="ln200">	void Init(const char* name)</a>
<a name="ln201">	{</a>
<a name="ln202">		mutex_init(&amp;fLock, &quot;daemon condition&quot;);</a>
<a name="ln203">		fCondition.Init(this, name);</a>
<a name="ln204">		fActivated = false;</a>
<a name="ln205">	}</a>
<a name="ln206"> </a>
<a name="ln207">	bool Lock()</a>
<a name="ln208">	{</a>
<a name="ln209">		return mutex_lock(&amp;fLock) == B_OK;</a>
<a name="ln210">	}</a>
<a name="ln211"> </a>
<a name="ln212">	void Unlock()</a>
<a name="ln213">	{</a>
<a name="ln214">		mutex_unlock(&amp;fLock);</a>
<a name="ln215">	}</a>
<a name="ln216"> </a>
<a name="ln217">	bool Wait(bigtime_t timeout, bool clearActivated)</a>
<a name="ln218">	{</a>
<a name="ln219">		MutexLocker locker(fLock);</a>
<a name="ln220">		if (clearActivated)</a>
<a name="ln221">			fActivated = false;</a>
<a name="ln222">		else if (fActivated)</a>
<a name="ln223">			return true;</a>
<a name="ln224"> </a>
<a name="ln225">		ConditionVariableEntry entry;</a>
<a name="ln226">		fCondition.Add(&amp;entry);</a>
<a name="ln227"> </a>
<a name="ln228">		locker.Unlock();</a>
<a name="ln229"> </a>
<a name="ln230">		return entry.Wait(B_RELATIVE_TIMEOUT, timeout) == B_OK;</a>
<a name="ln231">	}</a>
<a name="ln232"> </a>
<a name="ln233">	void WakeUp()</a>
<a name="ln234">	{</a>
<a name="ln235">		if (fActivated)</a>
<a name="ln236">			return;</a>
<a name="ln237"> </a>
<a name="ln238">		MutexLocker locker(fLock);</a>
<a name="ln239">		fActivated = true;</a>
<a name="ln240">		fCondition.NotifyOne();</a>
<a name="ln241">	}</a>
<a name="ln242"> </a>
<a name="ln243">	void ClearActivated()</a>
<a name="ln244">	{</a>
<a name="ln245">		MutexLocker locker(fLock);</a>
<a name="ln246">		fActivated = false;</a>
<a name="ln247">	}</a>
<a name="ln248"> </a>
<a name="ln249">private:</a>
<a name="ln250">	mutex				fLock;</a>
<a name="ln251">	ConditionVariable	fCondition;</a>
<a name="ln252">	bool				fActivated;</a>
<a name="ln253">};</a>
<a name="ln254"> </a>
<a name="ln255"> </a>
<a name="ln256">static DaemonCondition sPageWriterCondition;</a>
<a name="ln257">static DaemonCondition sPageDaemonCondition;</a>
<a name="ln258"> </a>
<a name="ln259"> </a>
<a name="ln260">#if PAGE_ALLOCATION_TRACING</a>
<a name="ln261"> </a>
<a name="ln262">namespace PageAllocationTracing {</a>
<a name="ln263"> </a>
<a name="ln264">class ReservePages : public AbstractTraceEntry {</a>
<a name="ln265">public:</a>
<a name="ln266">	ReservePages(uint32 count)</a>
<a name="ln267">		:</a>
<a name="ln268">		fCount(count)</a>
<a name="ln269">	{</a>
<a name="ln270">		Initialized();</a>
<a name="ln271">	}</a>
<a name="ln272"> </a>
<a name="ln273">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln274">	{</a>
<a name="ln275">		out.Print(&quot;page reserve:   %&quot; B_PRIu32, fCount);</a>
<a name="ln276">	}</a>
<a name="ln277"> </a>
<a name="ln278">private:</a>
<a name="ln279">	uint32		fCount;</a>
<a name="ln280">};</a>
<a name="ln281"> </a>
<a name="ln282"> </a>
<a name="ln283">class UnreservePages : public AbstractTraceEntry {</a>
<a name="ln284">public:</a>
<a name="ln285">	UnreservePages(uint32 count)</a>
<a name="ln286">		:</a>
<a name="ln287">		fCount(count)</a>
<a name="ln288">	{</a>
<a name="ln289">		Initialized();</a>
<a name="ln290">	}</a>
<a name="ln291"> </a>
<a name="ln292">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln293">	{</a>
<a name="ln294">		out.Print(&quot;page unreserve: %&quot; B_PRId32, fCount);</a>
<a name="ln295">	}</a>
<a name="ln296"> </a>
<a name="ln297">private:</a>
<a name="ln298">	uint32		fCount;</a>
<a name="ln299">};</a>
<a name="ln300"> </a>
<a name="ln301"> </a>
<a name="ln302">class AllocatePage</a>
<a name="ln303">	: public TRACE_ENTRY_SELECTOR(PAGE_ALLOCATION_TRACING_STACK_TRACE) {</a>
<a name="ln304">public:</a>
<a name="ln305">	AllocatePage(page_num_t pageNumber)</a>
<a name="ln306">		:</a>
<a name="ln307">		TraceEntryBase(PAGE_ALLOCATION_TRACING_STACK_TRACE, 0, true),</a>
<a name="ln308">		fPageNumber(pageNumber)</a>
<a name="ln309">	{</a>
<a name="ln310">		Initialized();</a>
<a name="ln311">	}</a>
<a name="ln312"> </a>
<a name="ln313">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln314">	{</a>
<a name="ln315">		out.Print(&quot;page alloc: %#&quot; B_PRIxPHYSADDR, fPageNumber);</a>
<a name="ln316">	}</a>
<a name="ln317"> </a>
<a name="ln318">private:</a>
<a name="ln319">	page_num_t	fPageNumber;</a>
<a name="ln320">};</a>
<a name="ln321"> </a>
<a name="ln322"> </a>
<a name="ln323">class AllocatePageRun</a>
<a name="ln324">	: public TRACE_ENTRY_SELECTOR(PAGE_ALLOCATION_TRACING_STACK_TRACE) {</a>
<a name="ln325">public:</a>
<a name="ln326">	AllocatePageRun(page_num_t startPage, uint32 length)</a>
<a name="ln327">		:</a>
<a name="ln328">		TraceEntryBase(PAGE_ALLOCATION_TRACING_STACK_TRACE, 0, true),</a>
<a name="ln329">		fStartPage(startPage),</a>
<a name="ln330">		fLength(length)</a>
<a name="ln331">	{</a>
<a name="ln332">		Initialized();</a>
<a name="ln333">	}</a>
<a name="ln334"> </a>
<a name="ln335">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln336">	{</a>
<a name="ln337">		out.Print(&quot;page alloc run: start %#&quot; B_PRIxPHYSADDR &quot; length: %&quot;</a>
<a name="ln338">			B_PRIu32, fStartPage, fLength);</a>
<a name="ln339">	}</a>
<a name="ln340"> </a>
<a name="ln341">private:</a>
<a name="ln342">	page_num_t	fStartPage;</a>
<a name="ln343">	uint32		fLength;</a>
<a name="ln344">};</a>
<a name="ln345"> </a>
<a name="ln346"> </a>
<a name="ln347">class FreePage</a>
<a name="ln348">	: public TRACE_ENTRY_SELECTOR(PAGE_ALLOCATION_TRACING_STACK_TRACE) {</a>
<a name="ln349">public:</a>
<a name="ln350">	FreePage(page_num_t pageNumber)</a>
<a name="ln351">		:</a>
<a name="ln352">		TraceEntryBase(PAGE_ALLOCATION_TRACING_STACK_TRACE, 0, true),</a>
<a name="ln353">		fPageNumber(pageNumber)</a>
<a name="ln354">	{</a>
<a name="ln355">		Initialized();</a>
<a name="ln356">	}</a>
<a name="ln357"> </a>
<a name="ln358">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln359">	{</a>
<a name="ln360">		out.Print(&quot;page free: %#&quot; B_PRIxPHYSADDR, fPageNumber);</a>
<a name="ln361">	}</a>
<a name="ln362"> </a>
<a name="ln363">private:</a>
<a name="ln364">	page_num_t	fPageNumber;</a>
<a name="ln365">};</a>
<a name="ln366"> </a>
<a name="ln367"> </a>
<a name="ln368">class ScrubbingPages : public AbstractTraceEntry {</a>
<a name="ln369">public:</a>
<a name="ln370">	ScrubbingPages(uint32 count)</a>
<a name="ln371">		:</a>
<a name="ln372">		fCount(count)</a>
<a name="ln373">	{</a>
<a name="ln374">		Initialized();</a>
<a name="ln375">	}</a>
<a name="ln376"> </a>
<a name="ln377">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln378">	{</a>
<a name="ln379">		out.Print(&quot;page scrubbing: %&quot; B_PRId32, fCount);</a>
<a name="ln380">	}</a>
<a name="ln381"> </a>
<a name="ln382">private:</a>
<a name="ln383">	uint32		fCount;</a>
<a name="ln384">};</a>
<a name="ln385"> </a>
<a name="ln386"> </a>
<a name="ln387">class ScrubbedPages : public AbstractTraceEntry {</a>
<a name="ln388">public:</a>
<a name="ln389">	ScrubbedPages(uint32 count)</a>
<a name="ln390">		:</a>
<a name="ln391">		fCount(count)</a>
<a name="ln392">	{</a>
<a name="ln393">		Initialized();</a>
<a name="ln394">	}</a>
<a name="ln395"> </a>
<a name="ln396">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln397">	{</a>
<a name="ln398">		out.Print(&quot;page scrubbed:  %&quot; B_PRId32, fCount);</a>
<a name="ln399">	}</a>
<a name="ln400"> </a>
<a name="ln401">private:</a>
<a name="ln402">	uint32		fCount;</a>
<a name="ln403">};</a>
<a name="ln404"> </a>
<a name="ln405"> </a>
<a name="ln406">class StolenPage : public AbstractTraceEntry {</a>
<a name="ln407">public:</a>
<a name="ln408">	StolenPage()</a>
<a name="ln409">	{</a>
<a name="ln410">		Initialized();</a>
<a name="ln411">	}</a>
<a name="ln412"> </a>
<a name="ln413">	virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln414">	{</a>
<a name="ln415">		out.Print(&quot;page stolen&quot;);</a>
<a name="ln416">	}</a>
<a name="ln417">};</a>
<a name="ln418"> </a>
<a name="ln419">}	// namespace PageAllocationTracing</a>
<a name="ln420"> </a>
<a name="ln421">#	define TA(x)	new(std::nothrow) PageAllocationTracing::x</a>
<a name="ln422"> </a>
<a name="ln423">#else</a>
<a name="ln424">#	define TA(x)</a>
<a name="ln425">#endif	// PAGE_ALLOCATION_TRACING</a>
<a name="ln426"> </a>
<a name="ln427"> </a>
<a name="ln428">#if PAGE_DAEMON_TRACING</a>
<a name="ln429"> </a>
<a name="ln430">namespace PageDaemonTracing {</a>
<a name="ln431"> </a>
<a name="ln432">class ActivatePage : public AbstractTraceEntry {</a>
<a name="ln433">	public:</a>
<a name="ln434">		ActivatePage(vm_page* page)</a>
<a name="ln435">			:</a>
<a name="ln436">			fCache(page-&gt;cache),</a>
<a name="ln437">			fPage(page)</a>
<a name="ln438">		{</a>
<a name="ln439">			Initialized();</a>
<a name="ln440">		}</a>
<a name="ln441"> </a>
<a name="ln442">		virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln443">		{</a>
<a name="ln444">			out.Print(&quot;page activated:   %p, cache: %p&quot;, fPage, fCache);</a>
<a name="ln445">		}</a>
<a name="ln446"> </a>
<a name="ln447">	private:</a>
<a name="ln448">		VMCache*	fCache;</a>
<a name="ln449">		vm_page*	fPage;</a>
<a name="ln450">};</a>
<a name="ln451"> </a>
<a name="ln452"> </a>
<a name="ln453">class DeactivatePage : public AbstractTraceEntry {</a>
<a name="ln454">	public:</a>
<a name="ln455">		DeactivatePage(vm_page* page)</a>
<a name="ln456">			:</a>
<a name="ln457">			fCache(page-&gt;cache),</a>
<a name="ln458">			fPage(page)</a>
<a name="ln459">		{</a>
<a name="ln460">			Initialized();</a>
<a name="ln461">		}</a>
<a name="ln462"> </a>
<a name="ln463">		virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln464">		{</a>
<a name="ln465">			out.Print(&quot;page deactivated: %p, cache: %p&quot;, fPage, fCache);</a>
<a name="ln466">		}</a>
<a name="ln467"> </a>
<a name="ln468">	private:</a>
<a name="ln469">		VMCache*	fCache;</a>
<a name="ln470">		vm_page*	fPage;</a>
<a name="ln471">};</a>
<a name="ln472"> </a>
<a name="ln473"> </a>
<a name="ln474">class FreedPageSwap : public AbstractTraceEntry {</a>
<a name="ln475">	public:</a>
<a name="ln476">		FreedPageSwap(vm_page* page)</a>
<a name="ln477">			:</a>
<a name="ln478">			fCache(page-&gt;cache),</a>
<a name="ln479">			fPage(page)</a>
<a name="ln480">		{</a>
<a name="ln481">			Initialized();</a>
<a name="ln482">		}</a>
<a name="ln483"> </a>
<a name="ln484">		virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln485">		{</a>
<a name="ln486">			out.Print(&quot;page swap freed:  %p, cache: %p&quot;, fPage, fCache);</a>
<a name="ln487">		}</a>
<a name="ln488"> </a>
<a name="ln489">	private:</a>
<a name="ln490">		VMCache*	fCache;</a>
<a name="ln491">		vm_page*	fPage;</a>
<a name="ln492">};</a>
<a name="ln493"> </a>
<a name="ln494">}	// namespace PageDaemonTracing</a>
<a name="ln495"> </a>
<a name="ln496">#	define TD(x)	new(std::nothrow) PageDaemonTracing::x</a>
<a name="ln497"> </a>
<a name="ln498">#else</a>
<a name="ln499">#	define TD(x)</a>
<a name="ln500">#endif	// PAGE_DAEMON_TRACING</a>
<a name="ln501"> </a>
<a name="ln502"> </a>
<a name="ln503">#if PAGE_WRITER_TRACING</a>
<a name="ln504"> </a>
<a name="ln505">namespace PageWriterTracing {</a>
<a name="ln506"> </a>
<a name="ln507">class WritePage : public AbstractTraceEntry {</a>
<a name="ln508">	public:</a>
<a name="ln509">		WritePage(vm_page* page)</a>
<a name="ln510">			:</a>
<a name="ln511">			fCache(page-&gt;Cache()),</a>
<a name="ln512">			fPage(page)</a>
<a name="ln513">		{</a>
<a name="ln514">			Initialized();</a>
<a name="ln515">		}</a>
<a name="ln516"> </a>
<a name="ln517">		virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln518">		{</a>
<a name="ln519">			out.Print(&quot;page write: %p, cache: %p&quot;, fPage, fCache);</a>
<a name="ln520">		}</a>
<a name="ln521"> </a>
<a name="ln522">	private:</a>
<a name="ln523">		VMCache*	fCache;</a>
<a name="ln524">		vm_page*	fPage;</a>
<a name="ln525">};</a>
<a name="ln526"> </a>
<a name="ln527">}	// namespace PageWriterTracing</a>
<a name="ln528"> </a>
<a name="ln529">#	define TPW(x)	new(std::nothrow) PageWriterTracing::x</a>
<a name="ln530"> </a>
<a name="ln531">#else</a>
<a name="ln532">#	define TPW(x)</a>
<a name="ln533">#endif	// PAGE_WRITER_TRACING</a>
<a name="ln534"> </a>
<a name="ln535"> </a>
<a name="ln536">#if PAGE_STATE_TRACING</a>
<a name="ln537"> </a>
<a name="ln538">namespace PageStateTracing {</a>
<a name="ln539"> </a>
<a name="ln540">class SetPageState : public AbstractTraceEntry {</a>
<a name="ln541">	public:</a>
<a name="ln542">		SetPageState(vm_page* page, uint8 newState)</a>
<a name="ln543">			:</a>
<a name="ln544">			fPage(page),</a>
<a name="ln545">			fOldState(page-&gt;State()),</a>
<a name="ln546">			fNewState(newState),</a>
<a name="ln547">			fBusy(page-&gt;busy),</a>
<a name="ln548">			fWired(page-&gt;WiredCount() &gt; 0),</a>
<a name="ln549">			fMapped(!page-&gt;mappings.IsEmpty()),</a>
<a name="ln550">			fAccessed(page-&gt;accessed),</a>
<a name="ln551">			fModified(page-&gt;modified)</a>
<a name="ln552">		{</a>
<a name="ln553">#if PAGE_STATE_TRACING_STACK_TRACE</a>
<a name="ln554">			fStackTrace = capture_tracing_stack_trace(</a>
<a name="ln555">				PAGE_STATE_TRACING_STACK_TRACE, 0, true);</a>
<a name="ln556">				// Don't capture userland stack trace to avoid potential</a>
<a name="ln557">				// deadlocks.</a>
<a name="ln558">#endif</a>
<a name="ln559">			Initialized();</a>
<a name="ln560">		}</a>
<a name="ln561"> </a>
<a name="ln562">#if PAGE_STATE_TRACING_STACK_TRACE</a>
<a name="ln563">		virtual void DumpStackTrace(TraceOutput&amp; out)</a>
<a name="ln564">		{</a>
<a name="ln565">			out.PrintStackTrace(fStackTrace);</a>
<a name="ln566">		}</a>
<a name="ln567">#endif</a>
<a name="ln568"> </a>
<a name="ln569">		virtual void AddDump(TraceOutput&amp; out)</a>
<a name="ln570">		{</a>
<a name="ln571">			out.Print(&quot;page set state: %p (%c%c%c%c%c): %s -&gt; %s&quot;, fPage,</a>
<a name="ln572">				fBusy ? 'b' : '-',</a>
<a name="ln573">				fWired ? 'w' : '-',</a>
<a name="ln574">				fMapped ? 'm' : '-',</a>
<a name="ln575">				fAccessed ? 'a' : '-',</a>
<a name="ln576">				fModified ? 'm' : '-',</a>
<a name="ln577">				page_state_to_string(fOldState),</a>
<a name="ln578">				page_state_to_string(fNewState));</a>
<a name="ln579">		}</a>
<a name="ln580"> </a>
<a name="ln581">	private:</a>
<a name="ln582">		vm_page*	fPage;</a>
<a name="ln583">#if PAGE_STATE_TRACING_STACK_TRACE</a>
<a name="ln584">		tracing_stack_trace* fStackTrace;</a>
<a name="ln585">#endif</a>
<a name="ln586">		uint8		fOldState;</a>
<a name="ln587">		uint8		fNewState;</a>
<a name="ln588">		bool		fBusy : 1;</a>
<a name="ln589">		bool		fWired : 1;</a>
<a name="ln590">		bool		fMapped : 1;</a>
<a name="ln591">		bool		fAccessed : 1;</a>
<a name="ln592">		bool		fModified : 1;</a>
<a name="ln593">};</a>
<a name="ln594"> </a>
<a name="ln595">}	// namespace PageStateTracing</a>
<a name="ln596"> </a>
<a name="ln597">#	define TPS(x)	new(std::nothrow) PageStateTracing::x</a>
<a name="ln598"> </a>
<a name="ln599">#else</a>
<a name="ln600">#	define TPS(x)</a>
<a name="ln601">#endif	// PAGE_STATE_TRACING</a>
<a name="ln602"> </a>
<a name="ln603"> </a>
<a name="ln604">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln605"> </a>
<a name="ln606">namespace BKernel {</a>
<a name="ln607"> </a>
<a name="ln608">class AllocationTrackingCallback {</a>
<a name="ln609">public:</a>
<a name="ln610">	virtual						~AllocationTrackingCallback();</a>
<a name="ln611"> </a>
<a name="ln612">	virtual	bool				ProcessTrackingInfo(</a>
<a name="ln613">									AllocationTrackingInfo* info,</a>
<a name="ln614">									page_num_t pageNumber) = 0;</a>
<a name="ln615">};</a>
<a name="ln616"> </a>
<a name="ln617">}</a>
<a name="ln618"> </a>
<a name="ln619">using BKernel::AllocationTrackingCallback;</a>
<a name="ln620"> </a>
<a name="ln621"> </a>
<a name="ln622">class AllocationCollectorCallback : public AllocationTrackingCallback {</a>
<a name="ln623">public:</a>
<a name="ln624">	AllocationCollectorCallback(bool resetInfos)</a>
<a name="ln625">		:</a>
<a name="ln626">		fResetInfos(resetInfos)</a>
<a name="ln627">	{</a>
<a name="ln628">	}</a>
<a name="ln629"> </a>
<a name="ln630">	virtual bool ProcessTrackingInfo(AllocationTrackingInfo* info,</a>
<a name="ln631">		page_num_t pageNumber)</a>
<a name="ln632">	{</a>
<a name="ln633">		if (!info-&gt;IsInitialized())</a>
<a name="ln634">			return true;</a>
<a name="ln635"> </a>
<a name="ln636">		addr_t caller = 0;</a>
<a name="ln637">		AbstractTraceEntryWithStackTrace* traceEntry = info-&gt;TraceEntry();</a>
<a name="ln638"> </a>
<a name="ln639">		if (traceEntry != NULL &amp;&amp; info-&gt;IsTraceEntryValid()) {</a>
<a name="ln640">			caller = tracing_find_caller_in_stack_trace(</a>
<a name="ln641">				traceEntry-&gt;StackTrace(), kVMPageCodeAddressRange, 1);</a>
<a name="ln642">		}</a>
<a name="ln643"> </a>
<a name="ln644">		caller_info* callerInfo = get_caller_info(caller);</a>
<a name="ln645">		if (callerInfo == NULL) {</a>
<a name="ln646">			kprintf(&quot;out of space for caller infos\n&quot;);</a>
<a name="ln647">			return false;</a>
<a name="ln648">		}</a>
<a name="ln649"> </a>
<a name="ln650">		callerInfo-&gt;count++;</a>
<a name="ln651"> </a>
<a name="ln652">		if (fResetInfos)</a>
<a name="ln653">			info-&gt;Clear();</a>
<a name="ln654"> </a>
<a name="ln655">		return true;</a>
<a name="ln656">	}</a>
<a name="ln657"> </a>
<a name="ln658">private:</a>
<a name="ln659">	bool	fResetInfos;</a>
<a name="ln660">};</a>
<a name="ln661"> </a>
<a name="ln662"> </a>
<a name="ln663">class AllocationInfoPrinterCallback : public AllocationTrackingCallback {</a>
<a name="ln664">public:</a>
<a name="ln665">	AllocationInfoPrinterCallback(bool printStackTrace, page_num_t pageFilter,</a>
<a name="ln666">		team_id teamFilter, thread_id threadFilter)</a>
<a name="ln667">		:</a>
<a name="ln668">		fPrintStackTrace(printStackTrace),</a>
<a name="ln669">		fPageFilter(pageFilter),</a>
<a name="ln670">		fTeamFilter(teamFilter),</a>
<a name="ln671">		fThreadFilter(threadFilter)</a>
<a name="ln672">	{</a>
<a name="ln673">	}</a>
<a name="ln674"> </a>
<a name="ln675">	virtual bool ProcessTrackingInfo(AllocationTrackingInfo* info,</a>
<a name="ln676">		page_num_t pageNumber)</a>
<a name="ln677">	{</a>
<a name="ln678">		if (!info-&gt;IsInitialized())</a>
<a name="ln679">			return true;</a>
<a name="ln680"> </a>
<a name="ln681">		if (fPageFilter != 0 &amp;&amp; pageNumber != fPageFilter)</a>
<a name="ln682">			return true;</a>
<a name="ln683"> </a>
<a name="ln684">		AbstractTraceEntryWithStackTrace* traceEntry = info-&gt;TraceEntry();</a>
<a name="ln685">		if (traceEntry != NULL &amp;&amp; !info-&gt;IsTraceEntryValid())</a>
<a name="ln686">			traceEntry = NULL;</a>
<a name="ln687"> </a>
<a name="ln688">		if (traceEntry != NULL) {</a>
<a name="ln689">			if (fTeamFilter != -1 &amp;&amp; traceEntry-&gt;TeamID() != fTeamFilter)</a>
<a name="ln690">				return true;</a>
<a name="ln691">			if (fThreadFilter != -1 &amp;&amp; traceEntry-&gt;ThreadID() != fThreadFilter)</a>
<a name="ln692">				return true;</a>
<a name="ln693">		} else {</a>
<a name="ln694">			// we need the info if we have filters set</a>
<a name="ln695">			if (fTeamFilter != -1 || fThreadFilter != -1)</a>
<a name="ln696">				return true;</a>
<a name="ln697">		}</a>
<a name="ln698"> </a>
<a name="ln699">		kprintf(&quot;page number %#&quot; B_PRIxPHYSADDR, pageNumber);</a>
<a name="ln700"> </a>
<a name="ln701">		if (traceEntry != NULL) {</a>
<a name="ln702">			kprintf(&quot;, team: %&quot; B_PRId32 &quot;, thread %&quot; B_PRId32</a>
<a name="ln703">				&quot;, time %&quot; B_PRId64 &quot;\n&quot;, traceEntry-&gt;TeamID(),</a>
<a name="ln704">				traceEntry-&gt;ThreadID(), traceEntry-&gt;Time());</a>
<a name="ln705"> </a>
<a name="ln706">			if (fPrintStackTrace)</a>
<a name="ln707">				tracing_print_stack_trace(traceEntry-&gt;StackTrace());</a>
<a name="ln708">		} else</a>
<a name="ln709">			kprintf(&quot;\n&quot;);</a>
<a name="ln710"> </a>
<a name="ln711">		return true;</a>
<a name="ln712">	}</a>
<a name="ln713"> </a>
<a name="ln714">private:</a>
<a name="ln715">	bool		fPrintStackTrace;</a>
<a name="ln716">	page_num_t	fPageFilter;</a>
<a name="ln717">	team_id		fTeamFilter;</a>
<a name="ln718">	thread_id	fThreadFilter;</a>
<a name="ln719">};</a>
<a name="ln720"> </a>
<a name="ln721"> </a>
<a name="ln722">class AllocationDetailPrinterCallback : public AllocationTrackingCallback {</a>
<a name="ln723">public:</a>
<a name="ln724">	AllocationDetailPrinterCallback(addr_t caller)</a>
<a name="ln725">		:</a>
<a name="ln726">		fCaller(caller)</a>
<a name="ln727">	{</a>
<a name="ln728">	}</a>
<a name="ln729"> </a>
<a name="ln730">	virtual bool ProcessTrackingInfo(AllocationTrackingInfo* info,</a>
<a name="ln731">		page_num_t pageNumber)</a>
<a name="ln732">	{</a>
<a name="ln733">		if (!info-&gt;IsInitialized())</a>
<a name="ln734">			return true;</a>
<a name="ln735"> </a>
<a name="ln736">		addr_t caller = 0;</a>
<a name="ln737">		AbstractTraceEntryWithStackTrace* traceEntry = info-&gt;TraceEntry();</a>
<a name="ln738">		if (traceEntry != NULL &amp;&amp; !info-&gt;IsTraceEntryValid())</a>
<a name="ln739">			traceEntry = NULL;</a>
<a name="ln740"> </a>
<a name="ln741">		if (traceEntry != NULL) {</a>
<a name="ln742">			caller = tracing_find_caller_in_stack_trace(</a>
<a name="ln743">				traceEntry-&gt;StackTrace(), kVMPageCodeAddressRange, 1);</a>
<a name="ln744">		}</a>
<a name="ln745"> </a>
<a name="ln746">		if (caller != fCaller)</a>
<a name="ln747">			return true;</a>
<a name="ln748"> </a>
<a name="ln749">		kprintf(&quot;page %#&quot; B_PRIxPHYSADDR &quot;\n&quot;, pageNumber);</a>
<a name="ln750">		if (traceEntry != NULL)</a>
<a name="ln751">			tracing_print_stack_trace(traceEntry-&gt;StackTrace());</a>
<a name="ln752"> </a>
<a name="ln753">		return true;</a>
<a name="ln754">	}</a>
<a name="ln755"> </a>
<a name="ln756">private:</a>
<a name="ln757">	addr_t	fCaller;</a>
<a name="ln758">};</a>
<a name="ln759"> </a>
<a name="ln760">#endif	// VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln761"> </a>
<a name="ln762"> </a>
<a name="ln763">static int</a>
<a name="ln764">find_page(int argc, char **argv)</a>
<a name="ln765">{</a>
<a name="ln766">	struct vm_page *page;</a>
<a name="ln767">	addr_t address;</a>
<a name="ln768">	int32 index = 1;</a>
<a name="ln769">	int i;</a>
<a name="ln770"> </a>
<a name="ln771">	struct {</a>
<a name="ln772">		const char*	name;</a>
<a name="ln773">		VMPageQueue*	queue;</a>
<a name="ln774">	} pageQueueInfos[] = {</a>
<a name="ln775">		{ &quot;free&quot;,		&amp;sFreePageQueue },</a>
<a name="ln776">		{ &quot;clear&quot;,		&amp;sClearPageQueue },</a>
<a name="ln777">		{ &quot;modified&quot;,	&amp;sModifiedPageQueue },</a>
<a name="ln778">		{ &quot;active&quot;,		&amp;sActivePageQueue },</a>
<a name="ln779">		{ &quot;inactive&quot;,	&amp;sInactivePageQueue },</a>
<a name="ln780">		{ &quot;cached&quot;,		&amp;sCachedPageQueue },</a>
<a name="ln781">		{ NULL, NULL }</a>
<a name="ln782">	};</a>
<a name="ln783"> </a>
<a name="ln784">	if (argc &lt; 2</a>
<a name="ln785">		|| strlen(argv[index]) &lt;= 2</a>
<a name="ln786">		|| argv[index][0] != '0'</a>
<a name="ln787">		|| argv[index][1] != 'x') {</a>
<a name="ln788">		kprintf(&quot;usage: find_page &lt;address&gt;\n&quot;);</a>
<a name="ln789">		return 0;</a>
<a name="ln790">	}</a>
<a name="ln791"> </a>
<a name="ln792">	address = strtoul(argv[index], NULL, 0);</a>
<a name="ln793">	page = (vm_page*)address;</a>
<a name="ln794"> </a>
<a name="ln795">	for (i = 0; pageQueueInfos[i].name; i++) {</a>
<a name="ln796">		VMPageQueue::Iterator it = pageQueueInfos[i].queue-&gt;GetIterator();</a>
<a name="ln797">		while (vm_page* p = it.Next()) {</a>
<a name="ln798">			if (p == page) {</a>
<a name="ln799">				kprintf(&quot;found page %p in queue %p (%s)\n&quot;, page,</a>
<a name="ln800">					pageQueueInfos[i].queue, pageQueueInfos[i].name);</a>
<a name="ln801">				return 0;</a>
<a name="ln802">			}</a>
<a name="ln803">		}</a>
<a name="ln804">	}</a>
<a name="ln805"> </a>
<a name="ln806">	kprintf(&quot;page %p isn't in any queue\n&quot;, page);</a>
<a name="ln807"> </a>
<a name="ln808">	return 0;</a>
<a name="ln809">}</a>
<a name="ln810"> </a>
<a name="ln811"> </a>
<a name="ln812">const char *</a>
<a name="ln813">page_state_to_string(int state)</a>
<a name="ln814">{</a>
<a name="ln815">	switch(state) {</a>
<a name="ln816">		case PAGE_STATE_ACTIVE:</a>
<a name="ln817">			return &quot;active&quot;;</a>
<a name="ln818">		case PAGE_STATE_INACTIVE:</a>
<a name="ln819">			return &quot;inactive&quot;;</a>
<a name="ln820">		case PAGE_STATE_MODIFIED:</a>
<a name="ln821">			return &quot;modified&quot;;</a>
<a name="ln822">		case PAGE_STATE_CACHED:</a>
<a name="ln823">			return &quot;cached&quot;;</a>
<a name="ln824">		case PAGE_STATE_FREE:</a>
<a name="ln825">			return &quot;free&quot;;</a>
<a name="ln826">		case PAGE_STATE_CLEAR:</a>
<a name="ln827">			return &quot;clear&quot;;</a>
<a name="ln828">		case PAGE_STATE_WIRED:</a>
<a name="ln829">			return &quot;wired&quot;;</a>
<a name="ln830">		case PAGE_STATE_UNUSED:</a>
<a name="ln831">			return &quot;unused&quot;;</a>
<a name="ln832">		default:</a>
<a name="ln833">			return &quot;unknown&quot;;</a>
<a name="ln834">	}</a>
<a name="ln835">}</a>
<a name="ln836"> </a>
<a name="ln837"> </a>
<a name="ln838">static int</a>
<a name="ln839">dump_page(int argc, char **argv)</a>
<a name="ln840">{</a>
<a name="ln841">	bool addressIsPointer = true;</a>
<a name="ln842">	bool physical = false;</a>
<a name="ln843">	bool searchMappings = false;</a>
<a name="ln844">	int32 index = 1;</a>
<a name="ln845"> </a>
<a name="ln846">	while (index &lt; argc) {</a>
<a name="ln847">		if (argv[index][0] != '-')</a>
<a name="ln848">			break;</a>
<a name="ln849"> </a>
<a name="ln850">		if (!strcmp(argv[index], &quot;-p&quot;)) {</a>
<a name="ln851">			addressIsPointer = false;</a>
<a name="ln852">			physical = true;</a>
<a name="ln853">		} else if (!strcmp(argv[index], &quot;-v&quot;)) {</a>
<a name="ln854">			addressIsPointer = false;</a>
<a name="ln855">		} else if (!strcmp(argv[index], &quot;-m&quot;)) {</a>
<a name="ln856">			searchMappings = true;</a>
<a name="ln857">		} else {</a>
<a name="ln858">			print_debugger_command_usage(argv[0]);</a>
<a name="ln859">			return 0;</a>
<a name="ln860">		}</a>
<a name="ln861"> </a>
<a name="ln862">		index++;</a>
<a name="ln863">	}</a>
<a name="ln864"> </a>
<a name="ln865">	if (index + 1 != argc) {</a>
<a name="ln866">		print_debugger_command_usage(argv[0]);</a>
<a name="ln867">		return 0;</a>
<a name="ln868">	}</a>
<a name="ln869"> </a>
<a name="ln870">	uint64 value;</a>
<a name="ln871">	if (!evaluate_debug_expression(argv[index], &amp;value, false))</a>
<a name="ln872">		return 0;</a>
<a name="ln873"> </a>
<a name="ln874">	uint64 pageAddress = value;</a>
<a name="ln875">	struct vm_page* page;</a>
<a name="ln876"> </a>
<a name="ln877">	if (addressIsPointer) {</a>
<a name="ln878">		page = (struct vm_page *)(addr_t)pageAddress;</a>
<a name="ln879">	} else {</a>
<a name="ln880">		if (!physical) {</a>
<a name="ln881">			VMAddressSpace *addressSpace = VMAddressSpace::Kernel();</a>
<a name="ln882"> </a>
<a name="ln883">			if (debug_get_debugged_thread()-&gt;team-&gt;address_space != NULL)</a>
<a name="ln884">				addressSpace = debug_get_debugged_thread()-&gt;team-&gt;address_space;</a>
<a name="ln885"> </a>
<a name="ln886">			uint32 flags = 0;</a>
<a name="ln887">			phys_addr_t physicalAddress;</a>
<a name="ln888">			if (addressSpace-&gt;TranslationMap()-&gt;QueryInterrupt(pageAddress,</a>
<a name="ln889">					&amp;physicalAddress, &amp;flags) != B_OK</a>
<a name="ln890">				|| (flags &amp; PAGE_PRESENT) == 0) {</a>
<a name="ln891">				kprintf(&quot;Virtual address not mapped to a physical page in this &quot;</a>
<a name="ln892">					&quot;address space.\n&quot;);</a>
<a name="ln893">				return 0;</a>
<a name="ln894">			}</a>
<a name="ln895">			pageAddress = physicalAddress;</a>
<a name="ln896">		}</a>
<a name="ln897"> </a>
<a name="ln898">		page = vm_lookup_page(pageAddress / B_PAGE_SIZE);</a>
<a name="ln899">	}</a>
<a name="ln900"> </a>
<a name="ln901">	kprintf(&quot;PAGE: %p\n&quot;, page);</a>
<a name="ln902">	kprintf(&quot;queue_next,prev: %p, %p\n&quot;, page-&gt;queue_link.next,</a>
<a name="ln903">		page-&gt;queue_link.previous);</a>
<a name="ln904">	kprintf(&quot;physical_number: %#&quot; B_PRIxPHYSADDR &quot;\n&quot;,</a>
<a name="ln905">		page-&gt;physical_page_number);</a>
<a name="ln906">	kprintf(&quot;cache:           %p\n&quot;, page-&gt;Cache());</a>
<a name="ln907">	kprintf(&quot;cache_offset:    %&quot; B_PRIuPHYSADDR &quot;\n&quot;, page-&gt;cache_offset);</a>
<a name="ln908">	kprintf(&quot;cache_next:      %p\n&quot;, page-&gt;cache_next);</a>
<a name="ln909">	kprintf(&quot;state:           %s\n&quot;, page_state_to_string(page-&gt;State()));</a>
<a name="ln910">	kprintf(&quot;wired_count:     %d\n&quot;, page-&gt;WiredCount());</a>
<a name="ln911">	kprintf(&quot;usage_count:     %d\n&quot;, page-&gt;usage_count);</a>
<a name="ln912">	kprintf(&quot;busy:            %d\n&quot;, page-&gt;busy);</a>
<a name="ln913">	kprintf(&quot;busy_writing:    %d\n&quot;, page-&gt;busy_writing);</a>
<a name="ln914">	kprintf(&quot;accessed:        %d\n&quot;, page-&gt;accessed);</a>
<a name="ln915">	kprintf(&quot;modified:        %d\n&quot;, page-&gt;modified);</a>
<a name="ln916">	#if DEBUG_PAGE_QUEUE</a>
<a name="ln917">		kprintf(&quot;queue:           %p\n&quot;, page-&gt;queue);</a>
<a name="ln918">	#endif</a>
<a name="ln919">	#if DEBUG_PAGE_ACCESS</a>
<a name="ln920">		kprintf(&quot;accessor:        %&quot; B_PRId32 &quot;\n&quot;, page-&gt;accessing_thread);</a>
<a name="ln921">	#endif</a>
<a name="ln922">	kprintf(&quot;area mappings:\n&quot;);</a>
<a name="ln923"> </a>
<a name="ln924">	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();</a>
<a name="ln925">	vm_page_mapping *mapping;</a>
<a name="ln926">	while ((mapping = iterator.Next()) != NULL) {</a>
<a name="ln927">		kprintf(&quot;  %p (%&quot; B_PRId32 &quot;)\n&quot;, mapping-&gt;area, mapping-&gt;area-&gt;id);</a>
<a name="ln928">		mapping = mapping-&gt;page_link.next;</a>
<a name="ln929">	}</a>
<a name="ln930"> </a>
<a name="ln931">	if (searchMappings) {</a>
<a name="ln932">		kprintf(&quot;all mappings:\n&quot;);</a>
<a name="ln933">		VMAddressSpace* addressSpace = VMAddressSpace::DebugFirst();</a>
<a name="ln934">		while (addressSpace != NULL) {</a>
<a name="ln935">			size_t pageCount = addressSpace-&gt;Size() / B_PAGE_SIZE;</a>
<a name="ln936">			for (addr_t address = addressSpace-&gt;Base(); pageCount != 0;</a>
<a name="ln937">					address += B_PAGE_SIZE, pageCount--) {</a>
<a name="ln938">				phys_addr_t physicalAddress;</a>
<a name="ln939">				uint32 flags = 0;</a>
<a name="ln940">				if (addressSpace-&gt;TranslationMap()-&gt;QueryInterrupt(address,</a>
<a name="ln941">						&amp;physicalAddress, &amp;flags) == B_OK</a>
<a name="ln942">					&amp;&amp; (flags &amp; PAGE_PRESENT) != 0</a>
<a name="ln943">					&amp;&amp; physicalAddress / B_PAGE_SIZE</a>
<a name="ln944">						== page-&gt;physical_page_number) {</a>
<a name="ln945">					VMArea* area = addressSpace-&gt;LookupArea(address);</a>
<a name="ln946">					kprintf(&quot;  aspace %&quot; B_PRId32 &quot;, area %&quot; B_PRId32 &quot;: %#&quot;</a>
<a name="ln947">						B_PRIxADDR &quot; (%c%c%s%s)\n&quot;, addressSpace-&gt;ID(),</a>
<a name="ln948">						area != NULL ? area-&gt;id : -1, address,</a>
<a name="ln949">						(flags &amp; B_KERNEL_READ_AREA) != 0 ? 'r' : '-',</a>
<a name="ln950">						(flags &amp; B_KERNEL_WRITE_AREA) != 0 ? 'w' : '-',</a>
<a name="ln951">						(flags &amp; PAGE_MODIFIED) != 0 ? &quot; modified&quot; : &quot;&quot;,</a>
<a name="ln952">						(flags &amp; PAGE_ACCESSED) != 0 ? &quot; accessed&quot; : &quot;&quot;);</a>
<a name="ln953">				}</a>
<a name="ln954">			}</a>
<a name="ln955">			addressSpace = VMAddressSpace::DebugNext(addressSpace);</a>
<a name="ln956">		}</a>
<a name="ln957">	}</a>
<a name="ln958"> </a>
<a name="ln959">	set_debug_variable(&quot;_cache&quot;, (addr_t)page-&gt;Cache());</a>
<a name="ln960">	#if DEBUG_PAGE_ACCESS</a>
<a name="ln961">		set_debug_variable(&quot;_accessor&quot;, page-&gt;accessing_thread);</a>
<a name="ln962">	#endif</a>
<a name="ln963"> </a>
<a name="ln964">	return 0;</a>
<a name="ln965">}</a>
<a name="ln966"> </a>
<a name="ln967"> </a>
<a name="ln968">static int</a>
<a name="ln969">dump_page_queue(int argc, char **argv)</a>
<a name="ln970">{</a>
<a name="ln971">	struct VMPageQueue *queue;</a>
<a name="ln972"> </a>
<a name="ln973">	if (argc &lt; 2) {</a>
<a name="ln974">		kprintf(&quot;usage: page_queue &lt;address/name&gt; [list]\n&quot;);</a>
<a name="ln975">		return 0;</a>
<a name="ln976">	}</a>
<a name="ln977"> </a>
<a name="ln978">	if (strlen(argv[1]) &gt;= 2 &amp;&amp; argv[1][0] == '0' &amp;&amp; argv[1][1] == 'x')</a>
<a name="ln979">		queue = (VMPageQueue*)strtoul(argv[1], NULL, 16);</a>
<a name="ln980">	else if (!strcmp(argv[1], &quot;free&quot;))</a>
<a name="ln981">		queue = &amp;sFreePageQueue;</a>
<a name="ln982">	else if (!strcmp(argv[1], &quot;clear&quot;))</a>
<a name="ln983">		queue = &amp;sClearPageQueue;</a>
<a name="ln984">	else if (!strcmp(argv[1], &quot;modified&quot;))</a>
<a name="ln985">		queue = &amp;sModifiedPageQueue;</a>
<a name="ln986">	else if (!strcmp(argv[1], &quot;active&quot;))</a>
<a name="ln987">		queue = &amp;sActivePageQueue;</a>
<a name="ln988">	else if (!strcmp(argv[1], &quot;inactive&quot;))</a>
<a name="ln989">		queue = &amp;sInactivePageQueue;</a>
<a name="ln990">	else if (!strcmp(argv[1], &quot;cached&quot;))</a>
<a name="ln991">		queue = &amp;sCachedPageQueue;</a>
<a name="ln992">	else {</a>
<a name="ln993">		kprintf(&quot;page_queue: unknown queue \&quot;%s\&quot;.\n&quot;, argv[1]);</a>
<a name="ln994">		return 0;</a>
<a name="ln995">	}</a>
<a name="ln996"> </a>
<a name="ln997">	kprintf(&quot;queue = %p, queue-&gt;head = %p, queue-&gt;tail = %p, queue-&gt;count = %&quot;</a>
<a name="ln998">		B_PRIuPHYSADDR &quot;\n&quot;, queue, queue-&gt;Head(), queue-&gt;Tail(),</a>
<a name="ln999">		queue-&gt;Count());</a>
<a name="ln1000"> </a>
<a name="ln1001">	if (argc == 3) {</a>
<a name="ln1002">		struct vm_page *page = queue-&gt;Head();</a>
<a name="ln1003"> </a>
<a name="ln1004">		kprintf(&quot;page        cache       type       state  wired  usage\n&quot;);</a>
<a name="ln1005">		for (page_num_t i = 0; page; i++, page = queue-&gt;Next(page)) {</a>
<a name="ln1006">			kprintf(&quot;%p  %p  %-7s %8s  %5d  %5d\n&quot;, page, page-&gt;Cache(),</a>
<a name="ln1007">				vm_cache_type_to_string(page-&gt;Cache()-&gt;type),</a>
<a name="ln1008">				page_state_to_string(page-&gt;State()),</a>
<a name="ln1009">				page-&gt;WiredCount(), page-&gt;usage_count);</a>
<a name="ln1010">		}</a>
<a name="ln1011">	}</a>
<a name="ln1012">	return 0;</a>
<a name="ln1013">}</a>
<a name="ln1014"> </a>
<a name="ln1015"> </a>
<a name="ln1016">static int</a>
<a name="ln1017">dump_page_stats(int argc, char **argv)</a>
<a name="ln1018">{</a>
<a name="ln1019">	page_num_t swappableModified = 0;</a>
<a name="ln1020">	page_num_t swappableModifiedInactive = 0;</a>
<a name="ln1021"> </a>
<a name="ln1022">	size_t counter[8];</a>
<a name="ln1023">	size_t busyCounter[8];</a>
<a name="ln1024">	memset(counter, 0, sizeof(counter));</a>
<a name="ln1025">	memset(busyCounter, 0, sizeof(busyCounter));</a>
<a name="ln1026"> </a>
<a name="ln1027">	struct page_run {</a>
<a name="ln1028">		page_num_t	start;</a>
<a name="ln1029">		page_num_t	end;</a>
<a name="ln1030"> </a>
<a name="ln1031">		page_num_t Length() const	{ return end - start; }</a>
<a name="ln1032">	};</a>
<a name="ln1033"> </a>
<a name="ln1034">	page_run currentFreeRun = { 0, 0 };</a>
<a name="ln1035">	page_run currentCachedRun = { 0, 0 };</a>
<a name="ln1036">	page_run longestFreeRun = { 0, 0 };</a>
<a name="ln1037">	page_run longestCachedRun = { 0, 0 };</a>
<a name="ln1038"> </a>
<a name="ln1039">	for (page_num_t i = 0; i &lt; sNumPages; i++) {</a>
<a name="ln1040">		if (sPages[i].State() &gt; 7) {</a>
<a name="ln1041">			panic(&quot;page %&quot; B_PRIuPHYSADDR &quot; at %p has invalid state!\n&quot;, i,</a>
<a name="ln1042">				&amp;sPages[i]);</a>
<a name="ln1043">		}</a>
<a name="ln1044"> </a>
<a name="ln1045">		uint32 pageState = sPages[i].State();</a>
<a name="ln1046"> </a>
<a name="ln1047">		counter[pageState]++;</a>
<a name="ln1048">		if (sPages[i].busy)</a>
<a name="ln1049">			busyCounter[pageState]++;</a>
<a name="ln1050"> </a>
<a name="ln1051">		if (pageState == PAGE_STATE_MODIFIED</a>
<a name="ln1052">			&amp;&amp; sPages[i].Cache() != NULL</a>
<a name="ln1053">			&amp;&amp; sPages[i].Cache()-&gt;temporary &amp;&amp; sPages[i].WiredCount() == 0) {</a>
<a name="ln1054">			swappableModified++;</a>
<a name="ln1055">			if (sPages[i].usage_count == 0)</a>
<a name="ln1056">				swappableModifiedInactive++;</a>
<a name="ln1057">		}</a>
<a name="ln1058"> </a>
<a name="ln1059">		// track free and cached pages runs</a>
<a name="ln1060">		if (pageState == PAGE_STATE_FREE || pageState == PAGE_STATE_CLEAR) {</a>
<a name="ln1061">			currentFreeRun.end = i + 1;</a>
<a name="ln1062">			currentCachedRun.end = i + 1;</a>
<a name="ln1063">		} else {</a>
<a name="ln1064">			if (currentFreeRun.Length() &gt; longestFreeRun.Length())</a>
<a name="ln1065">				longestFreeRun = currentFreeRun;</a>
<a name="ln1066">			currentFreeRun.start = currentFreeRun.end = i + 1;</a>
<a name="ln1067"> </a>
<a name="ln1068">			if (pageState == PAGE_STATE_CACHED) {</a>
<a name="ln1069">				currentCachedRun.end = i + 1;</a>
<a name="ln1070">			} else {</a>
<a name="ln1071">				if (currentCachedRun.Length() &gt; longestCachedRun.Length())</a>
<a name="ln1072">					longestCachedRun = currentCachedRun;</a>
<a name="ln1073">				currentCachedRun.start = currentCachedRun.end = i + 1;</a>
<a name="ln1074">			}</a>
<a name="ln1075">		}</a>
<a name="ln1076">	}</a>
<a name="ln1077"> </a>
<a name="ln1078">	kprintf(&quot;page stats:\n&quot;);</a>
<a name="ln1079">	kprintf(&quot;total: %&quot; B_PRIuPHYSADDR &quot;\n&quot;, sNumPages);</a>
<a name="ln1080"> </a>
<a name="ln1081">	kprintf(&quot;active: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1082">		counter[PAGE_STATE_ACTIVE], busyCounter[PAGE_STATE_ACTIVE]);</a>
<a name="ln1083">	kprintf(&quot;inactive: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1084">		counter[PAGE_STATE_INACTIVE], busyCounter[PAGE_STATE_INACTIVE]);</a>
<a name="ln1085">	kprintf(&quot;cached: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1086">		counter[PAGE_STATE_CACHED], busyCounter[PAGE_STATE_CACHED]);</a>
<a name="ln1087">	kprintf(&quot;unused: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1088">		counter[PAGE_STATE_UNUSED], busyCounter[PAGE_STATE_UNUSED]);</a>
<a name="ln1089">	kprintf(&quot;wired: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1090">		counter[PAGE_STATE_WIRED], busyCounter[PAGE_STATE_WIRED]);</a>
<a name="ln1091">	kprintf(&quot;modified: %&quot; B_PRIuSIZE &quot; (busy: %&quot; B_PRIuSIZE &quot;)\n&quot;,</a>
<a name="ln1092">		counter[PAGE_STATE_MODIFIED], busyCounter[PAGE_STATE_MODIFIED]);</a>
<a name="ln1093">	kprintf(&quot;free: %&quot; B_PRIuSIZE &quot;\n&quot;, counter[PAGE_STATE_FREE]);</a>
<a name="ln1094">	kprintf(&quot;clear: %&quot; B_PRIuSIZE &quot;\n&quot;, counter[PAGE_STATE_CLEAR]);</a>
<a name="ln1095"> </a>
<a name="ln1096">	kprintf(&quot;unreserved free pages: %&quot; B_PRId32 &quot;\n&quot;, sUnreservedFreePages);</a>
<a name="ln1097">	kprintf(&quot;unsatisfied page reservations: %&quot; B_PRId32 &quot;\n&quot;,</a>
<a name="ln1098">		sUnsatisfiedPageReservations);</a>
<a name="ln1099">	kprintf(&quot;mapped pages: %&quot; B_PRId32 &quot;\n&quot;, gMappedPagesCount);</a>
<a name="ln1100">	kprintf(&quot;longest free pages run: %&quot; B_PRIuPHYSADDR &quot; pages (at %&quot;</a>
<a name="ln1101">		B_PRIuPHYSADDR &quot;)\n&quot;, longestFreeRun.Length(),</a>
<a name="ln1102">		sPages[longestFreeRun.start].physical_page_number);</a>
<a name="ln1103">	kprintf(&quot;longest free/cached pages run: %&quot; B_PRIuPHYSADDR &quot; pages (at %&quot;</a>
<a name="ln1104">		B_PRIuPHYSADDR &quot;)\n&quot;, longestCachedRun.Length(),</a>
<a name="ln1105">		sPages[longestCachedRun.start].physical_page_number);</a>
<a name="ln1106"> </a>
<a name="ln1107">	kprintf(&quot;waiting threads:\n&quot;);</a>
<a name="ln1108">	for (PageReservationWaiterList::Iterator it</a>
<a name="ln1109">			= sPageReservationWaiters.GetIterator();</a>
<a name="ln1110">		PageReservationWaiter* waiter = it.Next();) {</a>
<a name="ln1111">		kprintf(&quot;  %6&quot; B_PRId32 &quot;: missing: %6&quot; B_PRIu32</a>
<a name="ln1112">			&quot;, don't touch: %6&quot; B_PRIu32 &quot;\n&quot;, waiter-&gt;thread-&gt;id,</a>
<a name="ln1113">			waiter-&gt;missing, waiter-&gt;dontTouch);</a>
<a name="ln1114">	}</a>
<a name="ln1115"> </a>
<a name="ln1116">	kprintf(&quot;\nfree queue: %p, count = %&quot; B_PRIuPHYSADDR &quot;\n&quot;, &amp;sFreePageQueue,</a>
<a name="ln1117">		sFreePageQueue.Count());</a>
<a name="ln1118">	kprintf(&quot;clear queue: %p, count = %&quot; B_PRIuPHYSADDR &quot;\n&quot;, &amp;sClearPageQueue,</a>
<a name="ln1119">		sClearPageQueue.Count());</a>
<a name="ln1120">	kprintf(&quot;modified queue: %p, count = %&quot; B_PRIuPHYSADDR &quot; (%&quot; B_PRId32</a>
<a name="ln1121">		&quot; temporary, %&quot; B_PRIuPHYSADDR &quot; swappable, &quot; &quot;inactive: %&quot;</a>
<a name="ln1122">		B_PRIuPHYSADDR &quot;)\n&quot;, &amp;sModifiedPageQueue, sModifiedPageQueue.Count(),</a>
<a name="ln1123">		sModifiedTemporaryPages, swappableModified, swappableModifiedInactive);</a>
<a name="ln1124">	kprintf(&quot;active queue: %p, count = %&quot; B_PRIuPHYSADDR &quot;\n&quot;,</a>
<a name="ln1125">		&amp;sActivePageQueue, sActivePageQueue.Count());</a>
<a name="ln1126">	kprintf(&quot;inactive queue: %p, count = %&quot; B_PRIuPHYSADDR &quot;\n&quot;,</a>
<a name="ln1127">		&amp;sInactivePageQueue, sInactivePageQueue.Count());</a>
<a name="ln1128">	kprintf(&quot;cached queue: %p, count = %&quot; B_PRIuPHYSADDR &quot;\n&quot;,</a>
<a name="ln1129">		&amp;sCachedPageQueue, sCachedPageQueue.Count());</a>
<a name="ln1130">	return 0;</a>
<a name="ln1131">}</a>
<a name="ln1132"> </a>
<a name="ln1133"> </a>
<a name="ln1134">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln1135"> </a>
<a name="ln1136">static caller_info*</a>
<a name="ln1137">get_caller_info(addr_t caller)</a>
<a name="ln1138">{</a>
<a name="ln1139">	// find the caller info</a>
<a name="ln1140">	for (int32 i = 0; i &lt; sCallerInfoCount; i++) {</a>
<a name="ln1141">		if (caller == sCallerInfoTable[i].caller)</a>
<a name="ln1142">			return &amp;sCallerInfoTable[i];</a>
<a name="ln1143">	}</a>
<a name="ln1144"> </a>
<a name="ln1145">	// not found, add a new entry, if there are free slots</a>
<a name="ln1146">	if (sCallerInfoCount &gt;= kCallerInfoTableSize)</a>
<a name="ln1147">		return NULL;</a>
<a name="ln1148"> </a>
<a name="ln1149">	caller_info* info = &amp;sCallerInfoTable[sCallerInfoCount++];</a>
<a name="ln1150">	info-&gt;caller = caller;</a>
<a name="ln1151">	info-&gt;count = 0;</a>
<a name="ln1152"> </a>
<a name="ln1153">	return info;</a>
<a name="ln1154">}</a>
<a name="ln1155"> </a>
<a name="ln1156"> </a>
<a name="ln1157">static int</a>
<a name="ln1158">caller_info_compare_count(const void* _a, const void* _b)</a>
<a name="ln1159">{</a>
<a name="ln1160">	const caller_info* a = (const caller_info*)_a;</a>
<a name="ln1161">	const caller_info* b = (const caller_info*)_b;</a>
<a name="ln1162">	return (int)(b-&gt;count - a-&gt;count);</a>
<a name="ln1163">}</a>
<a name="ln1164"> </a>
<a name="ln1165"> </a>
<a name="ln1166">static int</a>
<a name="ln1167">dump_page_allocations_per_caller(int argc, char** argv)</a>
<a name="ln1168">{</a>
<a name="ln1169">	bool resetAllocationInfos = false;</a>
<a name="ln1170">	bool printDetails = false;</a>
<a name="ln1171">	addr_t caller = 0;</a>
<a name="ln1172"> </a>
<a name="ln1173">	for (int32 i = 1; i &lt; argc; i++) {</a>
<a name="ln1174">		if (strcmp(argv[i], &quot;-d&quot;) == 0) {</a>
<a name="ln1175">			uint64 callerAddress;</a>
<a name="ln1176">			if (++i &gt;= argc</a>
<a name="ln1177">				|| !evaluate_debug_expression(argv[i], &amp;callerAddress, true)) {</a>
<a name="ln1178">				print_debugger_command_usage(argv[0]);</a>
<a name="ln1179">				return 0;</a>
<a name="ln1180">			}</a>
<a name="ln1181"> </a>
<a name="ln1182">			caller = callerAddress;</a>
<a name="ln1183">			printDetails = true;</a>
<a name="ln1184">		} else if (strcmp(argv[i], &quot;-r&quot;) == 0) {</a>
<a name="ln1185">			resetAllocationInfos = true;</a>
<a name="ln1186">		} else {</a>
<a name="ln1187">			print_debugger_command_usage(argv[0]);</a>
<a name="ln1188">			return 0;</a>
<a name="ln1189">		}</a>
<a name="ln1190">	}</a>
<a name="ln1191"> </a>
<a name="ln1192">	sCallerInfoCount = 0;</a>
<a name="ln1193"> </a>
<a name="ln1194">	AllocationCollectorCallback collectorCallback(resetAllocationInfos);</a>
<a name="ln1195">	AllocationDetailPrinterCallback detailsCallback(caller);</a>
<a name="ln1196">	AllocationTrackingCallback&amp; callback = printDetails</a>
<a name="ln1197">		? (AllocationTrackingCallback&amp;)detailsCallback</a>
<a name="ln1198">		: (AllocationTrackingCallback&amp;)collectorCallback;</a>
<a name="ln1199"> </a>
<a name="ln1200">	for (page_num_t i = 0; i &lt; sNumPages; i++)</a>
<a name="ln1201">		callback.ProcessTrackingInfo(&amp;sPages[i].allocation_tracking_info, i);</a>
<a name="ln1202"> </a>
<a name="ln1203">	if (printDetails)</a>
<a name="ln1204">		return 0;</a>
<a name="ln1205"> </a>
<a name="ln1206">	// sort the array</a>
<a name="ln1207">	qsort(sCallerInfoTable, sCallerInfoCount, sizeof(caller_info),</a>
<a name="ln1208">		&amp;caller_info_compare_count);</a>
<a name="ln1209"> </a>
<a name="ln1210">	kprintf(&quot;%&quot; B_PRId32 &quot; different callers\n\n&quot;, sCallerInfoCount);</a>
<a name="ln1211"> </a>
<a name="ln1212">	size_t totalAllocationCount = 0;</a>
<a name="ln1213"> </a>
<a name="ln1214">	kprintf(&quot;     count      caller\n&quot;);</a>
<a name="ln1215">	kprintf(&quot;----------------------------------\n&quot;);</a>
<a name="ln1216">	for (int32 i = 0; i &lt; sCallerInfoCount; i++) {</a>
<a name="ln1217">		caller_info&amp; info = sCallerInfoTable[i];</a>
<a name="ln1218">		kprintf(&quot;%10&quot; B_PRIuSIZE &quot;  %p&quot;, info.count, (void*)info.caller);</a>
<a name="ln1219"> </a>
<a name="ln1220">		const char* symbol;</a>
<a name="ln1221">		const char* imageName;</a>
<a name="ln1222">		bool exactMatch;</a>
<a name="ln1223">		addr_t baseAddress;</a>
<a name="ln1224"> </a>
<a name="ln1225">		if (elf_debug_lookup_symbol_address(info.caller, &amp;baseAddress, &amp;symbol,</a>
<a name="ln1226">				&amp;imageName, &amp;exactMatch) == B_OK) {</a>
<a name="ln1227">			kprintf(&quot;  %s + %#&quot; B_PRIxADDR &quot; (%s)%s\n&quot;, symbol,</a>
<a name="ln1228">				info.caller - baseAddress, imageName,</a>
<a name="ln1229">				exactMatch ? &quot;&quot; : &quot; (nearest)&quot;);</a>
<a name="ln1230">		} else</a>
<a name="ln1231">			kprintf(&quot;\n&quot;);</a>
<a name="ln1232"> </a>
<a name="ln1233">		totalAllocationCount += info.count;</a>
<a name="ln1234">	}</a>
<a name="ln1235"> </a>
<a name="ln1236">	kprintf(&quot;\ntotal page allocations: %&quot; B_PRIuSIZE &quot;\n&quot;,</a>
<a name="ln1237">		totalAllocationCount);</a>
<a name="ln1238"> </a>
<a name="ln1239">	return 0;</a>
<a name="ln1240">}</a>
<a name="ln1241"> </a>
<a name="ln1242"> </a>
<a name="ln1243">static int</a>
<a name="ln1244">dump_page_allocation_infos(int argc, char** argv)</a>
<a name="ln1245">{</a>
<a name="ln1246">	page_num_t pageFilter = 0;</a>
<a name="ln1247">	team_id teamFilter = -1;</a>
<a name="ln1248">	thread_id threadFilter = -1;</a>
<a name="ln1249">	bool printStackTraces = false;</a>
<a name="ln1250"> </a>
<a name="ln1251">	for (int32 i = 1; i &lt; argc; i++) {</a>
<a name="ln1252">		if (strcmp(argv[i], &quot;--stacktrace&quot;) == 0)</a>
<a name="ln1253">			printStackTraces = true;</a>
<a name="ln1254">		else if (strcmp(argv[i], &quot;-p&quot;) == 0) {</a>
<a name="ln1255">			uint64 pageNumber;</a>
<a name="ln1256">			if (++i &gt;= argc</a>
<a name="ln1257">				|| !evaluate_debug_expression(argv[i], &amp;pageNumber, true)) {</a>
<a name="ln1258">				print_debugger_command_usage(argv[0]);</a>
<a name="ln1259">				return 0;</a>
<a name="ln1260">			}</a>
<a name="ln1261"> </a>
<a name="ln1262">			pageFilter = pageNumber;</a>
<a name="ln1263">		} else if (strcmp(argv[i], &quot;--team&quot;) == 0) {</a>
<a name="ln1264">			uint64 team;</a>
<a name="ln1265">			if (++i &gt;= argc</a>
<a name="ln1266">				|| !evaluate_debug_expression(argv[i], &amp;team, true)) {</a>
<a name="ln1267">				print_debugger_command_usage(argv[0]);</a>
<a name="ln1268">				return 0;</a>
<a name="ln1269">			}</a>
<a name="ln1270"> </a>
<a name="ln1271">			teamFilter = team;</a>
<a name="ln1272">		} else if (strcmp(argv[i], &quot;--thread&quot;) == 0) {</a>
<a name="ln1273">			uint64 thread;</a>
<a name="ln1274">			if (++i &gt;= argc</a>
<a name="ln1275">				|| !evaluate_debug_expression(argv[i], &amp;thread, true)) {</a>
<a name="ln1276">				print_debugger_command_usage(argv[0]);</a>
<a name="ln1277">				return 0;</a>
<a name="ln1278">			}</a>
<a name="ln1279"> </a>
<a name="ln1280">			threadFilter = thread;</a>
<a name="ln1281">		} else {</a>
<a name="ln1282">			print_debugger_command_usage(argv[0]);</a>
<a name="ln1283">			return 0;</a>
<a name="ln1284">		}</a>
<a name="ln1285">	}</a>
<a name="ln1286"> </a>
<a name="ln1287">	AllocationInfoPrinterCallback callback(printStackTraces, pageFilter,</a>
<a name="ln1288">		teamFilter, threadFilter);</a>
<a name="ln1289"> </a>
<a name="ln1290">	for (page_num_t i = 0; i &lt; sNumPages; i++)</a>
<a name="ln1291">		callback.ProcessTrackingInfo(&amp;sPages[i].allocation_tracking_info, i);</a>
<a name="ln1292"> </a>
<a name="ln1293">	return 0;</a>
<a name="ln1294">}</a>
<a name="ln1295"> </a>
<a name="ln1296">#endif	// VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln1297"> </a>
<a name="ln1298"> </a>
<a name="ln1299">#ifdef TRACK_PAGE_USAGE_STATS</a>
<a name="ln1300"> </a>
<a name="ln1301">static void</a>
<a name="ln1302">track_page_usage(vm_page* page)</a>
<a name="ln1303">{</a>
<a name="ln1304">	if (page-&gt;WiredCount() == 0) {</a>
<a name="ln1305">		sNextPageUsage[(int32)page-&gt;usage_count + 128]++;</a>
<a name="ln1306">		sNextPageUsagePageCount++;</a>
<a name="ln1307">	}</a>
<a name="ln1308">}</a>
<a name="ln1309"> </a>
<a name="ln1310"> </a>
<a name="ln1311">static void</a>
<a name="ln1312">update_page_usage_stats()</a>
<a name="ln1313">{</a>
<a name="ln1314">	std::swap(sPageUsage, sNextPageUsage);</a>
<a name="ln1315">	sPageUsagePageCount = sNextPageUsagePageCount;</a>
<a name="ln1316"> </a>
<a name="ln1317">	memset(sNextPageUsage, 0, sizeof(page_num_t) * 256);</a>
<a name="ln1318">	sNextPageUsagePageCount = 0;</a>
<a name="ln1319"> </a>
<a name="ln1320">	// compute average</a>
<a name="ln1321">	if (sPageUsagePageCount &gt; 0) {</a>
<a name="ln1322">		int64 sum = 0;</a>
<a name="ln1323">		for (int32 i = 0; i &lt; 256; i++)</a>
<a name="ln1324">			sum += (int64)sPageUsage[i] * (i - 128);</a>
<a name="ln1325"> </a>
<a name="ln1326">		TRACE_DAEMON(&quot;average page usage: %f (%lu pages)\n&quot;,</a>
<a name="ln1327">			(float)sum / sPageUsagePageCount, sPageUsagePageCount);</a>
<a name="ln1328">	}</a>
<a name="ln1329">}</a>
<a name="ln1330"> </a>
<a name="ln1331"> </a>
<a name="ln1332">static int</a>
<a name="ln1333">dump_page_usage_stats(int argc, char** argv)</a>
<a name="ln1334">{</a>
<a name="ln1335">	kprintf(&quot;distribution of page usage counts (%lu pages):&quot;,</a>
<a name="ln1336">		sPageUsagePageCount);</a>
<a name="ln1337"> </a>
<a name="ln1338">	int64 sum = 0;</a>
<a name="ln1339">	for (int32 i = 0; i &lt; 256; i++) {</a>
<a name="ln1340">		if (i % 8 == 0)</a>
<a name="ln1341">			kprintf(&quot;\n%4ld:&quot;, i - 128);</a>
<a name="ln1342"> </a>
<a name="ln1343">		int64 count = sPageUsage[i];</a>
<a name="ln1344">		sum += count * (i - 128);</a>
<a name="ln1345"> </a>
<a name="ln1346">		kprintf(&quot;  %9llu&quot;, count);</a>
<a name="ln1347">	}</a>
<a name="ln1348"> </a>
<a name="ln1349">	kprintf(&quot;\n\n&quot;);</a>
<a name="ln1350"> </a>
<a name="ln1351">	kprintf(&quot;average usage count: %f\n&quot;,</a>
<a name="ln1352">		sPageUsagePageCount &gt; 0 ? (float)sum / sPageUsagePageCount : 0);</a>
<a name="ln1353"> </a>
<a name="ln1354">	return 0;</a>
<a name="ln1355">}</a>
<a name="ln1356"> </a>
<a name="ln1357">#endif	// TRACK_PAGE_USAGE_STATS</a>
<a name="ln1358"> </a>
<a name="ln1359"> </a>
<a name="ln1360">// #pragma mark - vm_page</a>
<a name="ln1361"> </a>
<a name="ln1362"> </a>
<a name="ln1363">inline void</a>
<a name="ln1364">vm_page::InitState(uint8 newState)</a>
<a name="ln1365">{</a>
<a name="ln1366">	state = newState;</a>
<a name="ln1367">}</a>
<a name="ln1368"> </a>
<a name="ln1369"> </a>
<a name="ln1370">inline void</a>
<a name="ln1371">vm_page::SetState(uint8 newState)</a>
<a name="ln1372">{</a>
<a name="ln1373">	TPS(SetPageState(this, newState));</a>
<a name="ln1374"> </a>
<a name="ln1375">	state = newState;</a>
<a name="ln1376">}</a>
<a name="ln1377"> </a>
<a name="ln1378"> </a>
<a name="ln1379">// #pragma mark -</a>
<a name="ln1380"> </a>
<a name="ln1381"> </a>
<a name="ln1382">static void</a>
<a name="ln1383">get_page_stats(page_stats&amp; _pageStats)</a>
<a name="ln1384">{</a>
<a name="ln1385">	_pageStats.totalFreePages = sUnreservedFreePages;</a>
<a name="ln1386">	_pageStats.cachedPages = sCachedPageQueue.Count();</a>
<a name="ln1387">	_pageStats.unsatisfiedReservations = sUnsatisfiedPageReservations;</a>
<a name="ln1388">	// TODO: We don't get an actual snapshot here!</a>
<a name="ln1389">}</a>
<a name="ln1390"> </a>
<a name="ln1391"> </a>
<a name="ln1392">static bool</a>
<a name="ln1393">do_active_paging(const page_stats&amp; pageStats)</a>
<a name="ln1394">{</a>
<a name="ln1395">	return pageStats.totalFreePages + pageStats.cachedPages</a>
<a name="ln1396">		&lt; pageStats.unsatisfiedReservations</a>
<a name="ln1397">			+ (int32)sFreeOrCachedPagesTarget;</a>
<a name="ln1398">}</a>
<a name="ln1399"> </a>
<a name="ln1400"> </a>
<a name="ln1401">/*!	Reserves as many pages as possible from \c sUnreservedFreePages up to</a>
<a name="ln1402">	\a count. Doesn't touch the last \a dontTouch pages of</a>
<a name="ln1403">	\c sUnreservedFreePages, though.</a>
<a name="ln1404">	\return The number of actually reserved pages.</a>
<a name="ln1405">*/</a>
<a name="ln1406">static uint32</a>
<a name="ln1407">reserve_some_pages(uint32 count, uint32 dontTouch)</a>
<a name="ln1408">{</a>
<a name="ln1409">	while (true) {</a>
<a name="ln1410">		int32 freePages = atomic_get(&amp;sUnreservedFreePages);</a>
<a name="ln1411">		if (freePages &lt;= (int32)dontTouch)</a>
<a name="ln1412">			return 0;</a>
<a name="ln1413"> </a>
<a name="ln1414">		int32 toReserve = std::min(count, freePages - dontTouch);</a>
<a name="ln1415">		if (atomic_test_and_set(&amp;sUnreservedFreePages,</a>
<a name="ln1416">					freePages - toReserve, freePages)</a>
<a name="ln1417">				== freePages) {</a>
<a name="ln1418">			return toReserve;</a>
<a name="ln1419">		}</a>
<a name="ln1420"> </a>
<a name="ln1421">		// the count changed in the meantime -- retry</a>
<a name="ln1422">	}</a>
<a name="ln1423">}</a>
<a name="ln1424"> </a>
<a name="ln1425"> </a>
<a name="ln1426">static void</a>
<a name="ln1427">wake_up_page_reservation_waiters()</a>
<a name="ln1428">{</a>
<a name="ln1429">	MutexLocker pageDeficitLocker(sPageDeficitLock);</a>
<a name="ln1430"> </a>
<a name="ln1431">	// TODO: If this is a low priority thread, we might want to disable</a>
<a name="ln1432">	// interrupts or otherwise ensure that we aren't unscheduled. Otherwise</a>
<a name="ln1433">	// high priority threads wait be kept waiting while a medium priority thread</a>
<a name="ln1434">	// prevents us from running.</a>
<a name="ln1435"> </a>
<a name="ln1436">	while (PageReservationWaiter* waiter = sPageReservationWaiters.Head()) {</a>
<a name="ln1437">		int32 reserved = reserve_some_pages(waiter-&gt;missing,</a>
<a name="ln1438">			waiter-&gt;dontTouch);</a>
<a name="ln1439">		if (reserved == 0)</a>
<a name="ln1440">			return;</a>
<a name="ln1441"> </a>
<a name="ln1442">		atomic_add(&amp;sUnsatisfiedPageReservations, -reserved);</a>
<a name="ln1443">		waiter-&gt;missing -= reserved;</a>
<a name="ln1444"> </a>
<a name="ln1445">		if (waiter-&gt;missing &gt; 0)</a>
<a name="ln1446">			return;</a>
<a name="ln1447"> </a>
<a name="ln1448">		sPageReservationWaiters.Remove(waiter);</a>
<a name="ln1449"> </a>
<a name="ln1450">		thread_unblock(waiter-&gt;thread, B_OK);</a>
<a name="ln1451">	}</a>
<a name="ln1452">}</a>
<a name="ln1453"> </a>
<a name="ln1454"> </a>
<a name="ln1455">static inline void</a>
<a name="ln1456">unreserve_pages(uint32 count)</a>
<a name="ln1457">{</a>
<a name="ln1458">	atomic_add(&amp;sUnreservedFreePages, count);</a>
<a name="ln1459">	if (atomic_get(&amp;sUnsatisfiedPageReservations) != 0)</a>
<a name="ln1460">		wake_up_page_reservation_waiters();</a>
<a name="ln1461">}</a>
<a name="ln1462"> </a>
<a name="ln1463"> </a>
<a name="ln1464">static void</a>
<a name="ln1465">free_page(vm_page* page, bool clear)</a>
<a name="ln1466">{</a>
<a name="ln1467">	DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln1468"> </a>
<a name="ln1469">	PAGE_ASSERT(page, !page-&gt;IsMapped());</a>
<a name="ln1470"> </a>
<a name="ln1471">	VMPageQueue* fromQueue;</a>
<a name="ln1472"> </a>
<a name="ln1473">	switch (page-&gt;State()) {</a>
<a name="ln1474">		case PAGE_STATE_ACTIVE:</a>
<a name="ln1475">			fromQueue = &amp;sActivePageQueue;</a>
<a name="ln1476">			break;</a>
<a name="ln1477">		case PAGE_STATE_INACTIVE:</a>
<a name="ln1478">			fromQueue = &amp;sInactivePageQueue;</a>
<a name="ln1479">			break;</a>
<a name="ln1480">		case PAGE_STATE_MODIFIED:</a>
<a name="ln1481">			fromQueue = &amp;sModifiedPageQueue;</a>
<a name="ln1482">			break;</a>
<a name="ln1483">		case PAGE_STATE_CACHED:</a>
<a name="ln1484">			fromQueue = &amp;sCachedPageQueue;</a>
<a name="ln1485">			break;</a>
<a name="ln1486">		case PAGE_STATE_FREE:</a>
<a name="ln1487">		case PAGE_STATE_CLEAR:</a>
<a name="ln1488">			panic(&quot;free_page(): page %p already free&quot;, page);</a>
<a name="ln1489">			return;</a>
<a name="ln1490">		case PAGE_STATE_WIRED:</a>
<a name="ln1491">		case PAGE_STATE_UNUSED:</a>
<a name="ln1492">			fromQueue = NULL;</a>
<a name="ln1493">			break;</a>
<a name="ln1494">		default:</a>
<a name="ln1495">			panic(&quot;free_page(): page %p in invalid state %d&quot;,</a>
<a name="ln1496">				page, page-&gt;State());</a>
<a name="ln1497">			return;</a>
<a name="ln1498">	}</a>
<a name="ln1499"> </a>
<a name="ln1500">	if (page-&gt;CacheRef() != NULL)</a>
<a name="ln1501">		panic(&quot;to be freed page %p has cache&quot;, page);</a>
<a name="ln1502">	if (page-&gt;IsMapped())</a>
<a name="ln1503">		panic(&quot;to be freed page %p has mappings&quot;, page);</a>
<a name="ln1504"> </a>
<a name="ln1505">	if (fromQueue != NULL)</a>
<a name="ln1506">		fromQueue-&gt;RemoveUnlocked(page);</a>
<a name="ln1507"> </a>
<a name="ln1508">	TA(FreePage(page-&gt;physical_page_number));</a>
<a name="ln1509"> </a>
<a name="ln1510">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln1511">	page-&gt;allocation_tracking_info.Clear();</a>
<a name="ln1512">#endif</a>
<a name="ln1513"> </a>
<a name="ln1514">	ReadLocker locker(sFreePageQueuesLock);</a>
<a name="ln1515"> </a>
<a name="ln1516">	DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1517"> </a>
<a name="ln1518">	if (clear) {</a>
<a name="ln1519">		page-&gt;SetState(PAGE_STATE_CLEAR);</a>
<a name="ln1520">		sClearPageQueue.PrependUnlocked(page);</a>
<a name="ln1521">	} else {</a>
<a name="ln1522">		page-&gt;SetState(PAGE_STATE_FREE);</a>
<a name="ln1523">		sFreePageQueue.PrependUnlocked(page);</a>
<a name="ln1524">	}</a>
<a name="ln1525"> </a>
<a name="ln1526">	locker.Unlock();</a>
<a name="ln1527">}</a>
<a name="ln1528"> </a>
<a name="ln1529"> </a>
<a name="ln1530">/*!	The caller must make sure that no-one else tries to change the page's state</a>
<a name="ln1531">	while the function is called. If the page has a cache, this can be done by</a>
<a name="ln1532">	locking the cache.</a>
<a name="ln1533">*/</a>
<a name="ln1534">static void</a>
<a name="ln1535">set_page_state(vm_page *page, int pageState)</a>
<a name="ln1536">{</a>
<a name="ln1537">	DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln1538"> </a>
<a name="ln1539">	if (pageState == page-&gt;State())</a>
<a name="ln1540">		return;</a>
<a name="ln1541"> </a>
<a name="ln1542">	VMPageQueue* fromQueue;</a>
<a name="ln1543"> </a>
<a name="ln1544">	switch (page-&gt;State()) {</a>
<a name="ln1545">		case PAGE_STATE_ACTIVE:</a>
<a name="ln1546">			fromQueue = &amp;sActivePageQueue;</a>
<a name="ln1547">			break;</a>
<a name="ln1548">		case PAGE_STATE_INACTIVE:</a>
<a name="ln1549">			fromQueue = &amp;sInactivePageQueue;</a>
<a name="ln1550">			break;</a>
<a name="ln1551">		case PAGE_STATE_MODIFIED:</a>
<a name="ln1552">			fromQueue = &amp;sModifiedPageQueue;</a>
<a name="ln1553">			break;</a>
<a name="ln1554">		case PAGE_STATE_CACHED:</a>
<a name="ln1555">			fromQueue = &amp;sCachedPageQueue;</a>
<a name="ln1556">			break;</a>
<a name="ln1557">		case PAGE_STATE_FREE:</a>
<a name="ln1558">		case PAGE_STATE_CLEAR:</a>
<a name="ln1559">			panic(&quot;set_page_state(): page %p is free/clear&quot;, page);</a>
<a name="ln1560">			return;</a>
<a name="ln1561">		case PAGE_STATE_WIRED:</a>
<a name="ln1562">		case PAGE_STATE_UNUSED:</a>
<a name="ln1563">			fromQueue = NULL;</a>
<a name="ln1564">			break;</a>
<a name="ln1565">		default:</a>
<a name="ln1566">			panic(&quot;set_page_state(): page %p in invalid state %d&quot;,</a>
<a name="ln1567">				page, page-&gt;State());</a>
<a name="ln1568">			return;</a>
<a name="ln1569">	}</a>
<a name="ln1570"> </a>
<a name="ln1571">	VMPageQueue* toQueue;</a>
<a name="ln1572"> </a>
<a name="ln1573">	switch (pageState) {</a>
<a name="ln1574">		case PAGE_STATE_ACTIVE:</a>
<a name="ln1575">			toQueue = &amp;sActivePageQueue;</a>
<a name="ln1576">			break;</a>
<a name="ln1577">		case PAGE_STATE_INACTIVE:</a>
<a name="ln1578">			toQueue = &amp;sInactivePageQueue;</a>
<a name="ln1579">			break;</a>
<a name="ln1580">		case PAGE_STATE_MODIFIED:</a>
<a name="ln1581">			toQueue = &amp;sModifiedPageQueue;</a>
<a name="ln1582">			break;</a>
<a name="ln1583">		case PAGE_STATE_CACHED:</a>
<a name="ln1584">			PAGE_ASSERT(page, !page-&gt;IsMapped());</a>
<a name="ln1585">			PAGE_ASSERT(page, !page-&gt;modified);</a>
<a name="ln1586">			toQueue = &amp;sCachedPageQueue;</a>
<a name="ln1587">			break;</a>
<a name="ln1588">		case PAGE_STATE_FREE:</a>
<a name="ln1589">		case PAGE_STATE_CLEAR:</a>
<a name="ln1590">			panic(&quot;set_page_state(): target state is free/clear&quot;);</a>
<a name="ln1591">			return;</a>
<a name="ln1592">		case PAGE_STATE_WIRED:</a>
<a name="ln1593">		case PAGE_STATE_UNUSED:</a>
<a name="ln1594">			toQueue = NULL;</a>
<a name="ln1595">			break;</a>
<a name="ln1596">		default:</a>
<a name="ln1597">			panic(&quot;set_page_state(): invalid target state %d&quot;, pageState);</a>
<a name="ln1598">			return;</a>
<a name="ln1599">	}</a>
<a name="ln1600"> </a>
<a name="ln1601">	VMCache* cache = page-&gt;Cache();</a>
<a name="ln1602">	if (cache != NULL &amp;&amp; cache-&gt;temporary) {</a>
<a name="ln1603">		if (pageState == PAGE_STATE_MODIFIED)</a>
<a name="ln1604">			atomic_add(&amp;sModifiedTemporaryPages, 1);</a>
<a name="ln1605">		else if (page-&gt;State() == PAGE_STATE_MODIFIED)</a>
<a name="ln1606">			atomic_add(&amp;sModifiedTemporaryPages, -1);</a>
<a name="ln1607">	}</a>
<a name="ln1608"> </a>
<a name="ln1609">	// move the page</a>
<a name="ln1610">	if (toQueue == fromQueue) {</a>
<a name="ln1611">		// Note: Theoretically we are required to lock when changing the page</a>
<a name="ln1612">		// state, even if we don't change the queue. We actually don't have to</a>
<a name="ln1613">		// do this, though, since only for the active queue there are different</a>
<a name="ln1614">		// page states and active pages have a cache that must be locked at</a>
<a name="ln1615">		// this point. So we rely on the fact that everyone must lock the cache</a>
<a name="ln1616">		// before trying to change/interpret the page state.</a>
<a name="ln1617">		PAGE_ASSERT(page, cache != NULL);</a>
<a name="ln1618">		cache-&gt;AssertLocked();</a>
<a name="ln1619">		page-&gt;SetState(pageState);</a>
<a name="ln1620">	} else {</a>
<a name="ln1621">		if (fromQueue != NULL)</a>
<a name="ln1622">			fromQueue-&gt;RemoveUnlocked(page);</a>
<a name="ln1623"> </a>
<a name="ln1624">		page-&gt;SetState(pageState);</a>
<a name="ln1625"> </a>
<a name="ln1626">		if (toQueue != NULL)</a>
<a name="ln1627">			toQueue-&gt;AppendUnlocked(page);</a>
<a name="ln1628">	}</a>
<a name="ln1629">}</a>
<a name="ln1630"> </a>
<a name="ln1631"> </a>
<a name="ln1632">/*! Moves a previously modified page into a now appropriate queue.</a>
<a name="ln1633">	The page queues must not be locked.</a>
<a name="ln1634">*/</a>
<a name="ln1635">static void</a>
<a name="ln1636">move_page_to_appropriate_queue(vm_page *page)</a>
<a name="ln1637">{</a>
<a name="ln1638">	DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln1639"> </a>
<a name="ln1640">	// Note, this logic must be in sync with what the page daemon does.</a>
<a name="ln1641">	int32 state;</a>
<a name="ln1642">	if (page-&gt;IsMapped())</a>
<a name="ln1643">		state = PAGE_STATE_ACTIVE;</a>
<a name="ln1644">	else if (page-&gt;modified)</a>
<a name="ln1645">		state = PAGE_STATE_MODIFIED;</a>
<a name="ln1646">	else</a>
<a name="ln1647">		state = PAGE_STATE_CACHED;</a>
<a name="ln1648"> </a>
<a name="ln1649">// TODO: If free + cached pages are low, we might directly want to free the</a>
<a name="ln1650">// page.</a>
<a name="ln1651">	set_page_state(page, state);</a>
<a name="ln1652">}</a>
<a name="ln1653"> </a>
<a name="ln1654"> </a>
<a name="ln1655">static void</a>
<a name="ln1656">clear_page(struct vm_page *page)</a>
<a name="ln1657">{</a>
<a name="ln1658">	vm_memset_physical(page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT, 0,</a>
<a name="ln1659">		B_PAGE_SIZE);</a>
<a name="ln1660">}</a>
<a name="ln1661"> </a>
<a name="ln1662"> </a>
<a name="ln1663">static status_t</a>
<a name="ln1664">mark_page_range_in_use(page_num_t startPage, page_num_t length, bool wired)</a>
<a name="ln1665">{</a>
<a name="ln1666">	TRACE((&quot;mark_page_range_in_use: start %#&quot; B_PRIxPHYSADDR &quot;, len %#&quot;</a>
<a name="ln1667">		B_PRIxPHYSADDR &quot;\n&quot;, startPage, length));</a>
<a name="ln1668"> </a>
<a name="ln1669">	if (sPhysicalPageOffset &gt; startPage) {</a>
<a name="ln1670">		dprintf(&quot;mark_page_range_in_use(%#&quot; B_PRIxPHYSADDR &quot;, %#&quot; B_PRIxPHYSADDR</a>
<a name="ln1671">			&quot;): start page is before free list\n&quot;, startPage, length);</a>
<a name="ln1672">		if (sPhysicalPageOffset - startPage &gt;= length)</a>
<a name="ln1673">			return B_OK;</a>
<a name="ln1674">		length -= sPhysicalPageOffset - startPage;</a>
<a name="ln1675">		startPage = sPhysicalPageOffset;</a>
<a name="ln1676">	}</a>
<a name="ln1677"> </a>
<a name="ln1678">	startPage -= sPhysicalPageOffset;</a>
<a name="ln1679"> </a>
<a name="ln1680">	if (startPage + length &gt; sNumPages) {</a>
<a name="ln1681">		dprintf(&quot;mark_page_range_in_use(%#&quot; B_PRIxPHYSADDR &quot;, %#&quot; B_PRIxPHYSADDR</a>
<a name="ln1682">			&quot;): range would extend past free list\n&quot;, startPage, length);</a>
<a name="ln1683">		if (startPage &gt;= sNumPages)</a>
<a name="ln1684">			return B_OK;</a>
<a name="ln1685">		length = sNumPages - startPage;</a>
<a name="ln1686">	}</a>
<a name="ln1687"> </a>
<a name="ln1688">	WriteLocker locker(sFreePageQueuesLock);</a>
<a name="ln1689"> </a>
<a name="ln1690">	for (page_num_t i = 0; i &lt; length; i++) {</a>
<a name="ln1691">		vm_page *page = &amp;sPages[startPage + i];</a>
<a name="ln1692">		switch (page-&gt;State()) {</a>
<a name="ln1693">			case PAGE_STATE_FREE:</a>
<a name="ln1694">			case PAGE_STATE_CLEAR:</a>
<a name="ln1695">			{</a>
<a name="ln1696">// TODO: This violates the page reservation policy, since we remove pages from</a>
<a name="ln1697">// the free/clear queues without having reserved them before. This should happen</a>
<a name="ln1698">// in the early boot process only, though.</a>
<a name="ln1699">				DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln1700">				VMPageQueue&amp; queue = page-&gt;State() == PAGE_STATE_FREE</a>
<a name="ln1701">					? sFreePageQueue : sClearPageQueue;</a>
<a name="ln1702">				queue.Remove(page);</a>
<a name="ln1703">				page-&gt;SetState(wired ? PAGE_STATE_WIRED : PAGE_STATE_UNUSED);</a>
<a name="ln1704">				page-&gt;busy = false;</a>
<a name="ln1705">				atomic_add(&amp;sUnreservedFreePages, -1);</a>
<a name="ln1706">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln1707">				break;</a>
<a name="ln1708">			}</a>
<a name="ln1709">			case PAGE_STATE_WIRED:</a>
<a name="ln1710">			case PAGE_STATE_UNUSED:</a>
<a name="ln1711">				break;</a>
<a name="ln1712">			case PAGE_STATE_ACTIVE:</a>
<a name="ln1713">			case PAGE_STATE_INACTIVE:</a>
<a name="ln1714">			case PAGE_STATE_MODIFIED:</a>
<a name="ln1715">			case PAGE_STATE_CACHED:</a>
<a name="ln1716">			default:</a>
<a name="ln1717">				// uh</a>
<a name="ln1718">				dprintf(&quot;mark_page_range_in_use: page %#&quot; B_PRIxPHYSADDR</a>
<a name="ln1719">					&quot; in non-free state %d!\n&quot;, startPage + i, page-&gt;State());</a>
<a name="ln1720">				break;</a>
<a name="ln1721">		}</a>
<a name="ln1722">	}</a>
<a name="ln1723"> </a>
<a name="ln1724">	return B_OK;</a>
<a name="ln1725">}</a>
<a name="ln1726"> </a>
<a name="ln1727"> </a>
<a name="ln1728">/*!</a>
<a name="ln1729">	This is a background thread that wakes up every now and then (every 100ms)</a>
<a name="ln1730">	and moves some pages from the free queue over to the clear queue.</a>
<a name="ln1731">	Given enough time, it will clear out all pages from the free queue - we</a>
<a name="ln1732">	could probably slow it down after having reached a certain threshold.</a>
<a name="ln1733">*/</a>
<a name="ln1734">static int32</a>
<a name="ln1735">page_scrubber(void *unused)</a>
<a name="ln1736">{</a>
<a name="ln1737">	(void)(unused);</a>
<a name="ln1738"> </a>
<a name="ln1739">	TRACE((&quot;page_scrubber starting...\n&quot;));</a>
<a name="ln1740"> </a>
<a name="ln1741">	for (;;) {</a>
<a name="ln1742">		snooze(100000); // 100ms</a>
<a name="ln1743"> </a>
<a name="ln1744">		if (sFreePageQueue.Count() == 0</a>
<a name="ln1745">				|| atomic_get(&amp;sUnreservedFreePages)</a>
<a name="ln1746">					&lt; (int32)sFreePagesTarget) {</a>
<a name="ln1747">			continue;</a>
<a name="ln1748">		}</a>
<a name="ln1749"> </a>
<a name="ln1750">		// Since we temporarily remove pages from the free pages reserve,</a>
<a name="ln1751">		// we must make sure we don't cause a violation of the page</a>
<a name="ln1752">		// reservation warranty. The following is usually stricter than</a>
<a name="ln1753">		// necessary, because we don't have information on how many of the</a>
<a name="ln1754">		// reserved pages have already been allocated.</a>
<a name="ln1755">		int32 reserved = reserve_some_pages(SCRUB_SIZE,</a>
<a name="ln1756">			kPageReserveForPriority[VM_PRIORITY_USER]);</a>
<a name="ln1757">		if (reserved == 0)</a>
<a name="ln1758">			continue;</a>
<a name="ln1759"> </a>
<a name="ln1760">		// get some pages from the free queue</a>
<a name="ln1761">		ReadLocker locker(sFreePageQueuesLock);</a>
<a name="ln1762"> </a>
<a name="ln1763">		vm_page *page[SCRUB_SIZE];</a>
<a name="ln1764">		int32 scrubCount = 0;</a>
<a name="ln1765">		for (int32 i = 0; i &lt; reserved; i++) {</a>
<a name="ln1766">			page[i] = sFreePageQueue.RemoveHeadUnlocked();</a>
<a name="ln1767">			if (page[i] == NULL)</a>
<a name="ln1768">				break;</a>
<a name="ln1769"> </a>
<a name="ln1770">			DEBUG_PAGE_ACCESS_START(page[i]);</a>
<a name="ln1771"> </a>
<a name="ln1772">			page[i]-&gt;SetState(PAGE_STATE_ACTIVE);</a>
<a name="ln1773">			page[i]-&gt;busy = true;</a>
<a name="ln1774">			scrubCount++;</a>
<a name="ln1775">		}</a>
<a name="ln1776"> </a>
<a name="ln1777">		locker.Unlock();</a>
<a name="ln1778"> </a>
<a name="ln1779">		if (scrubCount == 0) {</a>
<a name="ln1780">			unreserve_pages(reserved);</a>
<a name="ln1781">			continue;</a>
<a name="ln1782">		}</a>
<a name="ln1783"> </a>
<a name="ln1784">		TA(ScrubbingPages(scrubCount));</a>
<a name="ln1785"> </a>
<a name="ln1786">		// clear them</a>
<a name="ln1787">		for (int32 i = 0; i &lt; scrubCount; i++)</a>
<a name="ln1788">			clear_page(page[i]);</a>
<a name="ln1789"> </a>
<a name="ln1790">		locker.Lock();</a>
<a name="ln1791"> </a>
<a name="ln1792">		// and put them into the clear queue</a>
<a name="ln1793">		for (int32 i = 0; i &lt; scrubCount; i++) {</a>
<a name="ln1794">			page[i]-&gt;SetState(PAGE_STATE_CLEAR);</a>
<a name="ln1795">			page[i]-&gt;busy = false;</a>
<a name="ln1796">			DEBUG_PAGE_ACCESS_END(page[i]);</a>
<a name="ln1797">			sClearPageQueue.PrependUnlocked(page[i]);</a>
<a name="ln1798">		}</a>
<a name="ln1799"> </a>
<a name="ln1800">		locker.Unlock();</a>
<a name="ln1801"> </a>
<a name="ln1802">		unreserve_pages(reserved);</a>
<a name="ln1803"> </a>
<a name="ln1804">		TA(ScrubbedPages(scrubCount));</a>
<a name="ln1805">	}</a>
<a name="ln1806"> </a>
<a name="ln1807">	return 0;</a>
<a name="ln1808">}</a>
<a name="ln1809"> </a>
<a name="ln1810"> </a>
<a name="ln1811">static void</a>
<a name="ln1812">init_page_marker(vm_page &amp;marker)</a>
<a name="ln1813">{</a>
<a name="ln1814">	marker.SetCacheRef(NULL);</a>
<a name="ln1815">	marker.InitState(PAGE_STATE_UNUSED);</a>
<a name="ln1816">	marker.busy = true;</a>
<a name="ln1817">#if DEBUG_PAGE_QUEUE</a>
<a name="ln1818">	marker.queue = NULL;</a>
<a name="ln1819">#endif</a>
<a name="ln1820">#if DEBUG_PAGE_ACCESS</a>
<a name="ln1821">	marker.accessing_thread = thread_get_current_thread_id();</a>
<a name="ln1822">#endif</a>
<a name="ln1823">}</a>
<a name="ln1824"> </a>
<a name="ln1825"> </a>
<a name="ln1826">static void</a>
<a name="ln1827">remove_page_marker(struct vm_page &amp;marker)</a>
<a name="ln1828">{</a>
<a name="ln1829">	DEBUG_PAGE_ACCESS_CHECK(&amp;marker);</a>
<a name="ln1830"> </a>
<a name="ln1831">	if (marker.State() &lt; PAGE_STATE_FIRST_UNQUEUED)</a>
<a name="ln1832">		sPageQueues[marker.State()].RemoveUnlocked(&amp;marker);</a>
<a name="ln1833"> </a>
<a name="ln1834">	marker.SetState(PAGE_STATE_UNUSED);</a>
<a name="ln1835">}</a>
<a name="ln1836"> </a>
<a name="ln1837"> </a>
<a name="ln1838">static vm_page*</a>
<a name="ln1839">next_modified_page(page_num_t&amp; maxPagesToSee)</a>
<a name="ln1840">{</a>
<a name="ln1841">	InterruptsSpinLocker locker(sModifiedPageQueue.GetLock());</a>
<a name="ln1842"> </a>
<a name="ln1843">	while (maxPagesToSee &gt; 0) {</a>
<a name="ln1844">		vm_page* page = sModifiedPageQueue.Head();</a>
<a name="ln1845">		if (page == NULL)</a>
<a name="ln1846">			return NULL;</a>
<a name="ln1847"> </a>
<a name="ln1848">		sModifiedPageQueue.Requeue(page, true);</a>
<a name="ln1849"> </a>
<a name="ln1850">		maxPagesToSee--;</a>
<a name="ln1851"> </a>
<a name="ln1852">		if (!page-&gt;busy)</a>
<a name="ln1853">			return page;</a>
<a name="ln1854">	}</a>
<a name="ln1855"> </a>
<a name="ln1856">	return NULL;</a>
<a name="ln1857">}</a>
<a name="ln1858"> </a>
<a name="ln1859"> </a>
<a name="ln1860">// #pragma mark -</a>
<a name="ln1861"> </a>
<a name="ln1862"> </a>
<a name="ln1863">class PageWriteTransfer;</a>
<a name="ln1864">class PageWriteWrapper;</a>
<a name="ln1865"> </a>
<a name="ln1866"> </a>
<a name="ln1867">class PageWriterRun {</a>
<a name="ln1868">public:</a>
<a name="ln1869">	status_t Init(uint32 maxPages);</a>
<a name="ln1870"> </a>
<a name="ln1871">	void PrepareNextRun();</a>
<a name="ln1872">	void AddPage(vm_page* page);</a>
<a name="ln1873">	uint32 Go();</a>
<a name="ln1874"> </a>
<a name="ln1875">	void PageWritten(PageWriteTransfer* transfer, status_t status,</a>
<a name="ln1876">		bool partialTransfer, size_t bytesTransferred);</a>
<a name="ln1877"> </a>
<a name="ln1878">private:</a>
<a name="ln1879">	uint32				fMaxPages;</a>
<a name="ln1880">	uint32				fWrapperCount;</a>
<a name="ln1881">	uint32				fTransferCount;</a>
<a name="ln1882">	int32				fPendingTransfers;</a>
<a name="ln1883">	PageWriteWrapper*	fWrappers;</a>
<a name="ln1884">	PageWriteTransfer*	fTransfers;</a>
<a name="ln1885">	ConditionVariable	fAllFinishedCondition;</a>
<a name="ln1886">};</a>
<a name="ln1887"> </a>
<a name="ln1888"> </a>
<a name="ln1889">class PageWriteTransfer : public AsyncIOCallback {</a>
<a name="ln1890">public:</a>
<a name="ln1891">	void SetTo(PageWriterRun* run, vm_page* page, int32 maxPages);</a>
<a name="ln1892">	bool AddPage(vm_page* page);</a>
<a name="ln1893"> </a>
<a name="ln1894">	status_t Schedule(uint32 flags);</a>
<a name="ln1895"> </a>
<a name="ln1896">	void SetStatus(status_t status, size_t transferred);</a>
<a name="ln1897"> </a>
<a name="ln1898">	status_t Status() const	{ return fStatus; }</a>
<a name="ln1899">	struct VMCache* Cache() const { return fCache; }</a>
<a name="ln1900">	uint32 PageCount() const { return fPageCount; }</a>
<a name="ln1901"> </a>
<a name="ln1902">	virtual void IOFinished(status_t status, bool partialTransfer,</a>
<a name="ln1903">		generic_size_t bytesTransferred);</a>
<a name="ln1904">private:</a>
<a name="ln1905">	PageWriterRun*		fRun;</a>
<a name="ln1906">	struct VMCache*		fCache;</a>
<a name="ln1907">	off_t				fOffset;</a>
<a name="ln1908">	uint32				fPageCount;</a>
<a name="ln1909">	int32				fMaxPages;</a>
<a name="ln1910">	status_t			fStatus;</a>
<a name="ln1911">	uint32				fVecCount;</a>
<a name="ln1912">	generic_io_vec		fVecs[32]; // TODO: make dynamic/configurable</a>
<a name="ln1913">};</a>
<a name="ln1914"> </a>
<a name="ln1915"> </a>
<a name="ln1916">class PageWriteWrapper {</a>
<a name="ln1917">public:</a>
<a name="ln1918">	PageWriteWrapper();</a>
<a name="ln1919">	~PageWriteWrapper();</a>
<a name="ln1920">	void SetTo(vm_page* page);</a>
<a name="ln1921">	bool Done(status_t result);</a>
<a name="ln1922"> </a>
<a name="ln1923">private:</a>
<a name="ln1924">	vm_page*			fPage;</a>
<a name="ln1925">	struct VMCache*		fCache;</a>
<a name="ln1926">	bool				fIsActive;</a>
<a name="ln1927">};</a>
<a name="ln1928"> </a>
<a name="ln1929"> </a>
<a name="ln1930">PageWriteWrapper::PageWriteWrapper()</a>
<a name="ln1931">	:</a>
<a name="ln1932">	fIsActive(false)</a>
<a name="ln1933">{</a>
<a name="ln1934">}</a>
<a name="ln1935"> </a>
<a name="ln1936"> </a>
<a name="ln1937">PageWriteWrapper::~PageWriteWrapper()</a>
<a name="ln1938">{</a>
<a name="ln1939">	if (fIsActive)</a>
<a name="ln1940">		panic(&quot;page write wrapper going out of scope but isn't completed&quot;);</a>
<a name="ln1941">}</a>
<a name="ln1942"> </a>
<a name="ln1943"> </a>
<a name="ln1944">/*!	The page's cache must be locked.</a>
<a name="ln1945">*/</a>
<a name="ln1946">void</a>
<a name="ln1947">PageWriteWrapper::SetTo(vm_page* page)</a>
<a name="ln1948">{</a>
<a name="ln1949">	DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln1950"> </a>
<a name="ln1951">	if (page-&gt;busy)</a>
<a name="ln1952">		panic(&quot;setting page write wrapper to busy page&quot;);</a>
<a name="ln1953"> </a>
<a name="ln1954">	if (fIsActive)</a>
<a name="ln1955">		panic(&quot;re-setting page write wrapper that isn't completed&quot;);</a>
<a name="ln1956"> </a>
<a name="ln1957">	fPage = page;</a>
<a name="ln1958">	fCache = page-&gt;Cache();</a>
<a name="ln1959">	fIsActive = true;</a>
<a name="ln1960"> </a>
<a name="ln1961">	fPage-&gt;busy = true;</a>
<a name="ln1962">	fPage-&gt;busy_writing = true;</a>
<a name="ln1963"> </a>
<a name="ln1964">	// We have a modified page -- however, while we're writing it back,</a>
<a name="ln1965">	// the page might still be mapped. In order not to lose any changes to the</a>
<a name="ln1966">	// page, we mark it clean before actually writing it back; if</a>
<a name="ln1967">	// writing the page fails for some reason, we'll just keep it in the</a>
<a name="ln1968">	// modified page list, but that should happen only rarely.</a>
<a name="ln1969"> </a>
<a name="ln1970">	// If the page is changed after we cleared the dirty flag, but before we</a>
<a name="ln1971">	// had the chance to write it back, then we'll write it again later -- that</a>
<a name="ln1972">	// will probably not happen that often, though.</a>
<a name="ln1973"> </a>
<a name="ln1974">	vm_clear_map_flags(fPage, PAGE_MODIFIED);</a>
<a name="ln1975">}</a>
<a name="ln1976"> </a>
<a name="ln1977"> </a>
<a name="ln1978">/*!	The page's cache must be locked.</a>
<a name="ln1979">	The page queues must not be locked.</a>
<a name="ln1980">	\return \c true if the page was written successfully respectively could be</a>
<a name="ln1981">		handled somehow, \c false otherwise.</a>
<a name="ln1982">*/</a>
<a name="ln1983">bool</a>
<a name="ln1984">PageWriteWrapper::Done(status_t result)</a>
<a name="ln1985">{</a>
<a name="ln1986">	if (!fIsActive)</a>
<a name="ln1987">		panic(&quot;completing page write wrapper that is not active&quot;);</a>
<a name="ln1988"> </a>
<a name="ln1989">	DEBUG_PAGE_ACCESS_START(fPage);</a>
<a name="ln1990"> </a>
<a name="ln1991">	fPage-&gt;busy = false;</a>
<a name="ln1992">		// Set unbusy and notify later by hand, since we might free the page.</a>
<a name="ln1993"> </a>
<a name="ln1994">	bool success = true;</a>
<a name="ln1995"> </a>
<a name="ln1996">	if (result == B_OK) {</a>
<a name="ln1997">		// put it into the active/inactive queue</a>
<a name="ln1998">		move_page_to_appropriate_queue(fPage);</a>
<a name="ln1999">		fPage-&gt;busy_writing = false;</a>
<a name="ln2000">		DEBUG_PAGE_ACCESS_END(fPage);</a>
<a name="ln2001">	} else {</a>
<a name="ln2002">		// Writing the page failed. One reason would be that the cache has been</a>
<a name="ln2003">		// shrunk and the page does no longer belong to the file. Otherwise the</a>
<a name="ln2004">		// actual I/O failed, in which case we'll simply keep the page modified.</a>
<a name="ln2005"> </a>
<a name="ln2006">		if (!fPage-&gt;busy_writing) {</a>
<a name="ln2007">			// The busy_writing flag was cleared. That means the cache has been</a>
<a name="ln2008">			// shrunk while we were trying to write the page and we have to free</a>
<a name="ln2009">			// it now.</a>
<a name="ln2010">			vm_remove_all_page_mappings(fPage);</a>
<a name="ln2011">// TODO: Unmapping should already happen when resizing the cache!</a>
<a name="ln2012">			fCache-&gt;RemovePage(fPage);</a>
<a name="ln2013">			free_page(fPage, false);</a>
<a name="ln2014">			unreserve_pages(1);</a>
<a name="ln2015">		} else {</a>
<a name="ln2016">			// Writing the page failed -- mark the page modified and move it to</a>
<a name="ln2017">			// an appropriate queue other than the modified queue, so we don't</a>
<a name="ln2018">			// keep trying to write it over and over again. We keep</a>
<a name="ln2019">			// non-temporary pages in the modified queue, though, so they don't</a>
<a name="ln2020">			// get lost in the inactive queue.</a>
<a name="ln2021">			dprintf(&quot;PageWriteWrapper: Failed to write page %p: %s\n&quot;, fPage,</a>
<a name="ln2022">				strerror(result));</a>
<a name="ln2023"> </a>
<a name="ln2024">			fPage-&gt;modified = true;</a>
<a name="ln2025">			if (!fCache-&gt;temporary)</a>
<a name="ln2026">				set_page_state(fPage, PAGE_STATE_MODIFIED);</a>
<a name="ln2027">			else if (fPage-&gt;IsMapped())</a>
<a name="ln2028">				set_page_state(fPage, PAGE_STATE_ACTIVE);</a>
<a name="ln2029">			else</a>
<a name="ln2030">				set_page_state(fPage, PAGE_STATE_INACTIVE);</a>
<a name="ln2031"> </a>
<a name="ln2032">			fPage-&gt;busy_writing = false;</a>
<a name="ln2033">			DEBUG_PAGE_ACCESS_END(fPage);</a>
<a name="ln2034"> </a>
<a name="ln2035">			success = false;</a>
<a name="ln2036">		}</a>
<a name="ln2037">	}</a>
<a name="ln2038"> </a>
<a name="ln2039">	fCache-&gt;NotifyPageEvents(fPage, PAGE_EVENT_NOT_BUSY);</a>
<a name="ln2040">	fIsActive = false;</a>
<a name="ln2041"> </a>
<a name="ln2042">	return success;</a>
<a name="ln2043">}</a>
<a name="ln2044"> </a>
<a name="ln2045"> </a>
<a name="ln2046">/*!	The page's cache must be locked.</a>
<a name="ln2047">*/</a>
<a name="ln2048">void</a>
<a name="ln2049">PageWriteTransfer::SetTo(PageWriterRun* run, vm_page* page, int32 maxPages)</a>
<a name="ln2050">{</a>
<a name="ln2051">	fRun = run;</a>
<a name="ln2052">	fCache = page-&gt;Cache();</a>
<a name="ln2053">	fOffset = page-&gt;cache_offset;</a>
<a name="ln2054">	fPageCount = 1;</a>
<a name="ln2055">	fMaxPages = maxPages;</a>
<a name="ln2056">	fStatus = B_OK;</a>
<a name="ln2057"> </a>
<a name="ln2058">	fVecs[0].base = (phys_addr_t)page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT;</a>
<a name="ln2059">	fVecs[0].length = B_PAGE_SIZE;</a>
<a name="ln2060">	fVecCount = 1;</a>
<a name="ln2061">}</a>
<a name="ln2062"> </a>
<a name="ln2063"> </a>
<a name="ln2064">/*!	The page's cache must be locked.</a>
<a name="ln2065">*/</a>
<a name="ln2066">bool</a>
<a name="ln2067">PageWriteTransfer::AddPage(vm_page* page)</a>
<a name="ln2068">{</a>
<a name="ln2069">	if (page-&gt;Cache() != fCache</a>
<a name="ln2070">		|| (fMaxPages &gt;= 0 &amp;&amp; fPageCount &gt;= (uint32)fMaxPages))</a>
<a name="ln2071">		return false;</a>
<a name="ln2072"> </a>
<a name="ln2073">	phys_addr_t nextBase = fVecs[fVecCount - 1].base</a>
<a name="ln2074">		+ fVecs[fVecCount - 1].length;</a>
<a name="ln2075"> </a>
<a name="ln2076">	if ((phys_addr_t)page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT == nextBase</a>
<a name="ln2077">		&amp;&amp; (off_t)page-&gt;cache_offset == fOffset + fPageCount) {</a>
<a name="ln2078">		// append to last iovec</a>
<a name="ln2079">		fVecs[fVecCount - 1].length += B_PAGE_SIZE;</a>
<a name="ln2080">		fPageCount++;</a>
<a name="ln2081">		return true;</a>
<a name="ln2082">	}</a>
<a name="ln2083"> </a>
<a name="ln2084">	nextBase = fVecs[0].base - B_PAGE_SIZE;</a>
<a name="ln2085">	if ((phys_addr_t)page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT == nextBase</a>
<a name="ln2086">		&amp;&amp; (off_t)page-&gt;cache_offset == fOffset - 1) {</a>
<a name="ln2087">		// prepend to first iovec and adjust offset</a>
<a name="ln2088">		fVecs[0].base = nextBase;</a>
<a name="ln2089">		fVecs[0].length += B_PAGE_SIZE;</a>
<a name="ln2090">		fOffset = page-&gt;cache_offset;</a>
<a name="ln2091">		fPageCount++;</a>
<a name="ln2092">		return true;</a>
<a name="ln2093">	}</a>
<a name="ln2094"> </a>
<a name="ln2095">	if (((off_t)page-&gt;cache_offset == fOffset + fPageCount</a>
<a name="ln2096">			|| (off_t)page-&gt;cache_offset == fOffset - 1)</a>
<a name="ln2097">		&amp;&amp; fVecCount &lt; sizeof(fVecs) / sizeof(fVecs[0])) {</a>
<a name="ln2098">		// not physically contiguous or not in the right order</a>
<a name="ln2099">		uint32 vectorIndex;</a>
<a name="ln2100">		if ((off_t)page-&gt;cache_offset &lt; fOffset) {</a>
<a name="ln2101">			// we are pre-pending another vector, move the other vecs</a>
<a name="ln2102">			for (uint32 i = fVecCount; i &gt; 0; i--)</a>
<a name="ln2103">				fVecs[i] = fVecs[i - 1];</a>
<a name="ln2104"> </a>
<a name="ln2105">			fOffset = page-&gt;cache_offset;</a>
<a name="ln2106">			vectorIndex = 0;</a>
<a name="ln2107">		} else</a>
<a name="ln2108">			vectorIndex = fVecCount;</a>
<a name="ln2109"> </a>
<a name="ln2110">		fVecs[vectorIndex].base</a>
<a name="ln2111">			= (phys_addr_t)page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT;</a>
<a name="ln2112">		fVecs[vectorIndex].length = B_PAGE_SIZE;</a>
<a name="ln2113"> </a>
<a name="ln2114">		fVecCount++;</a>
<a name="ln2115">		fPageCount++;</a>
<a name="ln2116">		return true;</a>
<a name="ln2117">	}</a>
<a name="ln2118"> </a>
<a name="ln2119">	return false;</a>
<a name="ln2120">}</a>
<a name="ln2121"> </a>
<a name="ln2122"> </a>
<a name="ln2123">status_t</a>
<a name="ln2124">PageWriteTransfer::Schedule(uint32 flags)</a>
<a name="ln2125">{</a>
<a name="ln2126">	off_t writeOffset = (off_t)fOffset &lt;&lt; PAGE_SHIFT;</a>
<a name="ln2127">	generic_size_t writeLength = (phys_size_t)fPageCount &lt;&lt; PAGE_SHIFT;</a>
<a name="ln2128"> </a>
<a name="ln2129">	if (fRun != NULL) {</a>
<a name="ln2130">		return fCache-&gt;WriteAsync(writeOffset, fVecs, fVecCount, writeLength,</a>
<a name="ln2131">			flags | B_PHYSICAL_IO_REQUEST, this);</a>
<a name="ln2132">	}</a>
<a name="ln2133"> </a>
<a name="ln2134">	status_t status = fCache-&gt;Write(writeOffset, fVecs, fVecCount,</a>
<a name="ln2135">		flags | B_PHYSICAL_IO_REQUEST, &amp;writeLength);</a>
<a name="ln2136"> </a>
<a name="ln2137">	SetStatus(status, writeLength);</a>
<a name="ln2138">	return fStatus;</a>
<a name="ln2139">}</a>
<a name="ln2140"> </a>
<a name="ln2141"> </a>
<a name="ln2142">void</a>
<a name="ln2143">PageWriteTransfer::SetStatus(status_t status, size_t transferred)</a>
<a name="ln2144">{</a>
<a name="ln2145">	// only succeed if all pages up to the last one have been written fully</a>
<a name="ln2146">	// and the last page has at least been written partially</a>
<a name="ln2147">	if (status == B_OK &amp;&amp; transferred &lt;= (fPageCount - 1) * B_PAGE_SIZE)</a>
<a name="ln2148">		status = B_ERROR;</a>
<a name="ln2149"> </a>
<a name="ln2150">	fStatus = status;</a>
<a name="ln2151">}</a>
<a name="ln2152"> </a>
<a name="ln2153"> </a>
<a name="ln2154">void</a>
<a name="ln2155">PageWriteTransfer::IOFinished(status_t status, bool partialTransfer,</a>
<a name="ln2156">	generic_size_t bytesTransferred)</a>
<a name="ln2157">{</a>
<a name="ln2158">	SetStatus(status, bytesTransferred);</a>
<a name="ln2159">	fRun-&gt;PageWritten(this, fStatus, partialTransfer, bytesTransferred);</a>
<a name="ln2160">}</a>
<a name="ln2161"> </a>
<a name="ln2162"> </a>
<a name="ln2163">status_t</a>
<a name="ln2164">PageWriterRun::Init(uint32 maxPages)</a>
<a name="ln2165">{</a>
<a name="ln2166">	fMaxPages = maxPages;</a>
<a name="ln2167">	fWrapperCount = 0;</a>
<a name="ln2168">	fTransferCount = 0;</a>
<a name="ln2169">	fPendingTransfers = 0;</a>
<a name="ln2170"> </a>
<a name="ln2171">	fWrappers = new(std::nothrow) PageWriteWrapper[maxPages];</a>
<a name="ln2172">	fTransfers = new(std::nothrow) PageWriteTransfer[maxPages];</a>
<a name="ln2173">	if (fWrappers == NULL || fTransfers == NULL)</a>
<a name="ln2174">		return B_NO_MEMORY;</a>
<a name="ln2175"> </a>
<a name="ln2176">	return B_OK;</a>
<a name="ln2177">}</a>
<a name="ln2178"> </a>
<a name="ln2179"> </a>
<a name="ln2180">void</a>
<a name="ln2181">PageWriterRun::PrepareNextRun()</a>
<a name="ln2182">{</a>
<a name="ln2183">	fWrapperCount = 0;</a>
<a name="ln2184">	fTransferCount = 0;</a>
<a name="ln2185">	fPendingTransfers = 0;</a>
<a name="ln2186">}</a>
<a name="ln2187"> </a>
<a name="ln2188"> </a>
<a name="ln2189">/*!	The page's cache must be locked.</a>
<a name="ln2190">*/</a>
<a name="ln2191">void</a>
<a name="ln2192">PageWriterRun::AddPage(vm_page* page)</a>
<a name="ln2193">{</a>
<a name="ln2194">	fWrappers[fWrapperCount++].SetTo(page);</a>
<a name="ln2195"> </a>
<a name="ln2196">	if (fTransferCount == 0 || !fTransfers[fTransferCount - 1].AddPage(page)) {</a>
<a name="ln2197">		fTransfers[fTransferCount++].SetTo(this, page,</a>
<a name="ln2198">			page-&gt;Cache()-&gt;MaxPagesPerAsyncWrite());</a>
<a name="ln2199">	}</a>
<a name="ln2200">}</a>
<a name="ln2201"> </a>
<a name="ln2202"> </a>
<a name="ln2203">/*!	Writes all pages previously added.</a>
<a name="ln2204">	\return The number of pages that could not be written or otherwise handled.</a>
<a name="ln2205">*/</a>
<a name="ln2206">uint32</a>
<a name="ln2207">PageWriterRun::Go()</a>
<a name="ln2208">{</a>
<a name="ln2209">	atomic_set(&amp;fPendingTransfers, fTransferCount);</a>
<a name="ln2210"> </a>
<a name="ln2211">	fAllFinishedCondition.Init(this, &quot;page writer wait for I/O&quot;);</a>
<a name="ln2212">	ConditionVariableEntry waitEntry;</a>
<a name="ln2213">	fAllFinishedCondition.Add(&amp;waitEntry);</a>
<a name="ln2214"> </a>
<a name="ln2215">	// schedule writes</a>
<a name="ln2216">	for (uint32 i = 0; i &lt; fTransferCount; i++)</a>
<a name="ln2217">		fTransfers[i].Schedule(B_VIP_IO_REQUEST);</a>
<a name="ln2218"> </a>
<a name="ln2219">	// wait until all pages have been written</a>
<a name="ln2220">	waitEntry.Wait();</a>
<a name="ln2221"> </a>
<a name="ln2222">	// mark pages depending on whether they could be written or not</a>
<a name="ln2223"> </a>
<a name="ln2224">	uint32 failedPages = 0;</a>
<a name="ln2225">	uint32 wrapperIndex = 0;</a>
<a name="ln2226">	for (uint32 i = 0; i &lt; fTransferCount; i++) {</a>
<a name="ln2227">		PageWriteTransfer&amp; transfer = fTransfers[i];</a>
<a name="ln2228">		transfer.Cache()-&gt;Lock();</a>
<a name="ln2229"> </a>
<a name="ln2230">		for (uint32 j = 0; j &lt; transfer.PageCount(); j++) {</a>
<a name="ln2231">			if (!fWrappers[wrapperIndex++].Done(transfer.Status()))</a>
<a name="ln2232">				failedPages++;</a>
<a name="ln2233">		}</a>
<a name="ln2234"> </a>
<a name="ln2235">		transfer.Cache()-&gt;Unlock();</a>
<a name="ln2236">	}</a>
<a name="ln2237"> </a>
<a name="ln2238">	ASSERT(wrapperIndex == fWrapperCount);</a>
<a name="ln2239"> </a>
<a name="ln2240">	for (uint32 i = 0; i &lt; fTransferCount; i++) {</a>
<a name="ln2241">		PageWriteTransfer&amp; transfer = fTransfers[i];</a>
<a name="ln2242">		struct VMCache* cache = transfer.Cache();</a>
<a name="ln2243"> </a>
<a name="ln2244">		// We've acquired a references for each page</a>
<a name="ln2245">		for (uint32 j = 0; j &lt; transfer.PageCount(); j++) {</a>
<a name="ln2246">			// We release the cache references after all pages were made</a>
<a name="ln2247">			// unbusy again - otherwise releasing a vnode could deadlock.</a>
<a name="ln2248">			cache-&gt;ReleaseStoreRef();</a>
<a name="ln2249">			cache-&gt;ReleaseRef();</a>
<a name="ln2250">		}</a>
<a name="ln2251">	}</a>
<a name="ln2252"> </a>
<a name="ln2253">	return failedPages;</a>
<a name="ln2254">}</a>
<a name="ln2255"> </a>
<a name="ln2256"> </a>
<a name="ln2257">void</a>
<a name="ln2258">PageWriterRun::PageWritten(PageWriteTransfer* transfer, status_t status,</a>
<a name="ln2259">	bool partialTransfer, size_t bytesTransferred)</a>
<a name="ln2260">{</a>
<a name="ln2261">	if (atomic_add(&amp;fPendingTransfers, -1) == 1)</a>
<a name="ln2262">		fAllFinishedCondition.NotifyAll();</a>
<a name="ln2263">}</a>
<a name="ln2264"> </a>
<a name="ln2265"> </a>
<a name="ln2266">/*!	The page writer continuously takes some pages from the modified</a>
<a name="ln2267">	queue, writes them back, and moves them back to the active queue.</a>
<a name="ln2268">	It runs in its own thread, and is only there to keep the number</a>
<a name="ln2269">	of modified pages low, so that more pages can be reused with</a>
<a name="ln2270">	fewer costs.</a>
<a name="ln2271">*/</a>
<a name="ln2272">status_t</a>
<a name="ln2273">page_writer(void* /*unused*/)</a>
<a name="ln2274">{</a>
<a name="ln2275">	const uint32 kNumPages = 256;</a>
<a name="ln2276">#ifdef TRACE_VM_PAGE</a>
<a name="ln2277">	uint32 writtenPages = 0;</a>
<a name="ln2278">	bigtime_t lastWrittenTime = 0;</a>
<a name="ln2279">	bigtime_t pageCollectionTime = 0;</a>
<a name="ln2280">	bigtime_t pageWritingTime = 0;</a>
<a name="ln2281">#endif</a>
<a name="ln2282"> </a>
<a name="ln2283">	PageWriterRun run;</a>
<a name="ln2284">	if (run.Init(kNumPages) != B_OK) {</a>
<a name="ln2285">		panic(&quot;page writer: Failed to init PageWriterRun!&quot;);</a>
<a name="ln2286">		return B_ERROR;</a>
<a name="ln2287">	}</a>
<a name="ln2288"> </a>
<a name="ln2289">	page_num_t pagesSinceLastSuccessfulWrite = 0;</a>
<a name="ln2290"> </a>
<a name="ln2291">	while (true) {</a>
<a name="ln2292">// TODO: Maybe wait shorter when memory is low!</a>
<a name="ln2293">		if (sModifiedPageQueue.Count() &lt; kNumPages) {</a>
<a name="ln2294">			sPageWriterCondition.Wait(3000000, true);</a>
<a name="ln2295">				// all 3 seconds when no one triggers us</a>
<a name="ln2296">		}</a>
<a name="ln2297"> </a>
<a name="ln2298">		page_num_t modifiedPages = sModifiedPageQueue.Count();</a>
<a name="ln2299">		if (modifiedPages == 0)</a>
<a name="ln2300">			continue;</a>
<a name="ln2301"> </a>
<a name="ln2302">		if (modifiedPages &lt;= pagesSinceLastSuccessfulWrite) {</a>
<a name="ln2303">			// We ran through the whole queue without being able to write a</a>
<a name="ln2304">			// single page. Take a break.</a>
<a name="ln2305">			snooze(500000);</a>
<a name="ln2306">			pagesSinceLastSuccessfulWrite = 0;</a>
<a name="ln2307">		}</a>
<a name="ln2308"> </a>
<a name="ln2309">#if ENABLE_SWAP_SUPPORT</a>
<a name="ln2310">		page_stats pageStats;</a>
<a name="ln2311">		get_page_stats(pageStats);</a>
<a name="ln2312">		bool activePaging = do_active_paging(pageStats);</a>
<a name="ln2313">#endif</a>
<a name="ln2314"> </a>
<a name="ln2315">		// depending on how urgent it becomes to get pages to disk, we adjust</a>
<a name="ln2316">		// our I/O priority</a>
<a name="ln2317">		uint32 lowPagesState = low_resource_state(B_KERNEL_RESOURCE_PAGES);</a>
<a name="ln2318">		int32 ioPriority = B_IDLE_PRIORITY;</a>
<a name="ln2319">		if (lowPagesState &gt;= B_LOW_RESOURCE_CRITICAL</a>
<a name="ln2320">			|| modifiedPages &gt; MAX_PAGE_WRITER_IO_PRIORITY_THRESHOLD) {</a>
<a name="ln2321">			ioPriority = MAX_PAGE_WRITER_IO_PRIORITY;</a>
<a name="ln2322">		} else {</a>
<a name="ln2323">			ioPriority = (uint64)MAX_PAGE_WRITER_IO_PRIORITY * modifiedPages</a>
<a name="ln2324">				/ MAX_PAGE_WRITER_IO_PRIORITY_THRESHOLD;</a>
<a name="ln2325">		}</a>
<a name="ln2326"> </a>
<a name="ln2327">		thread_set_io_priority(ioPriority);</a>
<a name="ln2328"> </a>
<a name="ln2329">		uint32 numPages = 0;</a>
<a name="ln2330">		run.PrepareNextRun();</a>
<a name="ln2331"> </a>
<a name="ln2332">		// TODO: make this laptop friendly, too (ie. only start doing</a>
<a name="ln2333">		// something if someone else did something or there is really</a>
<a name="ln2334">		// enough to do).</a>
<a name="ln2335"> </a>
<a name="ln2336">		// collect pages to be written</a>
<a name="ln2337">#ifdef TRACE_VM_PAGE</a>
<a name="ln2338">		pageCollectionTime -= system_time();</a>
<a name="ln2339">#endif</a>
<a name="ln2340"> </a>
<a name="ln2341">		page_num_t maxPagesToSee = modifiedPages;</a>
<a name="ln2342"> </a>
<a name="ln2343">		while (numPages &lt; kNumPages &amp;&amp; maxPagesToSee &gt; 0) {</a>
<a name="ln2344">			vm_page *page = next_modified_page(maxPagesToSee);</a>
<a name="ln2345">			if (page == NULL)</a>
<a name="ln2346">				break;</a>
<a name="ln2347"> </a>
<a name="ln2348">			PageCacheLocker cacheLocker(page, false);</a>
<a name="ln2349">			if (!cacheLocker.IsLocked())</a>
<a name="ln2350">				continue;</a>
<a name="ln2351"> </a>
<a name="ln2352">			VMCache *cache = page-&gt;Cache();</a>
<a name="ln2353"> </a>
<a name="ln2354">			// If the page is busy or its state has changed while we were</a>
<a name="ln2355">			// locking the cache, just ignore it.</a>
<a name="ln2356">			if (page-&gt;busy || page-&gt;State() != PAGE_STATE_MODIFIED)</a>
<a name="ln2357">				continue;</a>
<a name="ln2358"> </a>
<a name="ln2359">			DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2360"> </a>
<a name="ln2361">			// Don't write back wired (locked) pages.</a>
<a name="ln2362">			if (page-&gt;WiredCount() &gt; 0) {</a>
<a name="ln2363">				set_page_state(page, PAGE_STATE_ACTIVE);</a>
<a name="ln2364">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2365">				continue;</a>
<a name="ln2366">			}</a>
<a name="ln2367"> </a>
<a name="ln2368">			// Write back temporary pages only when we're actively paging.</a>
<a name="ln2369">			if (cache-&gt;temporary</a>
<a name="ln2370">#if ENABLE_SWAP_SUPPORT</a>
<a name="ln2371">				&amp;&amp; (!activePaging</a>
<a name="ln2372">					|| !cache-&gt;CanWritePage(</a>
<a name="ln2373">							(off_t)page-&gt;cache_offset &lt;&lt; PAGE_SHIFT))</a>
<a name="ln2374">#endif</a>
<a name="ln2375">				) {</a>
<a name="ln2376">				// We can't/don't want to do anything with this page, so move it</a>
<a name="ln2377">				// to one of the other queues.</a>
<a name="ln2378">				if (page-&gt;mappings.IsEmpty())</a>
<a name="ln2379">					set_page_state(page, PAGE_STATE_INACTIVE);</a>
<a name="ln2380">				else</a>
<a name="ln2381">					set_page_state(page, PAGE_STATE_ACTIVE);</a>
<a name="ln2382"> </a>
<a name="ln2383">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2384">				continue;</a>
<a name="ln2385">			}</a>
<a name="ln2386"> </a>
<a name="ln2387">			// We need our own reference to the store, as it might currently be</a>
<a name="ln2388">			// destroyed.</a>
<a name="ln2389">			if (cache-&gt;AcquireUnreferencedStoreRef() != B_OK) {</a>
<a name="ln2390">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2391">				cacheLocker.Unlock();</a>
<a name="ln2392">				thread_yield();</a>
<a name="ln2393">				continue;</a>
<a name="ln2394">			}</a>
<a name="ln2395"> </a>
<a name="ln2396">			run.AddPage(page);</a>
<a name="ln2397">				// TODO: We're possibly adding pages of different caches and</a>
<a name="ln2398">				// thus maybe of different underlying file systems here. This</a>
<a name="ln2399">				// is a potential problem for loop file systems/devices, since</a>
<a name="ln2400">				// we could mark a page busy that would need to be accessed</a>
<a name="ln2401">				// when writing back another page, thus causing a deadlock.</a>
<a name="ln2402"> </a>
<a name="ln2403">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2404"> </a>
<a name="ln2405">			//dprintf(&quot;write page %p, cache %p (%ld)\n&quot;, page, page-&gt;cache, page-&gt;cache-&gt;ref_count);</a>
<a name="ln2406">			TPW(WritePage(page));</a>
<a name="ln2407"> </a>
<a name="ln2408">			cache-&gt;AcquireRefLocked();</a>
<a name="ln2409">			numPages++;</a>
<a name="ln2410">		}</a>
<a name="ln2411"> </a>
<a name="ln2412">#ifdef TRACE_VM_PAGE</a>
<a name="ln2413">		pageCollectionTime += system_time();</a>
<a name="ln2414">#endif</a>
<a name="ln2415">		if (numPages == 0)</a>
<a name="ln2416">			continue;</a>
<a name="ln2417"> </a>
<a name="ln2418">		// write pages to disk and do all the cleanup</a>
<a name="ln2419">#ifdef TRACE_VM_PAGE</a>
<a name="ln2420">		pageWritingTime -= system_time();</a>
<a name="ln2421">#endif</a>
<a name="ln2422">		uint32 failedPages = run.Go();</a>
<a name="ln2423">#ifdef TRACE_VM_PAGE</a>
<a name="ln2424">		pageWritingTime += system_time();</a>
<a name="ln2425"> </a>
<a name="ln2426">		// debug output only...</a>
<a name="ln2427">		writtenPages += numPages;</a>
<a name="ln2428">		if (writtenPages &gt;= 1024) {</a>
<a name="ln2429">			bigtime_t now = system_time();</a>
<a name="ln2430">			TRACE((&quot;page writer: wrote 1024 pages (total: %&quot; B_PRIu64 &quot; ms, &quot;</a>
<a name="ln2431">				&quot;collect: %&quot; B_PRIu64 &quot; ms, write: %&quot; B_PRIu64 &quot; ms)\n&quot;,</a>
<a name="ln2432">				(now - lastWrittenTime) / 1000,</a>
<a name="ln2433">				pageCollectionTime / 1000, pageWritingTime / 1000));</a>
<a name="ln2434">			lastWrittenTime = now;</a>
<a name="ln2435"> </a>
<a name="ln2436">			writtenPages -= 1024;</a>
<a name="ln2437">			pageCollectionTime = 0;</a>
<a name="ln2438">			pageWritingTime = 0;</a>
<a name="ln2439">		}</a>
<a name="ln2440">#endif</a>
<a name="ln2441"> </a>
<a name="ln2442">		if (failedPages == numPages)</a>
<a name="ln2443">			pagesSinceLastSuccessfulWrite += modifiedPages - maxPagesToSee;</a>
<a name="ln2444">		else</a>
<a name="ln2445">			pagesSinceLastSuccessfulWrite = 0;</a>
<a name="ln2446">	}</a>
<a name="ln2447"> </a>
<a name="ln2448">	return B_OK;</a>
<a name="ln2449">}</a>
<a name="ln2450"> </a>
<a name="ln2451"> </a>
<a name="ln2452">// #pragma mark -</a>
<a name="ln2453"> </a>
<a name="ln2454"> </a>
<a name="ln2455">// TODO: This should be done in the page daemon!</a>
<a name="ln2456">#if 0</a>
<a name="ln2457">#if ENABLE_SWAP_SUPPORT</a>
<a name="ln2458">static bool</a>
<a name="ln2459">free_page_swap_space(int32 index)</a>
<a name="ln2460">{</a>
<a name="ln2461">	vm_page *page = vm_page_at_index(index);</a>
<a name="ln2462">	PageCacheLocker locker(page);</a>
<a name="ln2463">	if (!locker.IsLocked())</a>
<a name="ln2464">		return false;</a>
<a name="ln2465"> </a>
<a name="ln2466">	DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2467"> </a>
<a name="ln2468">	VMCache* cache = page-&gt;Cache();</a>
<a name="ln2469">	if (cache-&gt;temporary &amp;&amp; page-&gt;WiredCount() == 0</a>
<a name="ln2470">			&amp;&amp; cache-&gt;HasPage(page-&gt;cache_offset &lt;&lt; PAGE_SHIFT)</a>
<a name="ln2471">			&amp;&amp; page-&gt;usage_count &gt; 0) {</a>
<a name="ln2472">		// TODO: how to judge a page is highly active?</a>
<a name="ln2473">		if (swap_free_page_swap_space(page)) {</a>
<a name="ln2474">			// We need to mark the page modified, since otherwise it could be</a>
<a name="ln2475">			// stolen and we'd lose its data.</a>
<a name="ln2476">			vm_page_set_state(page, PAGE_STATE_MODIFIED);</a>
<a name="ln2477">			TD(FreedPageSwap(page));</a>
<a name="ln2478">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2479">			return true;</a>
<a name="ln2480">		}</a>
<a name="ln2481">	}</a>
<a name="ln2482">	DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2483">	return false;</a>
<a name="ln2484">}</a>
<a name="ln2485">#endif</a>
<a name="ln2486">#endif	// 0</a>
<a name="ln2487"> </a>
<a name="ln2488"> </a>
<a name="ln2489">static vm_page *</a>
<a name="ln2490">find_cached_page_candidate(struct vm_page &amp;marker)</a>
<a name="ln2491">{</a>
<a name="ln2492">	DEBUG_PAGE_ACCESS_CHECK(&amp;marker);</a>
<a name="ln2493"> </a>
<a name="ln2494">	InterruptsSpinLocker locker(sCachedPageQueue.GetLock());</a>
<a name="ln2495">	vm_page *page;</a>
<a name="ln2496"> </a>
<a name="ln2497">	if (marker.State() == PAGE_STATE_UNUSED) {</a>
<a name="ln2498">		// Get the first free pages of the (in)active queue</a>
<a name="ln2499">		page = sCachedPageQueue.Head();</a>
<a name="ln2500">	} else {</a>
<a name="ln2501">		// Get the next page of the current queue</a>
<a name="ln2502">		if (marker.State() != PAGE_STATE_CACHED) {</a>
<a name="ln2503">			panic(&quot;invalid marker %p state&quot;, &amp;marker);</a>
<a name="ln2504">			return NULL;</a>
<a name="ln2505">		}</a>
<a name="ln2506"> </a>
<a name="ln2507">		page = sCachedPageQueue.Next(&amp;marker);</a>
<a name="ln2508">		sCachedPageQueue.Remove(&amp;marker);</a>
<a name="ln2509">		marker.SetState(PAGE_STATE_UNUSED);</a>
<a name="ln2510">	}</a>
<a name="ln2511"> </a>
<a name="ln2512">	while (page != NULL) {</a>
<a name="ln2513">		if (!page-&gt;busy) {</a>
<a name="ln2514">			// we found a candidate, insert marker</a>
<a name="ln2515">			marker.SetState(PAGE_STATE_CACHED);</a>
<a name="ln2516">			sCachedPageQueue.InsertAfter(page, &amp;marker);</a>
<a name="ln2517">			return page;</a>
<a name="ln2518">		}</a>
<a name="ln2519"> </a>
<a name="ln2520">		page = sCachedPageQueue.Next(page);</a>
<a name="ln2521">	}</a>
<a name="ln2522"> </a>
<a name="ln2523">	return NULL;</a>
<a name="ln2524">}</a>
<a name="ln2525"> </a>
<a name="ln2526"> </a>
<a name="ln2527">static bool</a>
<a name="ln2528">free_cached_page(vm_page *page, bool dontWait)</a>
<a name="ln2529">{</a>
<a name="ln2530">	// try to lock the page's cache</a>
<a name="ln2531">	if (vm_cache_acquire_locked_page_cache(page, dontWait) == NULL)</a>
<a name="ln2532">		return false;</a>
<a name="ln2533">	VMCache* cache = page-&gt;Cache();</a>
<a name="ln2534"> </a>
<a name="ln2535">	AutoLocker&lt;VMCache&gt; cacheLocker(cache, true);</a>
<a name="ln2536">	MethodDeleter&lt;VMCache&gt; _2(cache, &amp;VMCache::ReleaseRefLocked);</a>
<a name="ln2537"> </a>
<a name="ln2538">	// check again if that page is still a candidate</a>
<a name="ln2539">	if (page-&gt;busy || page-&gt;State() != PAGE_STATE_CACHED)</a>
<a name="ln2540">		return false;</a>
<a name="ln2541"> </a>
<a name="ln2542">	DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2543"> </a>
<a name="ln2544">	PAGE_ASSERT(page, !page-&gt;IsMapped());</a>
<a name="ln2545">	PAGE_ASSERT(page, !page-&gt;modified);</a>
<a name="ln2546"> </a>
<a name="ln2547">	// we can now steal this page</a>
<a name="ln2548"> </a>
<a name="ln2549">	cache-&gt;RemovePage(page);</a>
<a name="ln2550">		// Now the page doesn't have cache anymore, so no one else (e.g.</a>
<a name="ln2551">		// vm_page_allocate_page_run() can pick it up), since they would be</a>
<a name="ln2552">		// required to lock the cache first, which would fail.</a>
<a name="ln2553"> </a>
<a name="ln2554">	sCachedPageQueue.RemoveUnlocked(page);</a>
<a name="ln2555">	return true;</a>
<a name="ln2556">}</a>
<a name="ln2557"> </a>
<a name="ln2558"> </a>
<a name="ln2559">static uint32</a>
<a name="ln2560">free_cached_pages(uint32 pagesToFree, bool dontWait)</a>
<a name="ln2561">{</a>
<a name="ln2562">	vm_page marker;</a>
<a name="ln2563">	init_page_marker(marker);</a>
<a name="ln2564"> </a>
<a name="ln2565">	uint32 pagesFreed = 0;</a>
<a name="ln2566"> </a>
<a name="ln2567">	while (pagesFreed &lt; pagesToFree) {</a>
<a name="ln2568">		vm_page *page = find_cached_page_candidate(marker);</a>
<a name="ln2569">		if (page == NULL)</a>
<a name="ln2570">			break;</a>
<a name="ln2571"> </a>
<a name="ln2572">		if (free_cached_page(page, dontWait)) {</a>
<a name="ln2573">			ReadLocker locker(sFreePageQueuesLock);</a>
<a name="ln2574">			page-&gt;SetState(PAGE_STATE_FREE);</a>
<a name="ln2575">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2576">			sFreePageQueue.PrependUnlocked(page);</a>
<a name="ln2577">			locker.Unlock();</a>
<a name="ln2578"> </a>
<a name="ln2579">			TA(StolenPage());</a>
<a name="ln2580"> </a>
<a name="ln2581">			pagesFreed++;</a>
<a name="ln2582">		}</a>
<a name="ln2583">	}</a>
<a name="ln2584"> </a>
<a name="ln2585">	remove_page_marker(marker);</a>
<a name="ln2586"> </a>
<a name="ln2587">	return pagesFreed;</a>
<a name="ln2588">}</a>
<a name="ln2589"> </a>
<a name="ln2590"> </a>
<a name="ln2591">static void</a>
<a name="ln2592">idle_scan_active_pages(page_stats&amp; pageStats)</a>
<a name="ln2593">{</a>
<a name="ln2594">	VMPageQueue&amp; queue = sActivePageQueue;</a>
<a name="ln2595"> </a>
<a name="ln2596">	// We want to scan the whole queue in roughly kIdleRunsForFullQueue runs.</a>
<a name="ln2597">	uint32 maxToScan = queue.Count() / kIdleRunsForFullQueue + 1;</a>
<a name="ln2598"> </a>
<a name="ln2599">	while (maxToScan &gt; 0) {</a>
<a name="ln2600">		maxToScan--;</a>
<a name="ln2601"> </a>
<a name="ln2602">		// Get the next page. Note that we don't bother to lock here. We go with</a>
<a name="ln2603">		// the assumption that on all architectures reading/writing pointers is</a>
<a name="ln2604">		// atomic. Beyond that it doesn't really matter. We have to unlock the</a>
<a name="ln2605">		// queue anyway to lock the page's cache, and we'll recheck afterwards.</a>
<a name="ln2606">		vm_page* page = queue.Head();</a>
<a name="ln2607">		if (page == NULL)</a>
<a name="ln2608">			break;</a>
<a name="ln2609"> </a>
<a name="ln2610">		// lock the page's cache</a>
<a name="ln2611">		VMCache* cache = vm_cache_acquire_locked_page_cache(page, true);</a>
<a name="ln2612">		if (cache == NULL)</a>
<a name="ln2613">			continue;</a>
<a name="ln2614"> </a>
<a name="ln2615">		if (page-&gt;State() != PAGE_STATE_ACTIVE) {</a>
<a name="ln2616">			// page is no longer in the cache or in this queue</a>
<a name="ln2617">			cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2618">			continue;</a>
<a name="ln2619">		}</a>
<a name="ln2620"> </a>
<a name="ln2621">		if (page-&gt;busy) {</a>
<a name="ln2622">			// page is busy -- requeue at the end</a>
<a name="ln2623">			vm_page_requeue(page, true);</a>
<a name="ln2624">			cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2625">			continue;</a>
<a name="ln2626">		}</a>
<a name="ln2627"> </a>
<a name="ln2628">		DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2629"> </a>
<a name="ln2630">		// Get the page active/modified flags and update the page's usage count.</a>
<a name="ln2631">		// We completely unmap inactive temporary pages. This saves us to</a>
<a name="ln2632">		// iterate through the inactive list as well, since we'll be notified</a>
<a name="ln2633">		// via page fault whenever such an inactive page is used again.</a>
<a name="ln2634">		// We don't remove the mappings of non-temporary pages, since we</a>
<a name="ln2635">		// wouldn't notice when those would become unused and could thus be</a>
<a name="ln2636">		// moved to the cached list.</a>
<a name="ln2637">		int32 usageCount;</a>
<a name="ln2638">		if (page-&gt;WiredCount() &gt; 0 || page-&gt;usage_count &gt; 0</a>
<a name="ln2639">			|| !cache-&gt;temporary) {</a>
<a name="ln2640">			usageCount = vm_clear_page_mapping_accessed_flags(page);</a>
<a name="ln2641">		} else</a>
<a name="ln2642">			usageCount = vm_remove_all_page_mappings_if_unaccessed(page);</a>
<a name="ln2643"> </a>
<a name="ln2644">		if (usageCount &gt; 0) {</a>
<a name="ln2645">			usageCount += page-&gt;usage_count + kPageUsageAdvance;</a>
<a name="ln2646">			if (usageCount &gt; kPageUsageMax)</a>
<a name="ln2647">				usageCount = kPageUsageMax;</a>
<a name="ln2648">// TODO: This would probably also be the place to reclaim swap space.</a>
<a name="ln2649">		} else {</a>
<a name="ln2650">			usageCount += page-&gt;usage_count - (int32)kPageUsageDecline;</a>
<a name="ln2651">			if (usageCount &lt; 0) {</a>
<a name="ln2652">				usageCount = 0;</a>
<a name="ln2653">				set_page_state(page, PAGE_STATE_INACTIVE);</a>
<a name="ln2654">			}</a>
<a name="ln2655">		}</a>
<a name="ln2656"> </a>
<a name="ln2657">		page-&gt;usage_count = usageCount;</a>
<a name="ln2658"> </a>
<a name="ln2659">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2660"> </a>
<a name="ln2661">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2662">	}</a>
<a name="ln2663">}</a>
<a name="ln2664"> </a>
<a name="ln2665"> </a>
<a name="ln2666">static void</a>
<a name="ln2667">full_scan_inactive_pages(page_stats&amp; pageStats, int32 despairLevel)</a>
<a name="ln2668">{</a>
<a name="ln2669">	int32 pagesToFree = pageStats.unsatisfiedReservations</a>
<a name="ln2670">		+ sFreeOrCachedPagesTarget</a>
<a name="ln2671">		- (pageStats.totalFreePages + pageStats.cachedPages);</a>
<a name="ln2672">	if (pagesToFree &lt;= 0)</a>
<a name="ln2673">		return;</a>
<a name="ln2674"> </a>
<a name="ln2675">	bigtime_t time = system_time();</a>
<a name="ln2676">	uint32 pagesScanned = 0;</a>
<a name="ln2677">	uint32 pagesToCached = 0;</a>
<a name="ln2678">	uint32 pagesToModified = 0;</a>
<a name="ln2679">	uint32 pagesToActive = 0;</a>
<a name="ln2680"> </a>
<a name="ln2681">	// Determine how many pages at maximum to send to the modified queue. Since</a>
<a name="ln2682">	// it is relatively expensive to page out pages, we do that on a grander</a>
<a name="ln2683">	// scale only when things get desperate.</a>
<a name="ln2684">	uint32 maxToFlush = despairLevel &lt;= 1 ? 32 : 10000;</a>
<a name="ln2685"> </a>
<a name="ln2686">	vm_page marker;</a>
<a name="ln2687">	init_page_marker(marker);</a>
<a name="ln2688"> </a>
<a name="ln2689">	VMPageQueue&amp; queue = sInactivePageQueue;</a>
<a name="ln2690">	InterruptsSpinLocker queueLocker(queue.GetLock());</a>
<a name="ln2691">	uint32 maxToScan = queue.Count();</a>
<a name="ln2692"> </a>
<a name="ln2693">	vm_page* nextPage = queue.Head();</a>
<a name="ln2694"> </a>
<a name="ln2695">	while (pagesToFree &gt; 0 &amp;&amp; maxToScan &gt; 0) {</a>
<a name="ln2696">		maxToScan--;</a>
<a name="ln2697"> </a>
<a name="ln2698">		// get the next page</a>
<a name="ln2699">		vm_page* page = nextPage;</a>
<a name="ln2700">		if (page == NULL)</a>
<a name="ln2701">			break;</a>
<a name="ln2702">		nextPage = queue.Next(page);</a>
<a name="ln2703"> </a>
<a name="ln2704">		if (page-&gt;busy)</a>
<a name="ln2705">			continue;</a>
<a name="ln2706"> </a>
<a name="ln2707">		// mark the position</a>
<a name="ln2708">		queue.InsertAfter(page, &amp;marker);</a>
<a name="ln2709">		queueLocker.Unlock();</a>
<a name="ln2710"> </a>
<a name="ln2711">		// lock the page's cache</a>
<a name="ln2712">		VMCache* cache = vm_cache_acquire_locked_page_cache(page, true);</a>
<a name="ln2713">		if (cache == NULL || page-&gt;busy</a>
<a name="ln2714">				|| page-&gt;State() != PAGE_STATE_INACTIVE) {</a>
<a name="ln2715">			if (cache != NULL)</a>
<a name="ln2716">				cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2717">			queueLocker.Lock();</a>
<a name="ln2718">			nextPage = queue.Next(&amp;marker);</a>
<a name="ln2719">			queue.Remove(&amp;marker);</a>
<a name="ln2720">			continue;</a>
<a name="ln2721">		}</a>
<a name="ln2722"> </a>
<a name="ln2723">		pagesScanned++;</a>
<a name="ln2724"> </a>
<a name="ln2725">		DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2726"> </a>
<a name="ln2727">		// Get the accessed count, clear the accessed/modified flags and</a>
<a name="ln2728">		// unmap the page, if it hasn't been accessed.</a>
<a name="ln2729">		int32 usageCount;</a>
<a name="ln2730">		if (page-&gt;WiredCount() &gt; 0)</a>
<a name="ln2731">			usageCount = vm_clear_page_mapping_accessed_flags(page);</a>
<a name="ln2732">		else</a>
<a name="ln2733">			usageCount = vm_remove_all_page_mappings_if_unaccessed(page);</a>
<a name="ln2734"> </a>
<a name="ln2735">		// update usage count</a>
<a name="ln2736">		if (usageCount &gt; 0) {</a>
<a name="ln2737">			usageCount += page-&gt;usage_count + kPageUsageAdvance;</a>
<a name="ln2738">			if (usageCount &gt; kPageUsageMax)</a>
<a name="ln2739">				usageCount = kPageUsageMax;</a>
<a name="ln2740">		} else {</a>
<a name="ln2741">			usageCount += page-&gt;usage_count - (int32)kPageUsageDecline;</a>
<a name="ln2742">			if (usageCount &lt; 0)</a>
<a name="ln2743">				usageCount = 0;</a>
<a name="ln2744">		}</a>
<a name="ln2745"> </a>
<a name="ln2746">		page-&gt;usage_count = usageCount;</a>
<a name="ln2747"> </a>
<a name="ln2748">		// Move to fitting queue or requeue:</a>
<a name="ln2749">		// * Active mapped pages go to the active queue.</a>
<a name="ln2750">		// * Inactive mapped (i.e. wired) pages are requeued.</a>
<a name="ln2751">		// * The remaining pages are cachable. Thus, if unmodified they go to</a>
<a name="ln2752">		//   the cached queue, otherwise to the modified queue (up to a limit).</a>
<a name="ln2753">		//   Note that until in the idle scanning we don't exempt pages of</a>
<a name="ln2754">		//   temporary caches. Apparently we really need memory, so we better</a>
<a name="ln2755">		//   page out memory as well.</a>
<a name="ln2756">		bool isMapped = page-&gt;IsMapped();</a>
<a name="ln2757">		if (usageCount &gt; 0) {</a>
<a name="ln2758">			if (isMapped) {</a>
<a name="ln2759">				set_page_state(page, PAGE_STATE_ACTIVE);</a>
<a name="ln2760">				pagesToActive++;</a>
<a name="ln2761">			} else</a>
<a name="ln2762">				vm_page_requeue(page, true);</a>
<a name="ln2763">		} else if (isMapped) {</a>
<a name="ln2764">			vm_page_requeue(page, true);</a>
<a name="ln2765">		} else if (!page-&gt;modified) {</a>
<a name="ln2766">			set_page_state(page, PAGE_STATE_CACHED);</a>
<a name="ln2767">			pagesToFree--;</a>
<a name="ln2768">			pagesToCached++;</a>
<a name="ln2769">		} else if (maxToFlush &gt; 0) {</a>
<a name="ln2770">			set_page_state(page, PAGE_STATE_MODIFIED);</a>
<a name="ln2771">			maxToFlush--;</a>
<a name="ln2772">			pagesToModified++;</a>
<a name="ln2773">		} else</a>
<a name="ln2774">			vm_page_requeue(page, true);</a>
<a name="ln2775"> </a>
<a name="ln2776">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2777"> </a>
<a name="ln2778">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2779"> </a>
<a name="ln2780">		// remove the marker</a>
<a name="ln2781">		queueLocker.Lock();</a>
<a name="ln2782">		nextPage = queue.Next(&amp;marker);</a>
<a name="ln2783">		queue.Remove(&amp;marker);</a>
<a name="ln2784">	}</a>
<a name="ln2785"> </a>
<a name="ln2786">	queueLocker.Unlock();</a>
<a name="ln2787"> </a>
<a name="ln2788">	time = system_time() - time;</a>
<a name="ln2789">	TRACE_DAEMON(&quot;  -&gt; inactive scan (%7&quot; B_PRId64 &quot; us): scanned: %7&quot; B_PRIu32</a>
<a name="ln2790">		&quot;, moved: %&quot; B_PRIu32 &quot; -&gt; cached, %&quot; B_PRIu32 &quot; -&gt; modified, %&quot;</a>
<a name="ln2791">		B_PRIu32 &quot; -&gt; active\n&quot;, time, pagesScanned, pagesToCached,</a>
<a name="ln2792">		pagesToModified, pagesToActive);</a>
<a name="ln2793"> </a>
<a name="ln2794">	// wake up the page writer, if we tossed it some pages</a>
<a name="ln2795">	if (pagesToModified &gt; 0)</a>
<a name="ln2796">		sPageWriterCondition.WakeUp();</a>
<a name="ln2797">}</a>
<a name="ln2798"> </a>
<a name="ln2799"> </a>
<a name="ln2800">static void</a>
<a name="ln2801">full_scan_active_pages(page_stats&amp; pageStats, int32 despairLevel)</a>
<a name="ln2802">{</a>
<a name="ln2803">	vm_page marker;</a>
<a name="ln2804">	init_page_marker(marker);</a>
<a name="ln2805"> </a>
<a name="ln2806">	VMPageQueue&amp; queue = sActivePageQueue;</a>
<a name="ln2807">	InterruptsSpinLocker queueLocker(queue.GetLock());</a>
<a name="ln2808">	uint32 maxToScan = queue.Count();</a>
<a name="ln2809"> </a>
<a name="ln2810">	int32 pagesToDeactivate = pageStats.unsatisfiedReservations</a>
<a name="ln2811">		+ sFreeOrCachedPagesTarget</a>
<a name="ln2812">		- (pageStats.totalFreePages + pageStats.cachedPages)</a>
<a name="ln2813">		+ std::max((int32)sInactivePagesTarget - (int32)maxToScan, (int32)0);</a>
<a name="ln2814">	if (pagesToDeactivate &lt;= 0)</a>
<a name="ln2815">		return;</a>
<a name="ln2816"> </a>
<a name="ln2817">	bigtime_t time = system_time();</a>
<a name="ln2818">	uint32 pagesAccessed = 0;</a>
<a name="ln2819">	uint32 pagesToInactive = 0;</a>
<a name="ln2820">	uint32 pagesScanned = 0;</a>
<a name="ln2821"> </a>
<a name="ln2822">	vm_page* nextPage = queue.Head();</a>
<a name="ln2823"> </a>
<a name="ln2824">	while (pagesToDeactivate &gt; 0 &amp;&amp; maxToScan &gt; 0) {</a>
<a name="ln2825">		maxToScan--;</a>
<a name="ln2826"> </a>
<a name="ln2827">		// get the next page</a>
<a name="ln2828">		vm_page* page = nextPage;</a>
<a name="ln2829">		if (page == NULL)</a>
<a name="ln2830">			break;</a>
<a name="ln2831">		nextPage = queue.Next(page);</a>
<a name="ln2832"> </a>
<a name="ln2833">		if (page-&gt;busy)</a>
<a name="ln2834">			continue;</a>
<a name="ln2835"> </a>
<a name="ln2836">		// mark the position</a>
<a name="ln2837">		queue.InsertAfter(page, &amp;marker);</a>
<a name="ln2838">		queueLocker.Unlock();</a>
<a name="ln2839"> </a>
<a name="ln2840">		// lock the page's cache</a>
<a name="ln2841">		VMCache* cache = vm_cache_acquire_locked_page_cache(page, true);</a>
<a name="ln2842">		if (cache == NULL || page-&gt;busy || page-&gt;State() != PAGE_STATE_ACTIVE) {</a>
<a name="ln2843">			if (cache != NULL)</a>
<a name="ln2844">				cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2845">			queueLocker.Lock();</a>
<a name="ln2846">			nextPage = queue.Next(&amp;marker);</a>
<a name="ln2847">			queue.Remove(&amp;marker);</a>
<a name="ln2848">			continue;</a>
<a name="ln2849">		}</a>
<a name="ln2850"> </a>
<a name="ln2851">		pagesScanned++;</a>
<a name="ln2852"> </a>
<a name="ln2853">		DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln2854"> </a>
<a name="ln2855">		// Get the page active/modified flags and update the page's usage count.</a>
<a name="ln2856">		int32 usageCount = vm_clear_page_mapping_accessed_flags(page);</a>
<a name="ln2857"> </a>
<a name="ln2858">		if (usageCount &gt; 0) {</a>
<a name="ln2859">			usageCount += page-&gt;usage_count + kPageUsageAdvance;</a>
<a name="ln2860">			if (usageCount &gt; kPageUsageMax)</a>
<a name="ln2861">				usageCount = kPageUsageMax;</a>
<a name="ln2862">			pagesAccessed++;</a>
<a name="ln2863">// TODO: This would probably also be the place to reclaim swap space.</a>
<a name="ln2864">		} else {</a>
<a name="ln2865">			usageCount += page-&gt;usage_count - (int32)kPageUsageDecline;</a>
<a name="ln2866">			if (usageCount &lt;= 0) {</a>
<a name="ln2867">				usageCount = 0;</a>
<a name="ln2868">				set_page_state(page, PAGE_STATE_INACTIVE);</a>
<a name="ln2869">				pagesToInactive++;</a>
<a name="ln2870">			}</a>
<a name="ln2871">		}</a>
<a name="ln2872"> </a>
<a name="ln2873">		page-&gt;usage_count = usageCount;</a>
<a name="ln2874"> </a>
<a name="ln2875">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln2876"> </a>
<a name="ln2877">		cache-&gt;ReleaseRefAndUnlock();</a>
<a name="ln2878"> </a>
<a name="ln2879">		// remove the marker</a>
<a name="ln2880">		queueLocker.Lock();</a>
<a name="ln2881">		nextPage = queue.Next(&amp;marker);</a>
<a name="ln2882">		queue.Remove(&amp;marker);</a>
<a name="ln2883">	}</a>
<a name="ln2884"> </a>
<a name="ln2885">	time = system_time() - time;</a>
<a name="ln2886">	TRACE_DAEMON(&quot;  -&gt;   active scan (%7&quot; B_PRId64 &quot; us): scanned: %7&quot; B_PRIu32</a>
<a name="ln2887">		&quot;, moved: %&quot; B_PRIu32 &quot; -&gt; inactive, encountered %&quot; B_PRIu32 &quot; accessed&quot;</a>
<a name="ln2888">		&quot; ones\n&quot;, time, pagesScanned, pagesToInactive, pagesAccessed);</a>
<a name="ln2889">}</a>
<a name="ln2890"> </a>
<a name="ln2891"> </a>
<a name="ln2892">static void</a>
<a name="ln2893">page_daemon_idle_scan(page_stats&amp; pageStats)</a>
<a name="ln2894">{</a>
<a name="ln2895">	TRACE_DAEMON(&quot;page daemon: idle run\n&quot;);</a>
<a name="ln2896"> </a>
<a name="ln2897">	if (pageStats.totalFreePages &lt; (int32)sFreePagesTarget) {</a>
<a name="ln2898">		// We want more actually free pages, so free some from the cached</a>
<a name="ln2899">		// ones.</a>
<a name="ln2900">		uint32 freed = free_cached_pages(</a>
<a name="ln2901">			sFreePagesTarget - pageStats.totalFreePages, false);</a>
<a name="ln2902">		if (freed &gt; 0)</a>
<a name="ln2903">			unreserve_pages(freed);</a>
<a name="ln2904">		get_page_stats(pageStats);</a>
<a name="ln2905">	}</a>
<a name="ln2906"> </a>
<a name="ln2907">	// Walk the active list and move pages to the inactive queue.</a>
<a name="ln2908">	get_page_stats(pageStats);</a>
<a name="ln2909">	idle_scan_active_pages(pageStats);</a>
<a name="ln2910">}</a>
<a name="ln2911"> </a>
<a name="ln2912"> </a>
<a name="ln2913">static void</a>
<a name="ln2914">page_daemon_full_scan(page_stats&amp; pageStats, int32 despairLevel)</a>
<a name="ln2915">{</a>
<a name="ln2916">	TRACE_DAEMON(&quot;page daemon: full run: free: %&quot; B_PRIu32 &quot;, cached: %&quot;</a>
<a name="ln2917">		B_PRIu32 &quot;, to free: %&quot; B_PRIu32 &quot;\n&quot;, pageStats.totalFreePages,</a>
<a name="ln2918">		pageStats.cachedPages, pageStats.unsatisfiedReservations</a>
<a name="ln2919">			+ sFreeOrCachedPagesTarget</a>
<a name="ln2920">			- (pageStats.totalFreePages + pageStats.cachedPages));</a>
<a name="ln2921"> </a>
<a name="ln2922">	// Walk the inactive list and transfer pages to the cached and modified</a>
<a name="ln2923">	// queues.</a>
<a name="ln2924">	full_scan_inactive_pages(pageStats, despairLevel);</a>
<a name="ln2925"> </a>
<a name="ln2926">	// Free cached pages. Also wake up reservation waiters.</a>
<a name="ln2927">	get_page_stats(pageStats);</a>
<a name="ln2928">	int32 pagesToFree = pageStats.unsatisfiedReservations + sFreePagesTarget</a>
<a name="ln2929">		- (pageStats.totalFreePages);</a>
<a name="ln2930">	if (pagesToFree &gt; 0) {</a>
<a name="ln2931">		uint32 freed = free_cached_pages(pagesToFree, true);</a>
<a name="ln2932">		if (freed &gt; 0)</a>
<a name="ln2933">			unreserve_pages(freed);</a>
<a name="ln2934">	}</a>
<a name="ln2935"> </a>
<a name="ln2936">	// Walk the active list and move pages to the inactive queue.</a>
<a name="ln2937">	get_page_stats(pageStats);</a>
<a name="ln2938">	full_scan_active_pages(pageStats, despairLevel);</a>
<a name="ln2939">}</a>
<a name="ln2940"> </a>
<a name="ln2941"> </a>
<a name="ln2942">static status_t</a>
<a name="ln2943">page_daemon(void* /*unused*/)</a>
<a name="ln2944">{</a>
<a name="ln2945">	int32 despairLevel = 0;</a>
<a name="ln2946"> </a>
<a name="ln2947">	while (true) {</a>
<a name="ln2948">		sPageDaemonCondition.ClearActivated();</a>
<a name="ln2949"> </a>
<a name="ln2950">		// evaluate the free pages situation</a>
<a name="ln2951">		page_stats pageStats;</a>
<a name="ln2952">		get_page_stats(pageStats);</a>
<a name="ln2953"> </a>
<a name="ln2954">		if (!do_active_paging(pageStats)) {</a>
<a name="ln2955">			// Things look good -- just maintain statistics and keep the pool</a>
<a name="ln2956">			// of actually free pages full enough.</a>
<a name="ln2957">			despairLevel = 0;</a>
<a name="ln2958">			page_daemon_idle_scan(pageStats);</a>
<a name="ln2959">			sPageDaemonCondition.Wait(kIdleScanWaitInterval, false);</a>
<a name="ln2960">		} else {</a>
<a name="ln2961">			// Not enough free pages. We need to do some real work.</a>
<a name="ln2962">			despairLevel = std::max(despairLevel + 1, (int32)3);</a>
<a name="ln2963">			page_daemon_full_scan(pageStats, despairLevel);</a>
<a name="ln2964"> </a>
<a name="ln2965">			// Don't wait after the first full scan, but rather immediately</a>
<a name="ln2966">			// check whether we were successful in freeing enough pages and</a>
<a name="ln2967">			// re-run with increased despair level. The first scan is</a>
<a name="ln2968">			// conservative with respect to moving inactive modified pages to</a>
<a name="ln2969">			// the modified list to avoid thrashing. The second scan, however,</a>
<a name="ln2970">			// will not hold back.</a>
<a name="ln2971">			if (despairLevel &gt; 1)</a>
<a name="ln2972">				snooze(kBusyScanWaitInterval);</a>
<a name="ln2973">		}</a>
<a name="ln2974">	}</a>
<a name="ln2975"> </a>
<a name="ln2976">	return B_OK;</a>
<a name="ln2977">}</a>
<a name="ln2978"> </a>
<a name="ln2979"> </a>
<a name="ln2980">/*!	Returns how many pages could *not* be reserved.</a>
<a name="ln2981">*/</a>
<a name="ln2982">static uint32</a>
<a name="ln2983">reserve_pages(uint32 count, int priority, bool dontWait)</a>
<a name="ln2984">{</a>
<a name="ln2985">	int32 dontTouch = kPageReserveForPriority[priority];</a>
<a name="ln2986"> </a>
<a name="ln2987">	while (true) {</a>
<a name="ln2988">		count -= reserve_some_pages(count, dontTouch);</a>
<a name="ln2989">		if (count == 0)</a>
<a name="ln2990">			return 0;</a>
<a name="ln2991"> </a>
<a name="ln2992">		if (sUnsatisfiedPageReservations == 0) {</a>
<a name="ln2993">			count -= free_cached_pages(count, dontWait);</a>
<a name="ln2994">			if (count == 0)</a>
<a name="ln2995">				return count;</a>
<a name="ln2996">		}</a>
<a name="ln2997"> </a>
<a name="ln2998">		if (dontWait)</a>
<a name="ln2999">			return count;</a>
<a name="ln3000"> </a>
<a name="ln3001">		// we need to wait for pages to become available</a>
<a name="ln3002"> </a>
<a name="ln3003">		MutexLocker pageDeficitLocker(sPageDeficitLock);</a>
<a name="ln3004"> </a>
<a name="ln3005">		bool notifyDaemon = sUnsatisfiedPageReservations == 0;</a>
<a name="ln3006">		sUnsatisfiedPageReservations += count;</a>
<a name="ln3007"> </a>
<a name="ln3008">		if (atomic_get(&amp;sUnreservedFreePages) &gt; dontTouch) {</a>
<a name="ln3009">			// the situation changed</a>
<a name="ln3010">			sUnsatisfiedPageReservations -= count;</a>
<a name="ln3011">			continue;</a>
<a name="ln3012">		}</a>
<a name="ln3013"> </a>
<a name="ln3014">		PageReservationWaiter waiter;</a>
<a name="ln3015">		waiter.dontTouch = dontTouch;</a>
<a name="ln3016">		waiter.missing = count;</a>
<a name="ln3017">		waiter.thread = thread_get_current_thread();</a>
<a name="ln3018">		waiter.threadPriority = waiter.thread-&gt;priority;</a>
<a name="ln3019"> </a>
<a name="ln3020">		// insert ordered (i.e. after all waiters with higher or equal priority)</a>
<a name="ln3021">		PageReservationWaiter* otherWaiter = NULL;</a>
<a name="ln3022">		for (PageReservationWaiterList::Iterator it</a>
<a name="ln3023">				= sPageReservationWaiters.GetIterator();</a>
<a name="ln3024">			(otherWaiter = it.Next()) != NULL;) {</a>
<a name="ln3025">			if (waiter &lt; *otherWaiter)</a>
<a name="ln3026">				break;</a>
<a name="ln3027">		}</a>
<a name="ln3028"> </a>
<a name="ln3029">		sPageReservationWaiters.InsertBefore(otherWaiter, &amp;waiter);</a>
<a name="ln3030"> </a>
<a name="ln3031">		thread_prepare_to_block(waiter.thread, 0, THREAD_BLOCK_TYPE_OTHER,</a>
<a name="ln3032">			&quot;waiting for pages&quot;);</a>
<a name="ln3033"> </a>
<a name="ln3034">		if (notifyDaemon)</a>
<a name="ln3035">			sPageDaemonCondition.WakeUp();</a>
<a name="ln3036"> </a>
<a name="ln3037">		pageDeficitLocker.Unlock();</a>
<a name="ln3038"> </a>
<a name="ln3039">		low_resource(B_KERNEL_RESOURCE_PAGES, count, B_RELATIVE_TIMEOUT, 0);</a>
<a name="ln3040">		thread_block();</a>
<a name="ln3041"> </a>
<a name="ln3042">		pageDeficitLocker.Lock();</a>
<a name="ln3043"> </a>
<a name="ln3044">		return 0;</a>
<a name="ln3045">	}</a>
<a name="ln3046">}</a>
<a name="ln3047"> </a>
<a name="ln3048"> </a>
<a name="ln3049">//	#pragma mark - private kernel API</a>
<a name="ln3050"> </a>
<a name="ln3051"> </a>
<a name="ln3052">/*!	Writes a range of modified pages of a cache to disk.</a>
<a name="ln3053">	You need to hold the VMCache lock when calling this function.</a>
<a name="ln3054">	Note that the cache lock is released in this function.</a>
<a name="ln3055">	\param cache The cache.</a>
<a name="ln3056">	\param firstPage Offset (in page size units) of the first page in the range.</a>
<a name="ln3057">	\param endPage End offset (in page size units) of the page range. The page</a>
<a name="ln3058">		at this offset is not included.</a>
<a name="ln3059">*/</a>
<a name="ln3060">status_t</a>
<a name="ln3061">vm_page_write_modified_page_range(struct VMCache* cache, uint32 firstPage,</a>
<a name="ln3062">	uint32 endPage)</a>
<a name="ln3063">{</a>
<a name="ln3064">	static const int32 kMaxPages = 256;</a>
<a name="ln3065">	int32 maxPages = cache-&gt;MaxPagesPerWrite();</a>
<a name="ln3066">	if (maxPages &lt; 0 || maxPages &gt; kMaxPages)</a>
<a name="ln3067">		maxPages = kMaxPages;</a>
<a name="ln3068"> </a>
<a name="ln3069">	const uint32 allocationFlags = HEAP_DONT_WAIT_FOR_MEMORY</a>
<a name="ln3070">		| HEAP_DONT_LOCK_KERNEL_SPACE;</a>
<a name="ln3071"> </a>
<a name="ln3072">	PageWriteWrapper stackWrappersPool[2];</a>
<a name="ln3073">	PageWriteWrapper* stackWrappers[1];</a>
<a name="ln3074">	PageWriteWrapper* wrapperPool</a>
<a name="ln3075">		= new(malloc_flags(allocationFlags)) PageWriteWrapper[maxPages + 1];</a>
<a name="ln3076">	PageWriteWrapper** wrappers</a>
<a name="ln3077">		= new(malloc_flags(allocationFlags)) PageWriteWrapper*[maxPages];</a>
<a name="ln3078">	if (wrapperPool == NULL || wrappers == NULL) {</a>
<a name="ln3079">		// don't fail, just limit our capabilities</a>
<a name="ln3080">		free(wrapperPool);</a>
<a name="ln3081">		free(wrappers);</a>
<a name="ln3082">		wrapperPool = stackWrappersPool;</a>
<a name="ln3083">		wrappers = stackWrappers;</a>
<a name="ln3084">		maxPages = 1;</a>
<a name="ln3085">	}</a>
<a name="ln3086"> </a>
<a name="ln3087">	int32 nextWrapper = 0;</a>
<a name="ln3088">	int32 usedWrappers = 0;</a>
<a name="ln3089"> </a>
<a name="ln3090">	PageWriteTransfer transfer;</a>
<a name="ln3091">	bool transferEmpty = true;</a>
<a name="ln3092"> </a>
<a name="ln3093">	VMCachePagesTree::Iterator it</a>
<a name="ln3094">		= cache-&gt;pages.GetIterator(firstPage, true, true);</a>
<a name="ln3095"> </a>
<a name="ln3096">	while (true) {</a>
<a name="ln3097">		vm_page* page = it.Next();</a>
<a name="ln3098">		if (page == NULL || page-&gt;cache_offset &gt;= endPage) {</a>
<a name="ln3099">			if (transferEmpty)</a>
<a name="ln3100">				break;</a>
<a name="ln3101"> </a>
<a name="ln3102">			page = NULL;</a>
<a name="ln3103">		}</a>
<a name="ln3104"> </a>
<a name="ln3105">		if (page != NULL) {</a>
<a name="ln3106">			if (page-&gt;busy</a>
<a name="ln3107">				|| (page-&gt;State() != PAGE_STATE_MODIFIED</a>
<a name="ln3108">					&amp;&amp; !vm_test_map_modification(page))) {</a>
<a name="ln3109">				page = NULL;</a>
<a name="ln3110">			}</a>
<a name="ln3111">		}</a>
<a name="ln3112"> </a>
<a name="ln3113">		PageWriteWrapper* wrapper = NULL;</a>
<a name="ln3114">		if (page != NULL) {</a>
<a name="ln3115">			wrapper = &amp;wrapperPool[nextWrapper++];</a>
<a name="ln3116">			if (nextWrapper &gt; maxPages)</a>
<a name="ln3117">				nextWrapper = 0;</a>
<a name="ln3118"> </a>
<a name="ln3119">			DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln3120"> </a>
<a name="ln3121">			wrapper-&gt;SetTo(page);</a>
<a name="ln3122"> </a>
<a name="ln3123">			if (transferEmpty || transfer.AddPage(page)) {</a>
<a name="ln3124">				if (transferEmpty) {</a>
<a name="ln3125">					transfer.SetTo(NULL, page, maxPages);</a>
<a name="ln3126">					transferEmpty = false;</a>
<a name="ln3127">				}</a>
<a name="ln3128"> </a>
<a name="ln3129">				DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln3130"> </a>
<a name="ln3131">				wrappers[usedWrappers++] = wrapper;</a>
<a name="ln3132">				continue;</a>
<a name="ln3133">			}</a>
<a name="ln3134"> </a>
<a name="ln3135">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln3136">		}</a>
<a name="ln3137"> </a>
<a name="ln3138">		if (transferEmpty)</a>
<a name="ln3139">			continue;</a>
<a name="ln3140"> </a>
<a name="ln3141">		cache-&gt;Unlock();</a>
<a name="ln3142">		status_t status = transfer.Schedule(0);</a>
<a name="ln3143">		cache-&gt;Lock();</a>
<a name="ln3144"> </a>
<a name="ln3145">		for (int32 i = 0; i &lt; usedWrappers; i++)</a>
<a name="ln3146">			wrappers[i]-&gt;Done(status);</a>
<a name="ln3147"> </a>
<a name="ln3148">		usedWrappers = 0;</a>
<a name="ln3149"> </a>
<a name="ln3150">		if (page != NULL) {</a>
<a name="ln3151">			transfer.SetTo(NULL, page, maxPages);</a>
<a name="ln3152">			wrappers[usedWrappers++] = wrapper;</a>
<a name="ln3153">		} else</a>
<a name="ln3154">			transferEmpty = true;</a>
<a name="ln3155">	}</a>
<a name="ln3156"> </a>
<a name="ln3157">	if (wrapperPool != stackWrappersPool) {</a>
<a name="ln3158">		delete[] wrapperPool;</a>
<a name="ln3159">		delete[] wrappers;</a>
<a name="ln3160">	}</a>
<a name="ln3161"> </a>
<a name="ln3162">	return B_OK;</a>
<a name="ln3163">}</a>
<a name="ln3164"> </a>
<a name="ln3165"> </a>
<a name="ln3166">/*!	You need to hold the VMCache lock when calling this function.</a>
<a name="ln3167">	Note that the cache lock is released in this function.</a>
<a name="ln3168">*/</a>
<a name="ln3169">status_t</a>
<a name="ln3170">vm_page_write_modified_pages(VMCache *cache)</a>
<a name="ln3171">{</a>
<a name="ln3172">	return vm_page_write_modified_page_range(cache, 0,</a>
<a name="ln3173">		(cache-&gt;virtual_end + B_PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT);</a>
<a name="ln3174">}</a>
<a name="ln3175"> </a>
<a name="ln3176"> </a>
<a name="ln3177">/*!	Schedules the page writer to write back the specified \a page.</a>
<a name="ln3178">	Note, however, that it might not do this immediately, and it can well</a>
<a name="ln3179">	take several seconds until the page is actually written out.</a>
<a name="ln3180">*/</a>
<a name="ln3181">void</a>
<a name="ln3182">vm_page_schedule_write_page(vm_page *page)</a>
<a name="ln3183">{</a>
<a name="ln3184">	PAGE_ASSERT(page, page-&gt;State() == PAGE_STATE_MODIFIED);</a>
<a name="ln3185"> </a>
<a name="ln3186">	vm_page_requeue(page, false);</a>
<a name="ln3187"> </a>
<a name="ln3188">	sPageWriterCondition.WakeUp();</a>
<a name="ln3189">}</a>
<a name="ln3190"> </a>
<a name="ln3191"> </a>
<a name="ln3192">/*!	Cache must be locked.</a>
<a name="ln3193">*/</a>
<a name="ln3194">void</a>
<a name="ln3195">vm_page_schedule_write_page_range(struct VMCache *cache, uint32 firstPage,</a>
<a name="ln3196">	uint32 endPage)</a>
<a name="ln3197">{</a>
<a name="ln3198">	uint32 modified = 0;</a>
<a name="ln3199">	for (VMCachePagesTree::Iterator it</a>
<a name="ln3200">				= cache-&gt;pages.GetIterator(firstPage, true, true);</a>
<a name="ln3201">			vm_page *page = it.Next();) {</a>
<a name="ln3202">		if (page-&gt;cache_offset &gt;= endPage)</a>
<a name="ln3203">			break;</a>
<a name="ln3204"> </a>
<a name="ln3205">		if (!page-&gt;busy &amp;&amp; page-&gt;State() == PAGE_STATE_MODIFIED) {</a>
<a name="ln3206">			DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln3207">			vm_page_requeue(page, false);</a>
<a name="ln3208">			modified++;</a>
<a name="ln3209">			DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln3210">		}</a>
<a name="ln3211">	}</a>
<a name="ln3212"> </a>
<a name="ln3213">	if (modified &gt; 0)</a>
<a name="ln3214">		sPageWriterCondition.WakeUp();</a>
<a name="ln3215">}</a>
<a name="ln3216"> </a>
<a name="ln3217"> </a>
<a name="ln3218">void</a>
<a name="ln3219">vm_page_init_num_pages(kernel_args *args)</a>
<a name="ln3220">{</a>
<a name="ln3221">	// calculate the size of memory by looking at the physical_memory_range array</a>
<a name="ln3222">	sPhysicalPageOffset = args-&gt;physical_memory_range[0].start / B_PAGE_SIZE;</a>
<a name="ln3223">	page_num_t physicalPagesEnd = sPhysicalPageOffset</a>
<a name="ln3224">		+ args-&gt;physical_memory_range[0].size / B_PAGE_SIZE;</a>
<a name="ln3225"> </a>
<a name="ln3226">	sNonExistingPages = 0;</a>
<a name="ln3227">	sIgnoredPages = args-&gt;ignored_physical_memory / B_PAGE_SIZE;</a>
<a name="ln3228"> </a>
<a name="ln3229">	for (uint32 i = 1; i &lt; args-&gt;num_physical_memory_ranges; i++) {</a>
<a name="ln3230">		page_num_t start = args-&gt;physical_memory_range[i].start / B_PAGE_SIZE;</a>
<a name="ln3231">		if (start &gt; physicalPagesEnd)</a>
<a name="ln3232">			sNonExistingPages += start - physicalPagesEnd;</a>
<a name="ln3233">		physicalPagesEnd = start</a>
<a name="ln3234">			+ args-&gt;physical_memory_range[i].size / B_PAGE_SIZE;</a>
<a name="ln3235"> </a>
<a name="ln3236">#ifdef LIMIT_AVAILABLE_MEMORY</a>
<a name="ln3237">		page_num_t available</a>
<a name="ln3238">			= physicalPagesEnd - sPhysicalPageOffset - sNonExistingPages;</a>
<a name="ln3239">		if (available &gt; LIMIT_AVAILABLE_MEMORY * (1024 * 1024 / B_PAGE_SIZE)) {</a>
<a name="ln3240">			physicalPagesEnd = sPhysicalPageOffset + sNonExistingPages</a>
<a name="ln3241">				+ LIMIT_AVAILABLE_MEMORY * (1024 * 1024 / B_PAGE_SIZE);</a>
<a name="ln3242">			break;</a>
<a name="ln3243">		}</a>
<a name="ln3244">#endif</a>
<a name="ln3245">	}</a>
<a name="ln3246"> </a>
<a name="ln3247">	TRACE((&quot;first phys page = %#&quot; B_PRIxPHYSADDR &quot;, end %#&quot; B_PRIxPHYSADDR &quot;\n&quot;,</a>
<a name="ln3248">		sPhysicalPageOffset, physicalPagesEnd));</a>
<a name="ln3249"> </a>
<a name="ln3250">	sNumPages = physicalPagesEnd - sPhysicalPageOffset;</a>
<a name="ln3251">}</a>
<a name="ln3252"> </a>
<a name="ln3253"> </a>
<a name="ln3254">status_t</a>
<a name="ln3255">vm_page_init(kernel_args *args)</a>
<a name="ln3256">{</a>
<a name="ln3257">	TRACE((&quot;vm_page_init: entry\n&quot;));</a>
<a name="ln3258"> </a>
<a name="ln3259">	// init page queues</a>
<a name="ln3260">	sModifiedPageQueue.Init(&quot;modified pages queue&quot;);</a>
<a name="ln3261">	sInactivePageQueue.Init(&quot;inactive pages queue&quot;);</a>
<a name="ln3262">	sActivePageQueue.Init(&quot;active pages queue&quot;);</a>
<a name="ln3263">	sCachedPageQueue.Init(&quot;cached pages queue&quot;);</a>
<a name="ln3264">	sFreePageQueue.Init(&quot;free pages queue&quot;);</a>
<a name="ln3265">	sClearPageQueue.Init(&quot;clear pages queue&quot;);</a>
<a name="ln3266"> </a>
<a name="ln3267">	new (&amp;sPageReservationWaiters) PageReservationWaiterList;</a>
<a name="ln3268"> </a>
<a name="ln3269">	// map in the new free page table</a>
<a name="ln3270">	sPages = (vm_page *)vm_allocate_early(args, sNumPages * sizeof(vm_page),</a>
<a name="ln3271">		~0L, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA, 0);</a>
<a name="ln3272"> </a>
<a name="ln3273">	TRACE((&quot;vm_init: putting free_page_table @ %p, # ents %&quot; B_PRIuPHYSADDR</a>
<a name="ln3274">		&quot; (size %#&quot; B_PRIxPHYSADDR &quot;)\n&quot;, sPages, sNumPages,</a>
<a name="ln3275">		(phys_addr_t)(sNumPages * sizeof(vm_page))));</a>
<a name="ln3276"> </a>
<a name="ln3277">	// initialize the free page table</a>
<a name="ln3278">	for (uint32 i = 0; i &lt; sNumPages; i++) {</a>
<a name="ln3279">		sPages[i].Init(sPhysicalPageOffset + i);</a>
<a name="ln3280">		sFreePageQueue.Append(&amp;sPages[i]);</a>
<a name="ln3281"> </a>
<a name="ln3282">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln3283">		sPages[i].allocation_tracking_info.Clear();</a>
<a name="ln3284">#endif</a>
<a name="ln3285">	}</a>
<a name="ln3286"> </a>
<a name="ln3287">	sUnreservedFreePages = sNumPages;</a>
<a name="ln3288"> </a>
<a name="ln3289">	TRACE((&quot;initialized table\n&quot;));</a>
<a name="ln3290"> </a>
<a name="ln3291">	// mark the ranges between usable physical memory unused</a>
<a name="ln3292">	phys_addr_t previousEnd = 0;</a>
<a name="ln3293">	for (uint32 i = 0; i &lt; args-&gt;num_physical_memory_ranges; i++) {</a>
<a name="ln3294">		phys_addr_t base = args-&gt;physical_memory_range[i].start;</a>
<a name="ln3295">		phys_size_t size = args-&gt;physical_memory_range[i].size;</a>
<a name="ln3296">		if (base &gt; previousEnd) {</a>
<a name="ln3297">			mark_page_range_in_use(previousEnd / B_PAGE_SIZE,</a>
<a name="ln3298">				(base - previousEnd) / B_PAGE_SIZE, false);</a>
<a name="ln3299">		}</a>
<a name="ln3300">		previousEnd = base + size;</a>
<a name="ln3301">	}</a>
<a name="ln3302"> </a>
<a name="ln3303">	// mark the allocated physical page ranges wired</a>
<a name="ln3304">	for (uint32 i = 0; i &lt; args-&gt;num_physical_allocated_ranges; i++) {</a>
<a name="ln3305">		mark_page_range_in_use(</a>
<a name="ln3306">			args-&gt;physical_allocated_range[i].start / B_PAGE_SIZE,</a>
<a name="ln3307">			args-&gt;physical_allocated_range[i].size / B_PAGE_SIZE, true);</a>
<a name="ln3308">	}</a>
<a name="ln3309"> </a>
<a name="ln3310">	// The target of actually free pages. This must be at least the system</a>
<a name="ln3311">	// reserve, but should be a few more pages, so we don't have to extract</a>
<a name="ln3312">	// a cached page with each allocation.</a>
<a name="ln3313">	sFreePagesTarget = VM_PAGE_RESERVE_USER</a>
<a name="ln3314">		+ std::max((page_num_t)32, (sNumPages - sNonExistingPages) / 1024);</a>
<a name="ln3315"> </a>
<a name="ln3316">	// The target of free + cached and inactive pages. On low-memory machines</a>
<a name="ln3317">	// keep things tight. free + cached is the pool of immediately allocatable</a>
<a name="ln3318">	// pages. We want a few inactive pages, so when we're actually paging, we</a>
<a name="ln3319">	// have a reasonably large set of pages to work with.</a>
<a name="ln3320">	if (sUnreservedFreePages &lt; 16 * 1024) {</a>
<a name="ln3321">		sFreeOrCachedPagesTarget = sFreePagesTarget + 128;</a>
<a name="ln3322">		sInactivePagesTarget = sFreePagesTarget / 3;</a>
<a name="ln3323">	} else {</a>
<a name="ln3324">		sFreeOrCachedPagesTarget = 2 * sFreePagesTarget;</a>
<a name="ln3325">		sInactivePagesTarget = sFreePagesTarget / 2;</a>
<a name="ln3326">	}</a>
<a name="ln3327"> </a>
<a name="ln3328">	TRACE((&quot;vm_page_init: exit\n&quot;));</a>
<a name="ln3329"> </a>
<a name="ln3330">	return B_OK;</a>
<a name="ln3331">}</a>
<a name="ln3332"> </a>
<a name="ln3333"> </a>
<a name="ln3334">status_t</a>
<a name="ln3335">vm_page_init_post_area(kernel_args *args)</a>
<a name="ln3336">{</a>
<a name="ln3337">	void *dummy;</a>
<a name="ln3338"> </a>
<a name="ln3339">	dummy = sPages;</a>
<a name="ln3340">	create_area(&quot;page structures&quot;, &amp;dummy, B_EXACT_ADDRESS,</a>
<a name="ln3341">		PAGE_ALIGN(sNumPages * sizeof(vm_page)), B_ALREADY_WIRED,</a>
<a name="ln3342">		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);</a>
<a name="ln3343"> </a>
<a name="ln3344">	add_debugger_command(&quot;page_stats&quot;, &amp;dump_page_stats,</a>
<a name="ln3345">		&quot;Dump statistics about page usage&quot;);</a>
<a name="ln3346">	add_debugger_command_etc(&quot;page&quot;, &amp;dump_page,</a>
<a name="ln3347">		&quot;Dump page info&quot;,</a>
<a name="ln3348">		&quot;[ \&quot;-p\&quot; | \&quot;-v\&quot; ] [ \&quot;-m\&quot; ] &lt;address&gt;\n&quot;</a>
<a name="ln3349">		&quot;Prints information for the physical page. If neither \&quot;-p\&quot; nor\n&quot;</a>
<a name="ln3350">		&quot;\&quot;-v\&quot; are given, the provided address is interpreted as address of\n&quot;</a>
<a name="ln3351">		&quot;the vm_page data structure for the page in question. If \&quot;-p\&quot; is\n&quot;</a>
<a name="ln3352">		&quot;given, the address is the physical address of the page. If \&quot;-v\&quot; is\n&quot;</a>
<a name="ln3353">		&quot;given, the address is interpreted as virtual address in the current\n&quot;</a>
<a name="ln3354">		&quot;thread's address space and for the page it is mapped to (if any)\n&quot;</a>
<a name="ln3355">		&quot;information are printed. If \&quot;-m\&quot; is specified, the command will\n&quot;</a>
<a name="ln3356">		&quot;search all known address spaces for mappings to that page and print\n&quot;</a>
<a name="ln3357">		&quot;them.\n&quot;, 0);</a>
<a name="ln3358">	add_debugger_command(&quot;page_queue&quot;, &amp;dump_page_queue, &quot;Dump page queue&quot;);</a>
<a name="ln3359">	add_debugger_command(&quot;find_page&quot;, &amp;find_page,</a>
<a name="ln3360">		&quot;Find out which queue a page is actually in&quot;);</a>
<a name="ln3361"> </a>
<a name="ln3362">#ifdef TRACK_PAGE_USAGE_STATS</a>
<a name="ln3363">	add_debugger_command_etc(&quot;page_usage&quot;, &amp;dump_page_usage_stats,</a>
<a name="ln3364">		&quot;Dumps statistics about page usage counts&quot;,</a>
<a name="ln3365">		&quot;\n&quot;</a>
<a name="ln3366">		&quot;Dumps statistics about page usage counts.\n&quot;,</a>
<a name="ln3367">		B_KDEBUG_DONT_PARSE_ARGUMENTS);</a>
<a name="ln3368">#endif</a>
<a name="ln3369"> </a>
<a name="ln3370">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln3371">	add_debugger_command_etc(&quot;page_allocations_per_caller&quot;,</a>
<a name="ln3372">		&amp;dump_page_allocations_per_caller,</a>
<a name="ln3373">		&quot;Dump current page allocations summed up per caller&quot;,</a>
<a name="ln3374">		&quot;[ -d &lt;caller&gt; ] [ -r ]\n&quot;</a>
<a name="ln3375">		&quot;The current allocations will by summed up by caller (their count)\n&quot;</a>
<a name="ln3376">		&quot;printed in decreasing order by count.\n&quot;</a>
<a name="ln3377">		&quot;If \&quot;-d\&quot; is given, each allocation for caller &lt;caller&gt; is printed\n&quot;</a>
<a name="ln3378">		&quot;including the respective stack trace.\n&quot;</a>
<a name="ln3379">		&quot;If \&quot;-r\&quot; is given, the allocation infos are reset after gathering\n&quot;</a>
<a name="ln3380">		&quot;the information, so the next command invocation will only show the\n&quot;</a>
<a name="ln3381">		&quot;allocations made after the reset.\n&quot;, 0);</a>
<a name="ln3382">	add_debugger_command_etc(&quot;page_allocation_infos&quot;,</a>
<a name="ln3383">		&amp;dump_page_allocation_infos,</a>
<a name="ln3384">		&quot;Dump current page allocations&quot;,</a>
<a name="ln3385">		&quot;[ --stacktrace ] [ -p &lt;page number&gt; ] [ --team &lt;team ID&gt; ] &quot;</a>
<a name="ln3386">		&quot;[ --thread &lt;thread ID&gt; ]\n&quot;</a>
<a name="ln3387">		&quot;The current allocations filtered by optional values will be printed.\n&quot;</a>
<a name="ln3388">		&quot;The optional \&quot;-p\&quot; page number filters for a specific page,\n&quot;</a>
<a name="ln3389">		&quot;with \&quot;--team\&quot; and \&quot;--thread\&quot; allocations by specific teams\n&quot;</a>
<a name="ln3390">		&quot;and/or threads can be filtered (these only work if a corresponding\n&quot;</a>
<a name="ln3391">		&quot;tracing entry is still available).\n&quot;</a>
<a name="ln3392">		&quot;If \&quot;--stacktrace\&quot; is given, then stack traces of the allocation\n&quot;</a>
<a name="ln3393">		&quot;callers are printed, where available\n&quot;, 0);</a>
<a name="ln3394">#endif</a>
<a name="ln3395"> </a>
<a name="ln3396">	return B_OK;</a>
<a name="ln3397">}</a>
<a name="ln3398"> </a>
<a name="ln3399"> </a>
<a name="ln3400">status_t</a>
<a name="ln3401">vm_page_init_post_thread(kernel_args *args)</a>
<a name="ln3402">{</a>
<a name="ln3403">	new (&amp;sFreePageCondition) ConditionVariable;</a>
<a name="ln3404">	sFreePageCondition.Publish(&amp;sFreePageQueue, &quot;free page&quot;);</a>
<a name="ln3405"> </a>
<a name="ln3406">	// create a kernel thread to clear out pages</a>
<a name="ln3407"> </a>
<a name="ln3408">	thread_id thread = spawn_kernel_thread(&amp;page_scrubber, &quot;page scrubber&quot;,</a>
<a name="ln3409">		B_LOWEST_ACTIVE_PRIORITY, NULL);</a>
<a name="ln3410">	resume_thread(thread);</a>
<a name="ln3411"> </a>
<a name="ln3412">	// start page writer</a>
<a name="ln3413"> </a>
<a name="ln3414">	sPageWriterCondition.Init(&quot;page writer&quot;);</a>
<a name="ln3415"> </a>
<a name="ln3416">	thread = spawn_kernel_thread(&amp;page_writer, &quot;page writer&quot;,</a>
<a name="ln3417">		B_NORMAL_PRIORITY + 1, NULL);</a>
<a name="ln3418">	resume_thread(thread);</a>
<a name="ln3419"> </a>
<a name="ln3420">	// start page daemon</a>
<a name="ln3421"> </a>
<a name="ln3422">	sPageDaemonCondition.Init(&quot;page daemon&quot;);</a>
<a name="ln3423"> </a>
<a name="ln3424">	thread = spawn_kernel_thread(&amp;page_daemon, &quot;page daemon&quot;,</a>
<a name="ln3425">		B_NORMAL_PRIORITY, NULL);</a>
<a name="ln3426">	resume_thread(thread);</a>
<a name="ln3427"> </a>
<a name="ln3428">	return B_OK;</a>
<a name="ln3429">}</a>
<a name="ln3430"> </a>
<a name="ln3431"> </a>
<a name="ln3432">status_t</a>
<a name="ln3433">vm_mark_page_inuse(page_num_t page)</a>
<a name="ln3434">{</a>
<a name="ln3435">	return vm_mark_page_range_inuse(page, 1);</a>
<a name="ln3436">}</a>
<a name="ln3437"> </a>
<a name="ln3438"> </a>
<a name="ln3439">status_t</a>
<a name="ln3440">vm_mark_page_range_inuse(page_num_t startPage, page_num_t length)</a>
<a name="ln3441">{</a>
<a name="ln3442">	return mark_page_range_in_use(startPage, length, false);</a>
<a name="ln3443">}</a>
<a name="ln3444"> </a>
<a name="ln3445"> </a>
<a name="ln3446">/*!	Unreserve pages previously reserved with vm_page_reserve_pages().</a>
<a name="ln3447">*/</a>
<a name="ln3448">void</a>
<a name="ln3449">vm_page_unreserve_pages(vm_page_reservation* reservation)</a>
<a name="ln3450">{</a>
<a name="ln3451">	uint32 count = reservation-&gt;count;</a>
<a name="ln3452">	reservation-&gt;count = 0;</a>
<a name="ln3453"> </a>
<a name="ln3454">	if (count == 0)</a>
<a name="ln3455">		return;</a>
<a name="ln3456"> </a>
<a name="ln3457">	TA(UnreservePages(count));</a>
<a name="ln3458"> </a>
<a name="ln3459">	unreserve_pages(count);</a>
<a name="ln3460">}</a>
<a name="ln3461"> </a>
<a name="ln3462"> </a>
<a name="ln3463">/*!	With this call, you can reserve a number of free pages in the system.</a>
<a name="ln3464">	They will only be handed out to someone who has actually reserved them.</a>
<a name="ln3465">	This call returns as soon as the number of requested pages has been</a>
<a name="ln3466">	reached.</a>
<a name="ln3467">	The caller must not hold any cache lock or the function might deadlock.</a>
<a name="ln3468">*/</a>
<a name="ln3469">void</a>
<a name="ln3470">vm_page_reserve_pages(vm_page_reservation* reservation, uint32 count,</a>
<a name="ln3471">	int priority)</a>
<a name="ln3472">{</a>
<a name="ln3473">	reservation-&gt;count = count;</a>
<a name="ln3474"> </a>
<a name="ln3475">	if (count == 0)</a>
<a name="ln3476">		return;</a>
<a name="ln3477"> </a>
<a name="ln3478">	TA(ReservePages(count));</a>
<a name="ln3479"> </a>
<a name="ln3480">	reserve_pages(count, priority, false);</a>
<a name="ln3481">}</a>
<a name="ln3482"> </a>
<a name="ln3483"> </a>
<a name="ln3484">bool</a>
<a name="ln3485">vm_page_try_reserve_pages(vm_page_reservation* reservation, uint32 count,</a>
<a name="ln3486">	int priority)</a>
<a name="ln3487">{</a>
<a name="ln3488">	if (count == 0) {</a>
<a name="ln3489">		reservation-&gt;count = count;</a>
<a name="ln3490">		return true;</a>
<a name="ln3491">	}</a>
<a name="ln3492"> </a>
<a name="ln3493">	uint32 remaining = reserve_pages(count, priority, true);</a>
<a name="ln3494">	if (remaining == 0) {</a>
<a name="ln3495">		TA(ReservePages(count));</a>
<a name="ln3496">		reservation-&gt;count = count;</a>
<a name="ln3497">		return true;</a>
<a name="ln3498">	}</a>
<a name="ln3499"> </a>
<a name="ln3500">	unreserve_pages(count - remaining);</a>
<a name="ln3501"> </a>
<a name="ln3502">	return false;</a>
<a name="ln3503">}</a>
<a name="ln3504"> </a>
<a name="ln3505"> </a>
<a name="ln3506">vm_page *</a>
<a name="ln3507">vm_page_allocate_page(vm_page_reservation* reservation, uint32 flags)</a>
<a name="ln3508">{</a>
<a name="ln3509">	uint32 pageState = flags &amp; VM_PAGE_ALLOC_STATE;</a>
<a name="ln3510">	ASSERT(pageState != PAGE_STATE_FREE);</a>
<a name="ln3511">	ASSERT(pageState != PAGE_STATE_CLEAR);</a>
<a name="ln3512"> </a>
<a name="ln3513">	ASSERT(reservation-&gt;count &gt; 0);</a>
<a name="ln3514">	reservation-&gt;count--;</a>
<a name="ln3515"> </a>
<a name="ln3516">	VMPageQueue* queue;</a>
<a name="ln3517">	VMPageQueue* otherQueue;</a>
<a name="ln3518"> </a>
<a name="ln3519">	if ((flags &amp; VM_PAGE_ALLOC_CLEAR) != 0) {</a>
<a name="ln3520">		queue = &amp;sClearPageQueue;</a>
<a name="ln3521">		otherQueue = &amp;sFreePageQueue;</a>
<a name="ln3522">	} else {</a>
<a name="ln3523">		queue = &amp;sFreePageQueue;</a>
<a name="ln3524">		otherQueue = &amp;sClearPageQueue;</a>
<a name="ln3525">	}</a>
<a name="ln3526"> </a>
<a name="ln3527">	ReadLocker locker(sFreePageQueuesLock);</a>
<a name="ln3528"> </a>
<a name="ln3529">	vm_page* page = queue-&gt;RemoveHeadUnlocked();</a>
<a name="ln3530">	if (page == NULL) {</a>
<a name="ln3531">		// if the primary queue was empty, grab the page from the</a>
<a name="ln3532">		// secondary queue</a>
<a name="ln3533">		page = otherQueue-&gt;RemoveHeadUnlocked();</a>
<a name="ln3534"> </a>
<a name="ln3535">		if (page == NULL) {</a>
<a name="ln3536">			// Unlikely, but possible: the page we have reserved has moved</a>
<a name="ln3537">			// between the queues after we checked the first queue. Grab the</a>
<a name="ln3538">			// write locker to make sure this doesn't happen again.</a>
<a name="ln3539">			locker.Unlock();</a>
<a name="ln3540">			WriteLocker writeLocker(sFreePageQueuesLock);</a>
<a name="ln3541"> </a>
<a name="ln3542">			page = queue-&gt;RemoveHead();</a>
<a name="ln3543">			if (page == NULL)</a>
<a name="ln3544">				otherQueue-&gt;RemoveHead();</a>
<a name="ln3545"> </a>
<a name="ln3546">			if (page == NULL) {</a>
<a name="ln3547">				panic(&quot;Had reserved page, but there is none!&quot;);</a>
<a name="ln3548">				return NULL;</a>
<a name="ln3549">			}</a>
<a name="ln3550"> </a>
<a name="ln3551">			// downgrade to read lock</a>
<a name="ln3552">			locker.Lock();</a>
<a name="ln3553">		}</a>
<a name="ln3554">	}</a>
<a name="ln3555"> </a>
<a name="ln3556">	if (page-&gt;CacheRef() != NULL)</a>
<a name="ln3557">		panic(&quot;supposed to be free page %p has cache\n&quot;, page);</a>
<a name="ln3558"> </a>
<a name="ln3559">	DEBUG_PAGE_ACCESS_START(page);</a>
<a name="ln3560"> </a>
<a name="ln3561">	int oldPageState = page-&gt;State();</a>
<a name="ln3562">	page-&gt;SetState(pageState);</a>
<a name="ln3563">	page-&gt;busy = (flags &amp; VM_PAGE_ALLOC_BUSY) != 0;</a>
<a name="ln3564">	page-&gt;usage_count = 0;</a>
<a name="ln3565">	page-&gt;accessed = false;</a>
<a name="ln3566">	page-&gt;modified = false;</a>
<a name="ln3567"> </a>
<a name="ln3568">	locker.Unlock();</a>
<a name="ln3569"> </a>
<a name="ln3570">	if (pageState &lt; PAGE_STATE_FIRST_UNQUEUED)</a>
<a name="ln3571">		sPageQueues[pageState].AppendUnlocked(page);</a>
<a name="ln3572"> </a>
<a name="ln3573">	// clear the page, if we had to take it from the free queue and a clear</a>
<a name="ln3574">	// page was requested</a>
<a name="ln3575">	if ((flags &amp; VM_PAGE_ALLOC_CLEAR) != 0 &amp;&amp; oldPageState != PAGE_STATE_CLEAR)</a>
<a name="ln3576">		clear_page(page);</a>
<a name="ln3577"> </a>
<a name="ln3578">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln3579">	page-&gt;allocation_tracking_info.Init(</a>
<a name="ln3580">		TA(AllocatePage(page-&gt;physical_page_number)));</a>
<a name="ln3581">#else</a>
<a name="ln3582">	TA(AllocatePage(page-&gt;physical_page_number));</a>
<a name="ln3583">#endif</a>
<a name="ln3584"> </a>
<a name="ln3585">	return page;</a>
<a name="ln3586">}</a>
<a name="ln3587"> </a>
<a name="ln3588"> </a>
<a name="ln3589">static void</a>
<a name="ln3590">allocate_page_run_cleanup(VMPageQueue::PageList&amp; freePages,</a>
<a name="ln3591">	VMPageQueue::PageList&amp; clearPages)</a>
<a name="ln3592">{</a>
<a name="ln3593">	while (vm_page* page = freePages.RemoveHead()) {</a>
<a name="ln3594">		page-&gt;busy = false;</a>
<a name="ln3595">		page-&gt;SetState(PAGE_STATE_FREE);</a>
<a name="ln3596">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln3597">		sFreePageQueue.PrependUnlocked(page);</a>
<a name="ln3598">	}</a>
<a name="ln3599"> </a>
<a name="ln3600">	while (vm_page* page = clearPages.RemoveHead()) {</a>
<a name="ln3601">		page-&gt;busy = false;</a>
<a name="ln3602">		page-&gt;SetState(PAGE_STATE_CLEAR);</a>
<a name="ln3603">		DEBUG_PAGE_ACCESS_END(page);</a>
<a name="ln3604">		sClearPageQueue.PrependUnlocked(page);</a>
<a name="ln3605">	}</a>
<a name="ln3606">}</a>
<a name="ln3607"> </a>
<a name="ln3608"> </a>
<a name="ln3609">/*!	Tries to allocate the a contiguous run of \a length pages starting at</a>
<a name="ln3610">	index \a start.</a>
<a name="ln3611"> </a>
<a name="ln3612">	The caller must have write-locked the free/clear page queues. The function</a>
<a name="ln3613">	will unlock regardless of whether it succeeds or fails.</a>
<a name="ln3614"> </a>
<a name="ln3615">	If the function fails, it cleans up after itself, i.e. it will free all</a>
<a name="ln3616">	pages it managed to allocate.</a>
<a name="ln3617"> </a>
<a name="ln3618">	\param start The start index (into \c sPages) of the run.</a>
<a name="ln3619">	\param length The number of pages to allocate.</a>
<a name="ln3620">	\param flags Page allocation flags. Encodes the state the function shall</a>
<a name="ln3621">		set the allocated pages to, whether the pages shall be marked busy</a>
<a name="ln3622">		(VM_PAGE_ALLOC_BUSY), and whether the pages shall be cleared</a>
<a name="ln3623">		(VM_PAGE_ALLOC_CLEAR).</a>
<a name="ln3624">	\param freeClearQueueLocker Locked WriteLocker for the free/clear page</a>
<a name="ln3625">		queues in locked state. Will be unlocked by the function.</a>
<a name="ln3626">	\return The index of the first page that could not be allocated. \a length</a>
<a name="ln3627">		is returned when the function was successful.</a>
<a name="ln3628">*/</a>
<a name="ln3629">static page_num_t</a>
<a name="ln3630">allocate_page_run(page_num_t start, page_num_t length, uint32 flags,</a>
<a name="ln3631">	WriteLocker&amp; freeClearQueueLocker)</a>
<a name="ln3632">{</a>
<a name="ln3633">	uint32 pageState = flags &amp; VM_PAGE_ALLOC_STATE;</a>
<a name="ln3634">	ASSERT(pageState != PAGE_STATE_FREE);</a>
<a name="ln3635">	ASSERT(pageState != PAGE_STATE_CLEAR);</a>
<a name="ln3636">	ASSERT(start + length &lt;= sNumPages);</a>
<a name="ln3637"> </a>
<a name="ln3638">	// Pull the free/clear pages out of their respective queues. Cached pages</a>
<a name="ln3639">	// are allocated later.</a>
<a name="ln3640">	page_num_t cachedPages = 0;</a>
<a name="ln3641">	VMPageQueue::PageList freePages;</a>
<a name="ln3642">	VMPageQueue::PageList clearPages;</a>
<a name="ln3643">	page_num_t i = 0;</a>
<a name="ln3644">	for (; i &lt; length; i++) {</a>
<a name="ln3645">		bool pageAllocated = true;</a>
<a name="ln3646">		bool noPage = false;</a>
<a name="ln3647">		vm_page&amp; page = sPages[start + i];</a>
<a name="ln3648">		switch (page.State()) {</a>
<a name="ln3649">			case PAGE_STATE_CLEAR:</a>
<a name="ln3650">				DEBUG_PAGE_ACCESS_START(&amp;page);</a>
<a name="ln3651">				sClearPageQueue.Remove(&amp;page);</a>
<a name="ln3652">				clearPages.Add(&amp;page);</a>
<a name="ln3653">				break;</a>
<a name="ln3654">			case PAGE_STATE_FREE:</a>
<a name="ln3655">				DEBUG_PAGE_ACCESS_START(&amp;page);</a>
<a name="ln3656">				sFreePageQueue.Remove(&amp;page);</a>
<a name="ln3657">				freePages.Add(&amp;page);</a>
<a name="ln3658">				break;</a>
<a name="ln3659">			case PAGE_STATE_CACHED:</a>
<a name="ln3660">				// We allocate cached pages later.</a>
<a name="ln3661">				cachedPages++;</a>
<a name="ln3662">				pageAllocated = false;</a>
<a name="ln3663">				break;</a>
<a name="ln3664"> </a>
<a name="ln3665">			default:</a>
<a name="ln3666">				// Probably a page was cached when our caller checked. Now it's</a>
<a name="ln3667">				// gone and we have to abort.</a>
<a name="ln3668">				noPage = true;</a>
<a name="ln3669">				break;</a>
<a name="ln3670">		}</a>
<a name="ln3671"> </a>
<a name="ln3672">		if (noPage)</a>
<a name="ln3673">			break;</a>
<a name="ln3674"> </a>
<a name="ln3675">		if (pageAllocated) {</a>
<a name="ln3676">			page.SetState(flags &amp; VM_PAGE_ALLOC_STATE);</a>
<a name="ln3677">			page.busy = (flags &amp; VM_PAGE_ALLOC_BUSY) != 0;</a>
<a name="ln3678">			page.usage_count = 0;</a>
<a name="ln3679">			page.accessed = false;</a>
<a name="ln3680">			page.modified = false;</a>
<a name="ln3681">		}</a>
<a name="ln3682">	}</a>
<a name="ln3683"> </a>
<a name="ln3684">	if (i &lt; length) {</a>
<a name="ln3685">		// failed to allocate a page -- free all that we've got</a>
<a name="ln3686">		allocate_page_run_cleanup(freePages, clearPages);</a>
<a name="ln3687">		return i;</a>
<a name="ln3688">	}</a>
<a name="ln3689"> </a>
<a name="ln3690">	freeClearQueueLocker.Unlock();</a>
<a name="ln3691"> </a>
<a name="ln3692">	if (cachedPages &gt; 0) {</a>
<a name="ln3693">		// allocate the pages that weren't free but cached</a>
<a name="ln3694">		page_num_t freedCachedPages = 0;</a>
<a name="ln3695">		page_num_t nextIndex = start;</a>
<a name="ln3696">		vm_page* freePage = freePages.Head();</a>
<a name="ln3697">		vm_page* clearPage = clearPages.Head();</a>
<a name="ln3698">		while (cachedPages &gt; 0) {</a>
<a name="ln3699">			// skip, if we've already got the page</a>
<a name="ln3700">			if (freePage != NULL &amp;&amp; size_t(freePage - sPages) == nextIndex) {</a>
<a name="ln3701">				freePage = freePages.GetNext(freePage);</a>
<a name="ln3702">				nextIndex++;</a>
<a name="ln3703">				continue;</a>
<a name="ln3704">			}</a>
<a name="ln3705">			if (clearPage != NULL &amp;&amp; size_t(clearPage - sPages) == nextIndex) {</a>
<a name="ln3706">				clearPage = clearPages.GetNext(clearPage);</a>
<a name="ln3707">				nextIndex++;</a>
<a name="ln3708">				continue;</a>
<a name="ln3709">			}</a>
<a name="ln3710"> </a>
<a name="ln3711">			// free the page, if it is still cached</a>
<a name="ln3712">			vm_page&amp; page = sPages[nextIndex];</a>
<a name="ln3713">			if (!free_cached_page(&amp;page, false)) {</a>
<a name="ln3714">				// TODO: if the page turns out to have been freed already,</a>
<a name="ln3715">				// there would be no need to fail</a>
<a name="ln3716">				break;</a>
<a name="ln3717">			}</a>
<a name="ln3718"> </a>
<a name="ln3719">			page.SetState(flags &amp; VM_PAGE_ALLOC_STATE);</a>
<a name="ln3720">			page.busy = (flags &amp; VM_PAGE_ALLOC_BUSY) != 0;</a>
<a name="ln3721">			page.usage_count = 0;</a>
<a name="ln3722">			page.accessed = false;</a>
<a name="ln3723">			page.modified = false;</a>
<a name="ln3724"> </a>
<a name="ln3725">			freePages.InsertBefore(freePage, &amp;page);</a>
<a name="ln3726">			freedCachedPages++;</a>
<a name="ln3727">			cachedPages--;</a>
<a name="ln3728">			nextIndex++;</a>
<a name="ln3729">		}</a>
<a name="ln3730"> </a>
<a name="ln3731">		// If we have freed cached pages, we need to balance things.</a>
<a name="ln3732">		if (freedCachedPages &gt; 0)</a>
<a name="ln3733">			unreserve_pages(freedCachedPages);</a>
<a name="ln3734"> </a>
<a name="ln3735">		if (nextIndex - start &lt; length) {</a>
<a name="ln3736">			// failed to allocate all cached pages -- free all that we've got</a>
<a name="ln3737">			freeClearQueueLocker.Lock();</a>
<a name="ln3738">			allocate_page_run_cleanup(freePages, clearPages);</a>
<a name="ln3739">			freeClearQueueLocker.Unlock();</a>
<a name="ln3740"> </a>
<a name="ln3741">			return nextIndex - start;</a>
<a name="ln3742">		}</a>
<a name="ln3743">	}</a>
<a name="ln3744"> </a>
<a name="ln3745">	// clear pages, if requested</a>
<a name="ln3746">	if ((flags &amp; VM_PAGE_ALLOC_CLEAR) != 0) {</a>
<a name="ln3747">		for (VMPageQueue::PageList::Iterator it = freePages.GetIterator();</a>
<a name="ln3748">				vm_page* page = it.Next();) {</a>
<a name="ln3749"> 			clear_page(page);</a>
<a name="ln3750">		}</a>
<a name="ln3751">	}</a>
<a name="ln3752"> </a>
<a name="ln3753">	// add pages to target queue</a>
<a name="ln3754">	if (pageState &lt; PAGE_STATE_FIRST_UNQUEUED) {</a>
<a name="ln3755">		freePages.MoveFrom(&amp;clearPages);</a>
<a name="ln3756">		sPageQueues[pageState].AppendUnlocked(freePages, length);</a>
<a name="ln3757">	}</a>
<a name="ln3758"> </a>
<a name="ln3759">	// Note: We don't unreserve the pages since we pulled them out of the</a>
<a name="ln3760">	// free/clear queues without adjusting sUnreservedFreePages.</a>
<a name="ln3761"> </a>
<a name="ln3762">#if VM_PAGE_ALLOCATION_TRACKING_AVAILABLE</a>
<a name="ln3763">	AbstractTraceEntryWithStackTrace* traceEntry</a>
<a name="ln3764">		= TA(AllocatePageRun(start, length));</a>
<a name="ln3765"> </a>
<a name="ln3766">	for (page_num_t i = start; i &lt; start + length; i++)</a>
<a name="ln3767">		sPages[i].allocation_tracking_info.Init(traceEntry);</a>
<a name="ln3768">#else</a>
<a name="ln3769">	TA(AllocatePageRun(start, length));</a>
<a name="ln3770">#endif</a>
<a name="ln3771"> </a>
<a name="ln3772">	return length;</a>
<a name="ln3773">}</a>
<a name="ln3774"> </a>
<a name="ln3775"> </a>
<a name="ln3776">/*! Allocate a physically contiguous range of pages.</a>
<a name="ln3777"> </a>
<a name="ln3778">	\param flags Page allocation flags. Encodes the state the function shall</a>
<a name="ln3779">		set the allocated pages to, whether the pages shall be marked busy</a>
<a name="ln3780">		(VM_PAGE_ALLOC_BUSY), and whether the pages shall be cleared</a>
<a name="ln3781">		(VM_PAGE_ALLOC_CLEAR).</a>
<a name="ln3782">	\param length The number of contiguous pages to allocate.</a>
<a name="ln3783">	\param restrictions Restrictions to the physical addresses of the page run</a>
<a name="ln3784">		to allocate, including \c low_address, the first acceptable physical</a>
<a name="ln3785">		address where the page run may start, \c high_address, the last</a>
<a name="ln3786">		acceptable physical address where the page run may end (i.e. it must</a>
<a name="ln3787">		hold \code runStartAddress + length &lt;= high_address \endcode),</a>
<a name="ln3788">		\c alignment, the alignment of the page run start address, and</a>
<a name="ln3789">		\c boundary, multiples of which the page run must not cross.</a>
<a name="ln3790">		Values set to \c 0 are ignored.</a>
<a name="ln3791">	\param priority The page reservation priority (as passed to</a>
<a name="ln3792">		vm_page_reserve_pages()).</a>
<a name="ln3793">	\return The first page of the allocated page run on success; \c NULL</a>
<a name="ln3794">		when the allocation failed.</a>
<a name="ln3795">*/</a>
<a name="ln3796">vm_page*</a>
<a name="ln3797">vm_page_allocate_page_run(uint32 flags, page_num_t length,</a>
<a name="ln3798">	const physical_address_restrictions* restrictions, int priority)</a>
<a name="ln3799">{</a>
<a name="ln3800">	// compute start and end page index</a>
<a name="ln3801">	page_num_t requestedStart</a>
<a name="ln3802">		= std::max(restrictions-&gt;low_address / B_PAGE_SIZE, sPhysicalPageOffset)</a>
<a name="ln3803">			- sPhysicalPageOffset;</a>
<a name="ln3804">	page_num_t start = requestedStart;</a>
<a name="ln3805">	page_num_t end;</a>
<a name="ln3806">	if (restrictions-&gt;high_address &gt; 0) {</a>
<a name="ln3807">		end = std::max(restrictions-&gt;high_address / B_PAGE_SIZE,</a>
<a name="ln3808">				sPhysicalPageOffset)</a>
<a name="ln3809">			- sPhysicalPageOffset;</a>
<a name="ln3810">		end = std::min(end, sNumPages);</a>
<a name="ln3811">	} else</a>
<a name="ln3812">		end = sNumPages;</a>
<a name="ln3813"> </a>
<a name="ln3814">	// compute alignment mask</a>
<a name="ln3815">	page_num_t alignmentMask</a>
<a name="ln3816">		= std::max(restrictions-&gt;alignment / B_PAGE_SIZE, (phys_addr_t)1) - 1;</a>
<a name="ln3817">	ASSERT(((alignmentMask + 1) &amp; alignmentMask) == 0);</a>
<a name="ln3818">		// alignment must be a power of 2</a>
<a name="ln3819"> </a>
<a name="ln3820">	// compute the boundary mask</a>
<a name="ln3821">	uint32 boundaryMask = 0;</a>
<a name="ln3822">	if (restrictions-&gt;boundary != 0) {</a>
<a name="ln3823">		page_num_t boundary = restrictions-&gt;boundary / B_PAGE_SIZE;</a>
<a name="ln3824">		// boundary must be a power of two and not less than alignment and</a>
<a name="ln3825">		// length</a>
<a name="ln3826">		ASSERT(((boundary - 1) &amp; boundary) == 0);</a>
<a name="ln3827">		ASSERT(boundary &gt;= alignmentMask + 1);</a>
<a name="ln3828">		ASSERT(boundary &gt;= length);</a>
<a name="ln3829"> </a>
<a name="ln3830">		boundaryMask = -boundary;</a>
<a name="ln3831">	}</a>
<a name="ln3832"> </a>
<a name="ln3833">	vm_page_reservation reservation;</a>
<a name="ln3834">	vm_page_reserve_pages(&amp;reservation, length, priority);</a>
<a name="ln3835"> </a>
<a name="ln3836">	WriteLocker freeClearQueueLocker(sFreePageQueuesLock);</a>
<a name="ln3837"> </a>
<a name="ln3838">	// First we try to get a run with free pages only. If that fails, we also</a>
<a name="ln3839">	// consider cached pages. If there are only few free pages and many cached</a>
<a name="ln3840">	// ones, the odds are that we won't find enough contiguous ones, so we skip</a>
<a name="ln3841">	// the first iteration in this case.</a>
<a name="ln3842">	int32 freePages = sUnreservedFreePages;</a>
<a name="ln3843">	int useCached = freePages &gt; 0 &amp;&amp; (page_num_t)freePages &gt; 2 * length ? 0 : 1;</a>
<a name="ln3844"> </a>
<a name="ln3845">	for (;;) {</a>
<a name="ln3846">		if (alignmentMask != 0 || boundaryMask != 0) {</a>
<a name="ln3847">			page_num_t offsetStart = start + sPhysicalPageOffset;</a>
<a name="ln3848"> </a>
<a name="ln3849">			// enforce alignment</a>
<a name="ln3850">			if ((offsetStart &amp; alignmentMask) != 0)</a>
<a name="ln3851">				offsetStart = (offsetStart + alignmentMask) &amp; ~alignmentMask;</a>
<a name="ln3852"> </a>
<a name="ln3853">			// enforce boundary</a>
<a name="ln3854">			if (boundaryMask != 0 &amp;&amp; ((offsetStart ^ (offsetStart</a>
<a name="ln3855">				+ length - 1)) &amp; boundaryMask) != 0) {</a>
<a name="ln3856">				offsetStart = (offsetStart + length - 1) &amp; boundaryMask;</a>
<a name="ln3857">			}</a>
<a name="ln3858"> </a>
<a name="ln3859">			start = offsetStart - sPhysicalPageOffset;</a>
<a name="ln3860">		}</a>
<a name="ln3861"> </a>
<a name="ln3862">		if (start + length &gt; end) {</a>
<a name="ln3863">			if (useCached == 0) {</a>
<a name="ln3864">				// The first iteration with free pages only was unsuccessful.</a>
<a name="ln3865">				// Try again also considering cached pages.</a>
<a name="ln3866">				useCached = 1;</a>
<a name="ln3867">				start = requestedStart;</a>
<a name="ln3868">				continue;</a>
<a name="ln3869">			}</a>
<a name="ln3870"> </a>
<a name="ln3871">			dprintf(&quot;vm_page_allocate_page_run(): Failed to allocate run of &quot;</a>
<a name="ln3872">				&quot;length %&quot; B_PRIuPHYSADDR &quot; (%&quot; B_PRIuPHYSADDR &quot; %&quot;</a>
<a name="ln3873">				B_PRIuPHYSADDR &quot;) in second iteration (align: %&quot; B_PRIuPHYSADDR</a>
<a name="ln3874">				&quot; boundary: %&quot; B_PRIuPHYSADDR &quot;)!\n&quot;, length, requestedStart,</a>
<a name="ln3875">				end, restrictions-&gt;alignment, restrictions-&gt;boundary);</a>
<a name="ln3876"> </a>
<a name="ln3877">			freeClearQueueLocker.Unlock();</a>
<a name="ln3878">			vm_page_unreserve_pages(&amp;reservation);</a>
<a name="ln3879">			return NULL;</a>
<a name="ln3880">		}</a>
<a name="ln3881"> </a>
<a name="ln3882">		bool foundRun = true;</a>
<a name="ln3883">		page_num_t i;</a>
<a name="ln3884">		for (i = 0; i &lt; length; i++) {</a>
<a name="ln3885">			uint32 pageState = sPages[start + i].State();</a>
<a name="ln3886">			if (pageState != PAGE_STATE_FREE</a>
<a name="ln3887">				&amp;&amp; pageState != PAGE_STATE_CLEAR</a>
<a name="ln3888">				&amp;&amp; (pageState != PAGE_STATE_CACHED || useCached == 0)) {</a>
<a name="ln3889">				foundRun = false;</a>
<a name="ln3890">				break;</a>
<a name="ln3891">			}</a>
<a name="ln3892">		}</a>
<a name="ln3893"> </a>
<a name="ln3894">		if (foundRun) {</a>
<a name="ln3895">			i = allocate_page_run(start, length, flags, freeClearQueueLocker);</a>
<a name="ln3896">			if (i == length)</a>
<a name="ln3897">				return &amp;sPages[start];</a>
<a name="ln3898"> </a>
<a name="ln3899">			// apparently a cached page couldn't be allocated -- skip it and</a>
<a name="ln3900">			// continue</a>
<a name="ln3901">			freeClearQueueLocker.Lock();</a>
<a name="ln3902">		}</a>
<a name="ln3903"> </a>
<a name="ln3904">		start += i + 1;</a>
<a name="ln3905">	}</a>
<a name="ln3906">}</a>
<a name="ln3907"> </a>
<a name="ln3908"> </a>
<a name="ln3909">vm_page *</a>
<a name="ln3910">vm_page_at_index(int32 index)</a>
<a name="ln3911">{</a>
<a name="ln3912">	return &amp;sPages[index];</a>
<a name="ln3913">}</a>
<a name="ln3914"> </a>
<a name="ln3915"> </a>
<a name="ln3916">vm_page *</a>
<a name="ln3917">vm_lookup_page(page_num_t pageNumber)</a>
<a name="ln3918">{</a>
<a name="ln3919">	if (pageNumber &lt; sPhysicalPageOffset)</a>
<a name="ln3920">		return NULL;</a>
<a name="ln3921"> </a>
<a name="ln3922">	pageNumber -= sPhysicalPageOffset;</a>
<a name="ln3923">	if (pageNumber &gt;= sNumPages)</a>
<a name="ln3924">		return NULL;</a>
<a name="ln3925"> </a>
<a name="ln3926">	return &amp;sPages[pageNumber];</a>
<a name="ln3927">}</a>
<a name="ln3928"> </a>
<a name="ln3929"> </a>
<a name="ln3930">bool</a>
<a name="ln3931">vm_page_is_dummy(struct vm_page *page)</a>
<a name="ln3932">{</a>
<a name="ln3933">	return page &lt; sPages || page &gt;= sPages + sNumPages;</a>
<a name="ln3934">}</a>
<a name="ln3935"> </a>
<a name="ln3936"> </a>
<a name="ln3937">/*!	Free the page that belonged to a certain cache.</a>
<a name="ln3938">	You can use vm_page_set_state() manually if you prefer, but only</a>
<a name="ln3939">	if the page does not equal PAGE_STATE_MODIFIED.</a>
<a name="ln3940"> </a>
<a name="ln3941">	\param cache The cache the page was previously owned by or NULL. The page</a>
<a name="ln3942">		must have been removed from its cache before calling this method in</a>
<a name="ln3943">		either case.</a>
<a name="ln3944">	\param page The page to free.</a>
<a name="ln3945">	\param reservation If not NULL, the page count of the reservation will be</a>
<a name="ln3946">		incremented, thus allowing to allocate another page for the freed one at</a>
<a name="ln3947">		a later time.</a>
<a name="ln3948">*/</a>
<a name="ln3949">void</a>
<a name="ln3950">vm_page_free_etc(VMCache* cache, vm_page* page,</a>
<a name="ln3951">	vm_page_reservation* reservation)</a>
<a name="ln3952">{</a>
<a name="ln3953">	PAGE_ASSERT(page, page-&gt;State() != PAGE_STATE_FREE</a>
<a name="ln3954">		&amp;&amp; page-&gt;State() != PAGE_STATE_CLEAR);</a>
<a name="ln3955"> </a>
<a name="ln3956">	if (page-&gt;State() == PAGE_STATE_MODIFIED &amp;&amp; cache-&gt;temporary)</a>
<a name="ln3957">		atomic_add(&amp;sModifiedTemporaryPages, -1);</a>
<a name="ln3958"> </a>
<a name="ln3959">	free_page(page, false);</a>
<a name="ln3960">	if (reservation == NULL)</a>
<a name="ln3961">		unreserve_pages(1);</a>
<a name="ln3962">}</a>
<a name="ln3963"> </a>
<a name="ln3964"> </a>
<a name="ln3965">void</a>
<a name="ln3966">vm_page_set_state(vm_page *page, int pageState)</a>
<a name="ln3967">{</a>
<a name="ln3968">	PAGE_ASSERT(page, page-&gt;State() != PAGE_STATE_FREE</a>
<a name="ln3969">		&amp;&amp; page-&gt;State() != PAGE_STATE_CLEAR);</a>
<a name="ln3970"> </a>
<a name="ln3971">	if (pageState == PAGE_STATE_FREE || pageState == PAGE_STATE_CLEAR) {</a>
<a name="ln3972">		free_page(page, pageState == PAGE_STATE_CLEAR);</a>
<a name="ln3973">		unreserve_pages(1);</a>
<a name="ln3974">	} else</a>
<a name="ln3975">		set_page_state(page, pageState);</a>
<a name="ln3976">}</a>
<a name="ln3977"> </a>
<a name="ln3978"> </a>
<a name="ln3979">/*!	Moves a page to either the tail of the head of its current queue,</a>
<a name="ln3980">	depending on \a tail.</a>
<a name="ln3981">	The page must have a cache and the cache must be locked!</a>
<a name="ln3982">*/</a>
<a name="ln3983">void</a>
<a name="ln3984">vm_page_requeue(struct vm_page *page, bool tail)</a>
<a name="ln3985">{</a>
<a name="ln3986">	PAGE_ASSERT(page, page-&gt;Cache() != NULL);</a>
<a name="ln3987">	page-&gt;Cache()-&gt;AssertLocked();</a>
<a name="ln3988">	// DEBUG_PAGE_ACCESS_CHECK(page);</a>
<a name="ln3989">		// TODO: This assertion cannot be satisfied by idle_scan_active_pages()</a>
<a name="ln3990">		// when it requeues busy pages. The reason is that vm_soft_fault()</a>
<a name="ln3991">		// (respectively fault_get_page()) and the file cache keep newly</a>
<a name="ln3992">		// allocated pages accessed while they are reading them from disk. It</a>
<a name="ln3993">		// would probably be better to change that code and reenable this</a>
<a name="ln3994">		// check.</a>
<a name="ln3995"> </a>
<a name="ln3996">	VMPageQueue *queue = NULL;</a>
<a name="ln3997"> </a>
<a name="ln3998">	switch (page-&gt;State()) {</a>
<a name="ln3999">		case PAGE_STATE_ACTIVE:</a>
<a name="ln4000">			queue = &amp;sActivePageQueue;</a>
<a name="ln4001">			break;</a>
<a name="ln4002">		case PAGE_STATE_INACTIVE:</a>
<a name="ln4003">			queue = &amp;sInactivePageQueue;</a>
<a name="ln4004">			break;</a>
<a name="ln4005">		case PAGE_STATE_MODIFIED:</a>
<a name="ln4006">			queue = &amp;sModifiedPageQueue;</a>
<a name="ln4007">			break;</a>
<a name="ln4008">		case PAGE_STATE_CACHED:</a>
<a name="ln4009">			queue = &amp;sCachedPageQueue;</a>
<a name="ln4010">			break;</a>
<a name="ln4011">		case PAGE_STATE_FREE:</a>
<a name="ln4012">		case PAGE_STATE_CLEAR:</a>
<a name="ln4013">			panic(&quot;vm_page_requeue() called for free/clear page %p&quot;, page);</a>
<a name="ln4014">			return;</a>
<a name="ln4015">		case PAGE_STATE_WIRED:</a>
<a name="ln4016">		case PAGE_STATE_UNUSED:</a>
<a name="ln4017">			return;</a>
<a name="ln4018">		default:</a>
<a name="ln4019">			panic(&quot;vm_page_touch: vm_page %p in invalid state %d\n&quot;,</a>
<a name="ln4020">				page, page-&gt;State());</a>
<a name="ln4021">			break;</a>
<a name="ln4022">	}</a>
<a name="ln4023"> </a>
<a name="ln4024">	queue-&gt;RequeueUnlocked(page, tail);</a>
<a name="ln4025">}</a>
<a name="ln4026"> </a>
<a name="ln4027"> </a>
<a name="ln4028">page_num_t</a>
<a name="ln4029">vm_page_num_pages(void)</a>
<a name="ln4030">{</a>
<a name="ln4031">	return sNumPages - sNonExistingPages;</a>
<a name="ln4032">}</a>
<a name="ln4033"> </a>
<a name="ln4034"> </a>
<a name="ln4035">/*! There is a subtle distinction between the page counts returned by</a>
<a name="ln4036">	this function and vm_page_num_free_pages():</a>
<a name="ln4037">	The latter returns the number of pages that are completely uncommitted,</a>
<a name="ln4038">	whereas this one returns the number of pages that are available for</a>
<a name="ln4039">	use by being reclaimed as well (IOW it factors in things like cache pages</a>
<a name="ln4040">	as available).</a>
<a name="ln4041">*/</a>
<a name="ln4042">page_num_t</a>
<a name="ln4043">vm_page_num_available_pages(void)</a>
<a name="ln4044">{</a>
<a name="ln4045">	return vm_available_memory() / B_PAGE_SIZE;</a>
<a name="ln4046">}</a>
<a name="ln4047"> </a>
<a name="ln4048"> </a>
<a name="ln4049">page_num_t</a>
<a name="ln4050">vm_page_num_free_pages(void)</a>
<a name="ln4051">{</a>
<a name="ln4052">	int32 count = sUnreservedFreePages + sCachedPageQueue.Count();</a>
<a name="ln4053">	return count &gt; 0 ? count : 0;</a>
<a name="ln4054">}</a>
<a name="ln4055"> </a>
<a name="ln4056"> </a>
<a name="ln4057">page_num_t</a>
<a name="ln4058">vm_page_num_unused_pages(void)</a>
<a name="ln4059">{</a>
<a name="ln4060">	int32 count = sUnreservedFreePages;</a>
<a name="ln4061">	return count &gt; 0 ? count : 0;</a>
<a name="ln4062">}</a>
<a name="ln4063"> </a>
<a name="ln4064"> </a>
<a name="ln4065">void</a>
<a name="ln4066">vm_page_get_stats(system_info *info)</a>
<a name="ln4067">{</a>
<a name="ln4068">	// Note: there's no locking protecting any of the queues or counters here,</a>
<a name="ln4069">	// so we run the risk of getting bogus values when evaluating them</a>
<a name="ln4070">	// throughout this function. As these stats are for informational purposes</a>
<a name="ln4071">	// only, it is not really worth introducing such locking. Therefore we just</a>
<a name="ln4072">	// ensure that we don't under- or overflow any of the values.</a>
<a name="ln4073"> </a>
<a name="ln4074">	// The pages used for the block cache buffers. Those should not be counted</a>
<a name="ln4075">	// as used but as cached pages.</a>
<a name="ln4076">	// TODO: We should subtract the blocks that are in use ATM, since those</a>
<a name="ln4077">	// can't really be freed in a low memory situation.</a>
<a name="ln4078">	page_num_t blockCachePages = block_cache_used_memory() / B_PAGE_SIZE;</a>
<a name="ln4079">	info-&gt;block_cache_pages = blockCachePages;</a>
<a name="ln4080"> </a>
<a name="ln4081">	// Non-temporary modified pages are special as they represent pages that</a>
<a name="ln4082">	// can be written back, so they could be freed if necessary, for us</a>
<a name="ln4083">	// basically making them into cached pages with a higher overhead. The</a>
<a name="ln4084">	// modified queue count is therefore split into temporary and non-temporary</a>
<a name="ln4085">	// counts that are then added to the corresponding number.</a>
<a name="ln4086">	page_num_t modifiedNonTemporaryPages</a>
<a name="ln4087">		= (sModifiedPageQueue.Count() - sModifiedTemporaryPages);</a>
<a name="ln4088"> </a>
<a name="ln4089">	info-&gt;max_pages = vm_page_num_pages();</a>
<a name="ln4090">	info-&gt;cached_pages = sCachedPageQueue.Count() + modifiedNonTemporaryPages</a>
<a name="ln4091">		+ blockCachePages;</a>
<a name="ln4092"> </a>
<a name="ln4093">	// max_pages is composed of:</a>
<a name="ln4094">	//	active + inactive + unused + wired + modified + cached + free + clear</a>
<a name="ln4095">	// So taking out the cached (including modified non-temporary), free and</a>
<a name="ln4096">	// clear ones leaves us with all used pages.</a>
<a name="ln4097">	uint32 subtractPages = info-&gt;cached_pages + sFreePageQueue.Count()</a>
<a name="ln4098">		+ sClearPageQueue.Count();</a>
<a name="ln4099">	info-&gt;used_pages = subtractPages &gt; info-&gt;max_pages</a>
<a name="ln4100">		? 0 : info-&gt;max_pages - subtractPages;</a>
<a name="ln4101"> </a>
<a name="ln4102">	if (info-&gt;used_pages + info-&gt;cached_pages &gt; info-&gt;max_pages) {</a>
<a name="ln4103">		// Something was shuffled around while we were summing up the counts.</a>
<a name="ln4104">		// Make the values sane, preferring the worse case of more used pages.</a>
<a name="ln4105">		info-&gt;cached_pages = info-&gt;max_pages - info-&gt;used_pages;</a>
<a name="ln4106">	}</a>
<a name="ln4107"> </a>
<a name="ln4108">	info-&gt;page_faults = vm_num_page_faults();</a>
<a name="ln4109">	info-&gt;ignored_pages = sIgnoredPages;</a>
<a name="ln4110"> </a>
<a name="ln4111">	// TODO: We don't consider pages used for page directories/tables yet.</a>
<a name="ln4112">}</a>
<a name="ln4113"> </a>
<a name="ln4114"> </a>
<a name="ln4115">/*!	Returns the greatest address within the last page of accessible physical</a>
<a name="ln4116">	memory.</a>
<a name="ln4117">	The value is inclusive, i.e. in case of a 32 bit phys_addr_t 0xffffffff</a>
<a name="ln4118">	means the that the last page ends at exactly 4 GB.</a>
<a name="ln4119">*/</a>
<a name="ln4120">phys_addr_t</a>
<a name="ln4121">vm_page_max_address()</a>
<a name="ln4122">{</a>
<a name="ln4123">	return ((phys_addr_t)sPhysicalPageOffset + sNumPages) * B_PAGE_SIZE - 1;</a>
<a name="ln4124">}</a>
<a name="ln4125"> </a>
<a name="ln4126"> </a>
<a name="ln4127">RANGE_MARKER_FUNCTION_END(vm_page)</a>

</code></pre>
<div class="balloon" rel="3080"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v611/" target="_blank">V611</a> The memory was allocated using 'new' operator but was released using the 'free' function. Consider inspecting operation logics behind the 'wrapperPool' variable.</p></div>
<div class="balloon" rel="3081"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v611/" target="_blank">V611</a> The memory was allocated using 'new' operator but was released using the 'free' function. Consider inspecting operation logics behind the 'wrappers' variable.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
